{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gpt_tests' has no attribute 'test_unidirectional_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ada251a3e854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpt_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_unidirectional_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gpt_tests' has no attribute 'test_unidirectional_attn'"
     ]
    }
   ],
   "source": [
    "import gpt_tests\n",
    "gpt_tests.test_unidirectional_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from typing import *\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import gpt_tests\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from math import sqrt\n",
    "import bert_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the GPT-2 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size/num_heads\n",
    "        self.attn_lin = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.out_lin = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        product = self.attn_lin(x)\n",
    "        seq_len = x.shape[1]\n",
    "        good_format = rearrange(product, 'b n (qkv h p) -> qkv b h n p', qkv = 3, h = self.num_heads)\n",
    "        queries, keys, values = good_format[0], good_format[1], good_format[2]\n",
    "        attn_score = t.einsum('bhfp,bhtp -> bhft', keys, queries) / sqrt(self.head_size)\n",
    "        \n",
    "        arange = t.arange(seq_len, device=x.device)\n",
    "        arange_rows = repeat(arange, 'a -> b a', b = seq_len)\n",
    "        arange_cols = repeat(arange, 'a -> a b', b = seq_len)\n",
    "        attn_score[:,:,arange_rows < arange_cols] = -1e4\n",
    "        \n",
    "        attn_pattn = t.softmax(attn_score, dim=-2)\n",
    "        # attn_pattn: b h n n; values: b h n p\n",
    "        out_by_head = t.einsum('bhft,bhfp->bhtp', attn_pattn, values)\n",
    "        out = rearrange(out_by_head, 'b h t p -> b t (h p)') # b n hidden_size\n",
    "        return self.out_lin(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gpt_tests' has no attribute 'test_unidirectional_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2bbedfdcdde2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_unidirectional_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiHeadedAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gpt_tests' has no attribute 'test_unidirectional_attn'"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_unidirectional_attn(MultiHeadedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size*4, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = self.attn(x1) + x\n",
    "        x3 = self.ln2(x2)\n",
    "        return self.mlp(x3) + x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        \n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        final_encoding = encoding[:,-1]\n",
    "        \n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        \n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_replace(s):\n",
    "    s = s.replace(\"embedding.weight\", \"embedding\")\n",
    "    s = s.replace(\"linear1\", \"mlp.0\")\n",
    "    s = s.replace(\"linear2\", \"mlp.2\")\n",
    "    s = s.replace(\"ln.\", \"blocks.12.\")\n",
    "    return s\n",
    "\n",
    "their_dict = pretrained_gpt.state_dict()\n",
    "for k in list(their_dict.keys()):\n",
    "    their_dict[string_replace(k)] = their_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    top_logit_idxs = t.argsort(logits, descending=True)[0,:top_k]\n",
    "    top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "    print(top_logit_words)\n",
    "    print(probs[0,top_logit_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gpt.cpu()\n",
    "feed_gpt(pretrained_gpt, \"Students at the machine learning bootcamp really enjoyed the\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt_top(model: nn.Module, input_ids: List[int], top_k: int = 10):\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    return t.argsort(logits, descending=True)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_str = \"The machine learning bootcamp started out nicely. But soon, I got an ominous feeling. Shockingly, I discovered\"\n",
    "input_ids = tokenizer(start_str)[\"input_ids\"]\n",
    "for i in range(100):\n",
    "    new_token = feed_gpt_top(my_gpt, input_ids, 1)\n",
    "    print(tokenizer.decode(new_token), end = \" \")\n",
    "    input_ids.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Modified(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        self.encoding = encoding\n",
    "        final_encoding = encoding[:,-1]\n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt_modified = GPT2Modified(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt_modified.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_thing():\n",
    "    s = 'My life motto: Fortune favors the bold'\n",
    "    ids = tokenizer(s)[\"input_ids\"]\n",
    "    thing = [ids[:i]+[0]*(10-i) for i in range(6,10)]\n",
    "    print([len(t) for t in thing])\n",
    "\n",
    "print(tokenizer.decode(t.tensor(create_padded_thing())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
