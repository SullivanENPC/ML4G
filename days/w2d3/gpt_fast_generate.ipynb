{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "from torch import nn \n",
    "from einops import rearrange, reduce, repeat\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import transformers\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gpt_tests\n",
    "import bert_sol\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking encoding:\n",
      "Congrats! You've passed the test!\n",
      "Checking new key and value:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class UnidirectionalMultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads \n",
    "        self.head_size = hidden_size // num_heads\n",
    "        assert self.head_size * num_heads == hidden_size\n",
    "        self.attentionLL = nn.Linear(hidden_size, num_heads*self.head_size*3) \n",
    "        self.outputLL = nn.Linear(num_heads*self.head_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, past_key_values = None, return_key_values = False): # [batch, seq_length, hidden_size]\n",
    "        if past_key_values is None:\n",
    "            # Shape: batch seq_len hidden_size*3\n",
    "            KQV = self.attentionLL(x)\n",
    "            KQV = rearrange(KQV, \"batch seq_len (three num_heads head_size) -> batch num_heads seq_len head_size three \", num_heads=self.num_heads, three=3)\n",
    "            Q = KQV[:, :, :, :, 0]\n",
    "            K = KQV[:, :, :, :, 1]\n",
    "            V = KQV[:, :, :, :, 2]\n",
    "            # Multiplying K and Q\n",
    "            attention_pattern = einsum('b n s h, b n t h -> b n s t', K, Q)\n",
    "            # Scale\n",
    "            attention_pattern = attention_pattern / math.sqrt(self.head_size)\n",
    "            # Key (row) must be less than Query (col), if not we set it to 1e-4\n",
    "            attention_pattern = torch.triu(attention_pattern) + (-1e4) * torch.tril(torch.ones_like(attention_pattern), diagonal=-1)        \n",
    "            # Softmax: batch num_heads key_len query_len, so we want to softmax over the keys\n",
    "            #  so dim=2\n",
    "            attention_pattern = torch.nn.Softmax(dim=2)(attention_pattern)\n",
    "            # Multiply by V\n",
    "            out = einsum('b n k q, b n k h -> b n q h', attention_pattern, V)\n",
    "            out = rearrange(out, 'batch num_heads seq_len head_size -> batch seq_len (num_heads head_size)')\n",
    "            out = self.outputLL(out)\n",
    "            if return_key_values:\n",
    "                assert x.shape[0] == 1\n",
    "                return out, torch.cat((K,V), dim = 3) \n",
    "            else:\n",
    "                return out\n",
    "        else:\n",
    "            assert x.shape == (1,1,self.hidden_size)\n",
    "            kqv = self.attentionLL(x)\n",
    "            kqv = rearrange(kqv, \"batch seq_len (three num_heads head_size) -> batch num_heads seq_len head_size three \", num_heads=self.num_heads, three=3)\n",
    "            q = kqv[0, :, :, :, 0]\n",
    "            k = kqv[0, :, :, :, 1]\n",
    "            v = kqv[0, :, :, :, 2]\n",
    "            oldK, oldV = torch.split(past_key_values, (self.head_size, self.head_size), dim = 2)\n",
    "            K = torch.cat((oldK, k), dim = 1)\n",
    "            V = torch.cat((oldV, v), dim = 1)\n",
    "            attention_pattern = einsum('n s h, n t h -> n s t', q, K)\n",
    "            attention_pattern = attention_pattern / math.sqrt(self.head_size)\n",
    "            attention_pattern = torch.nn.Softmax(dim=2)(attention_pattern)\n",
    "            out = einsum('n s t, n t h -> n s h', attention_pattern, V)\n",
    "            out = rearrange(out, '(batch num_heads) seq_len head_size -> batch seq_len (num_heads head_size)', batch = 1)\n",
    "            out = self.outputLL(out)\n",
    "            if return_key_values:\n",
    "                return out, torch.cat((k,v), dim = 2).unsqueeze(0)\n",
    "            else:\n",
    "                return out\n",
    "\n",
    "gpt_tests.test_attn_cache(UnidirectionalMultiheadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, \n",
    "                dropout: float, layer_norm_epsilon: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = UnidirectionalMultiheadAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, past_key_values = None, return_key_values = False):\n",
    "        if return_key_values:\n",
    "            res = x\n",
    "            x, keyvals = self.attn(self.ln1(x), past_key_values=past_key_values, return_key_values = True)\n",
    "            x = x + res\n",
    "            x = x + self.dropout(self.linear2(torch.nn.functional.gelu(self.linear1(self.ln2(x)))))\n",
    "            return x, keyvals\n",
    "        else:\n",
    "            x = x + self.attn(self.ln1(x), past_key_values=past_key_values, return_key_values = False)\n",
    "            x = x + self.dropout(self.linear2(torch.nn.functional.gelu(self.linear1(self.ln2(x)))))\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 213.333s to generate a 500-token sentence without cache and 38.479s with cache.\n"
     ]
    }
   ],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size,\n",
    "                hidden_size, max_position_embeddings, dropout, \n",
    "                layer_norm_epsilon, use_cache=False):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.GPTBlocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon) \n",
    "                for i in range(num_layers)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, layer_norm_epsilon)\n",
    "        self.use_cache = use_cache\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.past_key_values = torch.zeros((num_layers, num_heads, 0, 2*self.head_size))\n",
    "\n",
    "    def forward(self, input_ids): # [batch, seq_len]\n",
    "        if not self.use_cache:\n",
    "            tokens = self.token_embedding(input_ids)\n",
    "            batch, seq_len = input_ids.shape\n",
    "            position_ids = repeat(torch.arange(seq_len), 's -> b s', b = batch) \n",
    "            positions = self.position_embedding(position_ids)\n",
    "            embedding = tokens + positions\n",
    "            x = self.dropout(embedding)\n",
    "            x = self.GPTBlocks(x)\n",
    "            self.last_token_encodings = x\n",
    "            final_encodings = self.layer_norm(x)[:,-1,:]\n",
    "            logits = einsum('b c, v c -> b v', final_encodings, self.token_embedding.weight)\n",
    "            return GPT2Output(logits, final_encodings)\n",
    "        else:\n",
    "            if self.past_key_values.shape[2] == 0:\n",
    "                tokens = self.token_embedding(input_ids)\n",
    "                batch, seq_len = input_ids.shape\n",
    "                position_ids = repeat(torch.arange(seq_len), 's -> b s', b = batch) \n",
    "                positions = self.position_embedding(position_ids)\n",
    "                embedding = tokens + positions\n",
    "                x = self.dropout(embedding)\n",
    "                new_key_values = []\n",
    "                for gptblock in self.GPTBlocks:\n",
    "                    x, new_key_value = gptblock(x, return_key_values = True)\n",
    "                    new_key_values.append(new_key_value)\n",
    "                self.past_key_values = torch.cat(new_key_values, dim=0)\n",
    "                final_encodings = self.layer_norm(x)[:,-1,:]\n",
    "                logits = einsum('b c, v c -> b v', final_encodings, self.token_embedding.weight)\n",
    "                return GPT2Output(logits, final_encodings)\n",
    "            else:\n",
    "                tokens = self.token_embedding(input_ids[:,-1:])\n",
    "                batch, seq_len = input_ids.shape\n",
    "                position_ids = repeat(torch.arange(seq_len), 's -> b s', b = batch) \n",
    "                positions = self.position_embedding(position_ids[:,-1:])\n",
    "                embedding = tokens + positions\n",
    "                x = self.dropout(embedding)\n",
    "                new_key_values = []\n",
    "                for i,gptblock in enumerate(self.GPTBlocks):\n",
    "                    x, new_key_value = gptblock(x, \n",
    "                            past_key_values = self.past_key_values[i,:,:,:], \n",
    "                            return_key_values = True)\n",
    "                    new_key_values.append(new_key_value)\n",
    "                new_key_values = torch.cat(new_key_values, dim = 0)\n",
    "                self.past_key_values = torch.cat((self.past_key_values, new_key_values), dim=2)\n",
    "                final_encodings = self.layer_norm(x)[:,-1,:]\n",
    "                logits = einsum('b c, v c -> b v', final_encodings, self.token_embedding.weight)\n",
    "                return GPT2Output(logits, final_encodings)\n",
    "\n",
    "gpt_tests.test_gpt_cache(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, \n",
    "    hidden_size=768, max_position_embeddings=1024, dropout=0.1, \n",
    "    layer_norm_epsilon=1e-5)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = {}\n",
    "pretrained_dict = pretrained_gpt.state_dict()\n",
    "for a,b in zip(pretrained_dict.keys(), my_gpt.state_dict().keys()):\n",
    "    new_dict[b] = pretrained_dict[a] \n",
    "my_gpt.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
