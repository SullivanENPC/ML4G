{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from days.w2d4.attention import *\n",
    "import torch as t\n",
    "from torch import einsum\n",
    "from torch.nn import Module\n",
    "import einops\n",
    "from typing import Callable\n",
    "from torchtyping import patch_typeguard, TensorType\n",
    "import numpy as np\n",
    "import days.w2d1.bert_tests as tests\n",
    "import torchtext\n",
    "import gin\n",
    "import transformers\n",
    "from bert_finetuning import *\n",
    "from torch.profiler import profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 256\n",
    "batch_size = 4\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root=\".data\", split = (\"train\", \"test\"))\n",
    "train_batches = extract_data(data_train, max_seq_len, batch_size, tokenizer)\n",
    "test_batches = extract_data(data_test, max_seq_len, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class_bert = setup_bert()\n",
    "class_bert.train()\n",
    "class_bert.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels = next(train_batches)\n",
    "tokens_cuda = tokens.input_ids.cuda()\n",
    "\n",
    "with profile(with_stack=True, profile_memory=True) as prof:\n",
    "        outputs = class_bert(tokens_cuda)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Source Location                                                              \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "                                          aten::one_hot         0.08%     166.000us         1.02%       2.132ms       2.132ms       0.000us         0.00%     404.000us     404.000us           0 b           0 b     226.53 Mb           0 b             1  /home/ubuntu/mlab/days/w2d4/attention.py(182): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(195): bert_embedding                \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(221): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                            aten::zeros         0.05%     109.000us         0.45%     948.000us     948.000us       0.000us         0.00%     397.000us     397.000us           0 b           0 b     226.53 Mb           0 b             1  /home/ubuntu/mlab/days/w2d4/attention.py(182): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(195): bert_embedding                \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(221): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                            aten::empty         0.04%      88.000us         0.33%     688.000us     688.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     226.53 Mb     226.53 Mb             1  /home/ubuntu/mlab/days/w2d4/attention.py(182): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(195): bert_embedding                \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(221): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                           aten::einsum         3.83%       8.007ms        14.08%      29.484ms       2.457ms       0.000us         0.00%       1.103ms      91.917us           0 b           0 b     216.00 Mb           0 b            12  ...untu/.local/lib/python3.8/site-packages/torch/functional.py(327): einsum  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(55): raw_attention_pattern          \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(114): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(165): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                            aten::empty         0.97%       2.038ms         1.76%       3.678ms     102.167us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     216.00 Mb     216.00 Mb            36  ...untu/.local/lib/python3.8/site-packages/torch/functional.py(327): einsum  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(55): raw_attention_pattern          \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(114): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(165): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                           aten::linear         1.80%       3.767ms         9.82%      20.563ms     856.792us       0.000us         0.00%      11.585ms     482.708us           0 b           0 b     180.00 Mb           0 b            24  .../.local/lib/python3.8/site-packages/torch/nn/functional.py(1848): linear  \n",
      "                                                                                                                                                                                                                                                             ...cal/lib/python3.8/site-packages/torch/nn/modules/linear.py(103): forward  \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(127): bert_mlp                      \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(138): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                           aten::matmul         2.05%       4.283ms         5.77%      12.074ms     503.083us       0.000us         0.00%      10.888ms     453.667us           0 b           0 b     180.00 Mb           0 b            24  .../.local/lib/python3.8/site-packages/torch/nn/functional.py(1848): linear  \n",
      "                                                                                                                                                                                                                                                             ...cal/lib/python3.8/site-packages/torch/nn/modules/linear.py(103): forward  \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(127): bert_mlp                      \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(138): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                               aten::mm         1.62%       3.386ms         2.90%       6.061ms     252.542us      10.888ms        28.97%      10.888ms     453.667us           0 b           0 b     180.00 Mb     180.00 Mb            24  .../.local/lib/python3.8/site-packages/torch/nn/functional.py(1848): linear  \n",
      "                                                                                                                                                                                                                                                             ...cal/lib/python3.8/site-packages/torch/nn/modules/linear.py(103): forward  \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(127): bert_mlp                      \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(138): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                              aten::bmm         0.91%       1.899ms         1.92%       4.016ms     334.667us     660.000us         1.76%     660.000us      55.000us           0 b           0 b     144.00 Mb           0 b            12  ...untu/.local/lib/python3.8/site-packages/torch/functional.py(327): einsum  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(55): raw_attention_pattern          \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(114): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(165): forward                       \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "                                              aten::div         0.86%       1.798ms         1.71%       3.575ms     297.917us     565.000us         1.50%     565.000us      47.083us           0 b           0 b     144.00 Mb     144.00 Mb            12  /home/ubuntu/mlab/days/w2d4/attention.py(55): raw_attention_pattern          \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(114): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                             /home/ubuntu/mlab/days/w2d4/attention.py(165): forward                       \n",
      "                                                                                                                                                                                                                                                             ...lib/python3.8/site-packages/torch/nn/modules/module.py(1102): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                                                                          \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "Self CPU time total: 209.330ms\n",
      "Self CUDA time total: 37.587ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=10).table(sort_by=\"cuda_memory_usage\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat, empty = sorted(prof.key_averages(group_by_stack_n=10), key=lambda e:e.cuda_memory_usage, reverse=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.002252 STD: 0.12 VALS [-0.09587 -0.2646 0.1785 0.04304 0.002363 0.04754 0.1104 0.03812 -0.09448 0.02245...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 5, 1), (1200, 120, 30, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = t.randn((10,4,5))\n",
    "x2 = x1.repeat(1,1,1,6)\n",
    "x1.stride(), x2.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4808905216, 4831009984)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.data_ptr(), x2.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
