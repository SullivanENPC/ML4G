{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "attention pattern raw MATCH!!!!!!!!\n SHAPE (2, 12, 3, 3) MEAN: 0.007011 STD: 0.115 VALS [0.07875 0.1818 0.1318 0.1061 -0.176 0.1207 -0.04416 -0.08958 -0.008028 -0.03693...]\n"
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    K = rearrange(project_key(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "    Q = rearrange(project_query(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "\n",
    "    KbyQ = t.einsum(\"bsnh,btnh -> bnst\", K, Q)\n",
    "\n",
    "    d_k = token_activations.shape[2]/num_heads\n",
    "\n",
    "    out = KbyQ/t.sqrt(t.tensor([d_k]))\n",
    "    return out\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "attention MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.002339 STD: 0.1166 VALS [0.106 -0.1369 -0.0374 -0.0001321 -0.07958 0.09887 0.02255 0.05939 0.086 0.1509...]\n"
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    s = t.nn.Softmax(dim=2)\n",
    "\n",
    "    out = s(attention_pattern) # batch_size, head_num, key_token, query_token\n",
    "\n",
    "    out = rearrange(out, \"b n k q -> b n q k 1\")\n",
    "\n",
    "    V = rearrange(project_value(token_activations), \"b k (n h) -> b n 1 k h\", n = num_heads) # batch_size, num_heads, 1, key, head_size\n",
    "\n",
    "    out = einsum(\"bnqkh,bnqkh -> bnqh\", out, V)\n",
    "\n",
    "    out = rearrange(out, \"b n q h -> b q (n h)\")\n",
    "\n",
    "    return project_output(out)\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "\n",
    "        hidden_dim = num_heads * hidden_size\n",
    "\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_scores = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "\n",
    "        attention = bert_attention(input, self.num_heads, attention_scores, self.value, self.output)\n",
    "\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert mlp MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.003054 STD: 0.1041 VALS [0.1262 0.01134 0.06912 0.05845 0.06832 0.06498 -0.07017 -0.1155 -0.004871 0.2145...]\n"
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    out = linear_1(token_activations)\n",
    "    out = t.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear_1 = t.nn.Linear(input,intermediate_size)\n",
    "        self.linear_2 = t.nn.Linear(intermediate,input)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return bert_mlp(input,self.linear_1,self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "EinopsError",
     "evalue": " Error while processing sum-reduction pattern \"...d -> ...\".\n Input tensor shape: torch.Size([3, 5, 6]). Additional info: {}.\n Invalid axis identifier: …d\nnot a valid python identifier",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mhashable_axes_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashable_axes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/einops/einops.py\u001b[0m in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_lengths)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'->'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParsedExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0mrght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParsedExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrght\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/einops/parsing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'() '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0madd_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mcurrent_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/einops/parsing.py\u001b[0m in \u001b[0;36madd_axis_name\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_number\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_axis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis identifier: {}\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_number\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m: Invalid axis identifier: …d\nnot a valid python identifier",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-fe9c0de23c98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"...d -> ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mother_a_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_red\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n Input is list. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'Additional info: {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing sum-reduction pattern \"...d -> ...\".\n Input tensor shape: torch.Size([3, 5, 6]). Additional info: {}.\n Invalid axis identifier: …d\nnot a valid python identifier"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "layer norm MATCH!!!!!!!!\n SHAPE (20, 10) MEAN: 4.768e-09 STD: 1.003 VALS [-0.94 -1.641 -0.1301 -0.3103 1.493 -0.2086 -0.1952 -0.2518 1.973 0.2104...]\n"
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mean = input.mean(-1).unsqueeze(-1)\n",
    "        mean.detach()\n",
    "        stdev = input.std(-1,unbiased = False).unsqueeze(-1)\n",
    "        stdev.detach()\n",
    "        out = (input - mean)/stdev\n",
    "        return out*self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}