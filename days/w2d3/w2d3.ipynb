{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "from torchtyping import TensorType\n",
    "import einops\n",
    "import transformers\n",
    "import gpt_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.linear_attn = torch.nn.Linear(hidden_size, 3 * hidden_size)\n",
    "        self.linear_output = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch_size\", \"seq_len\", \"hidden_size\"]):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        attn_concat = self.linear_attn(x)\n",
    "\n",
    "        q, k, v = torch.split(attn_concat, self.hidden_size, dim=-1)\n",
    "        q = einops.rearrange(q, \"b n (h l) -> b h n l\", l=self.head_size)\n",
    "        k = einops.rearrange(k, \"b n (h l) -> b h n l\", l=self.head_size)\n",
    "        v = einops.rearrange(v, \"b n (h l) -> b h n l\", l=self.head_size)\n",
    "        \n",
    "        attn_raw = torch.einsum(\"bhts, bhfs -> bhtf\", q, k)\n",
    "\n",
    "        neg_inf = torch.tensor(-1e4)\n",
    "        attn_mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1)\n",
    "        attn_masked = torch.where(attn_mask, neg_inf, attn_raw) / math.sqrt(self.head_size)\n",
    "        attn_scores = torch.softmax(attn_masked, dim=-1)\n",
    "\n",
    "        attn = torch.einsum(\"bhtf, bhfs -> bhts\", attn_scores, v)\n",
    "        attn = einops.rearrange(attn, \"b h n l -> b n (h l)\")\n",
    "\n",
    "        return self.linear_output(attn)\n",
    "\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(MultiheadAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2Block(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        dropout: float,\n",
    "        layer_norm_epsilon: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_1 = torch.nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiheadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.norm_2 = torch.nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.linear_1 = torch.nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.linear_2 = torch.nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch_size\", \"seq_len\", \"hidden_size\"]):\n",
    "        normed_1 = self.norm_1(x)\n",
    "        attn = self.attn(normed_1)\n",
    "        attn_resid = x + attn\n",
    "        normed_2 = self.norm_2(attn_resid)\n",
    "        linear_1 = self.linear_1(normed_2)\n",
    "        gelu = torch.nn.functional.gelu(linear_1)\n",
    "        linear_2 = self.linear_2(gelu)\n",
    "        dropout = self.dropout(linear_2)\n",
    "        return dropout + attn_resid\n",
    "\n",
    "\n",
    "gpt_tests.test_gpt_block(GPT2Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        max_position_embeddings,\n",
    "        dropout,\n",
    "        layer_norm_epsilon,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embeddings = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embeddings = torch.nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.gpt_blocks = torch.nn.ModuleList([\n",
    "            GPT2Block(hidden_size=hidden_size, num_heads=num_heads, dropout=dropout, layer_norm_epsilon=layer_norm_epsilon)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = torch.nn.LayerNorm(hidden_size)\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    def forward(self, input_ids: TensorType[\"batch_size\", \"seq_len\"]):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        pos_ids = torch.arange(seq_len)\n",
    "        x = self.token_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.gpt_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        final_encoding = x[:, -1]\n",
    "        logits = final_encoding @ self.token_embeddings.weight.T\n",
    "\n",
    "        return GPT2Output(\n",
    "            logits=logits,\n",
    "            final_encoding=final_encoding,\n",
    "        )\n",
    "    \n",
    "    def next_token(\n",
    "        self,\n",
    "        input_ids: TensorType[\"seq_len\"],\n",
    "        temperature: float,\n",
    "        freq_penalty: float = 2.0\n",
    "    ) -> TensorType[\"vocab_size\"]:\n",
    "        output = self.forward(input_ids)\n",
    "        id_frequencies = torch.bincount(input_ids.squeeze(0), minlength=self.vocab_size).unsqueeze(0)\n",
    "        return torch.nn.functional.softmax(output.logits / temperature - id_frequencies * freq_penalty)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_length: int = 30,\n",
    "        temperature: float = 1.0,\n",
    "        freq_penalty: float = 2.0,\n",
    "    ):\n",
    "        output_ids = torch.tensor([self.tokenizer(text).input_ids])\n",
    "        seq_len = output_ids.shape[1]\n",
    "\n",
    "        while seq_len < max_length:\n",
    "            output_logits = self.next_token(output_ids, temperature, freq_penalty)\n",
    "            next_token_id = torch.argmax(output_logits, dim=-1, keepdim=True)\n",
    "            output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "            seq_len = output_ids.shape[1]\n",
    "\n",
    "            if next_token_id == self.tokenizer.eos_token_id:\n",
    "                break  # reached end of sentence\n",
    "        \n",
    "        return self.tokenizer.batch_decode(output_ids)\n",
    "\n",
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_gpt = GPT2(\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    vocab_size=50257,\n",
    "    hidden_size=768,\n",
    "    max_position_embeddings=1024,\n",
    "    dropout=0.1,\n",
    "    layer_norm_epsilon=1e-5,\n",
    ")\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_our_model_and_theirs(our_model, their_model):\n",
    "    for our_key, their_key in zip(our_model.state_dict(), their_model.state_dict()):\n",
    "        print(f\"{our_key:50} {their_key}\")\n",
    "\n",
    "\n",
    "# align_our_model_and_theirs(our_gpt, pretrained_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_their_state_to_ours(our_model, their_model):\n",
    "    our_new_state_dict = our_model.state_dict()\n",
    "    for k, v in zip(our_new_state_dict.keys(), their_model.state_dict().values()):\n",
    "        our_new_state_dict[k] = v\n",
    "    our_model.load_state_dict(our_new_state_dict)\n",
    "\n",
    "\n",
    "copy_their_state_to_ours(our_gpt, pretrained_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I am a dog lover. I love to play with my dogs and have been doing it for years.\\n\\nMy husband is an avid dog lover']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_gpt.generate(\"I am a dog\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3cfef2114ea42ecc59022793481ca7184625663f643bce446a46eafcd379519"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('mlab-gX83EaMB-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
