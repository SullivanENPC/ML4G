{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import einops\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "def mlp_model(obs_size, action_size, hidden_size):\n",
    "    return nn.Sequential(CastToFloat(), nn.Linear(obs_size, hidden_size),\n",
    "                         nn.ReLU(), nn.Linear(hidden_size, hidden_size),\n",
    "                         nn.ReLU(), dqn_head(hidden_size, action_size))\n",
    "\n",
    "def atari_model(obs_n_channels, action_size):\n",
    "    return nn.Sequential(Rearrange(\"n h w c -> n c h w\"), PixelByteToFloat(),\n",
    "                         nn.Conv2d(obs_n_channels, 32, 8, stride=4), nn.ReLU(),\n",
    "                         nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "                         nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n",
    "                         nn.Flatten(), dqn_head(3136, action_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import einops\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_probs(model, states):\n",
    "    return torch.distributions.categorical.Categorical(model(states))\n",
    "\n",
    "def batch_loss(model, states, actions, rewards):\n",
    "    logprob = action_probs(model, states).log_prob(actions)\n",
    "    return (-1.0 * logprob * rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss(states, rewards, value_fn, discount_factor):\n",
    "    \"\"\"\n",
    "    value_fn takes in a tensor of dim 1?? and applies value function element wise\n",
    "    values is a tensor of \n",
    "    \"\"\"\n",
    "    values = value_fn(states)\n",
    "    tds = discount_factor * values[1:] + rewards[:-1] - values[:-1]\n",
    "    tds = torch.pow(gamma_lambda, torch.arange(values.shape[-1] - 1)) * tds\n",
    "    return torch.sum(tds, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_name, model):\n",
    "    env = gym.make(env_name)\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action = action_probs(model, state).sample().item()\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state = torch.tensor(state_next, dtype=torch.float32)\n",
    "        ep_reward += reward\n",
    "        \n",
    "    return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env_name, num_epochs, batch_size, lr, hidden_size=64, eval_episodes=10,\n",
    "    rewards_to_go=False, gen_adv_est=False\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    max_steps = num_epochs * batch_size\n",
    "    steps = 0\n",
    "    in_size = env.reset().shape[0]\n",
    "    out_size = env.action_space.n\n",
    "    model = PGNet(in_size, hidden_size, out_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    final_ep_rewards = []\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        done = False\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        ep_rewards = []\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "        \n",
    "        for step in range(batch_size):\n",
    "            action = action_probs(model, state).sample().item()\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            ep_states.append(state)\n",
    "            ep_actions.append(action)\n",
    "            ep_rewards.append(reward)\n",
    "            if done:\n",
    "                final_ep_rewards.append(sum(ep_rewards))\n",
    "                if rewards_to_go:\n",
    "                    ep_rewards = (np.cumsum(ep_rewards[::-1])[::-1] - ep_rewards).tolist()\n",
    "                else:\n",
    "                    ep_rewards = [sum(ep_rewards)] * len(ep_rewards)\n",
    "                batch_states.extend(ep_states)\n",
    "                batch_actions.extend(ep_actions)\n",
    "                batch_rewards.extend(ep_rewards)\n",
    "                ep_states = []\n",
    "                ep_actions = []\n",
    "                ep_rewards = []\n",
    "                state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "                done = False\n",
    "            else:\n",
    "                state = torch.tensor(state_next, dtype=torch.float32)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        batch_states = torch.stack(batch_states)\n",
    "        batch_actions = torch.tensor(batch_actions, dtype=torch.float32)\n",
    "        batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
    "        L = batch_loss(model, batch_states, batch_actions, batch_rewards)\n",
    "        if gen_adv_est:\n",
    "            L += value_loss(\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        eval_rewards = []\n",
    "        for ep in range(eval_episodes):\n",
    "            reward = evaluate(env_name, model)\n",
    "            eval_rewards.append(reward)\n",
    "        mean_reward = sum(eval_rewards) / eval_episodes\n",
    "        print(f\"epoch {epoch}: reward {mean_reward}\")\n",
    "        \n",
    "    num_groups = 200\n",
    "    eps_per_group = len(final_ep_rewards) // num_groups\n",
    "    final_ep_rewards = [\n",
    "        sum(final_ep_rewards[i*eps_per_group : (i+1)*eps_per_group]) / eps_per_group \n",
    "        for i in range(num_groups)\n",
    "    ]\n",
    "    plt.plot(final_ep_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- could predict returns\n",
    "- estimating based on its own thing\n",
    "- takes longer to train because it has high variance\n",
    "- if you don't actually get the reward, you won't learn that you were close to it\n",
    "- if you're trying to train a value function, why not use 2 steps rather than 1 step? \n",
    "  - TD1 is one step\n",
    "  - TD-2 train on two steps\n",
    "- value function is biased because you're just approximating it\n",
    "- TD-N is N steps\n",
    "  - higher Ns will learn faster about events later in the episode\n",
    "  - unbiased because it's not an \"estimate of the value function\"\n",
    "  - but it makes things harder to learn because you're not caching previous information\n",
    "- monte carlo is just every future reward summed up\n",
    "- if you use a long N, you're going to get a lot of noise\n",
    "  - but a 1 step estimator \n",
    "- TD-N is closer to TD-infinity, whihc is monte carlo\n",
    "- you could weight between many TD-n's using TD-lambda\n",
    "  - exponential weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    \"CartPole-v1\", \n",
    "    num_epochs=10,\n",
    "    batch_size=5_000,\n",
    "    lr=1e-2,\n",
    "    hidden_size=64,\n",
    "    eval_episodes=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Policy Gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
