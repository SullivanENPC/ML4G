{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.006603 STD: 0.1129 VALS [0.05867 0.01584 -0.05108 -0.212 -0.2484 0.05948 0.04164 0.0499 -0.05705 0.1175...]\n",
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.0001962 STD: 0.1178 VALS [-0.1035 0.03562 -0.07408 0.1086 0.01748 0.05069 -0.14 -0.01052 0.01697 0.04285...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n",
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n",
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5117 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.139e-09 STD: 1 VALS [0.007131 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n",
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n",
      "True\n",
      "True\n",
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [0.2316 0.08455 -0.5146 1.436 2.029 -1.117 2.775 -0.5305 -0.4485 -0.2485...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.4321 0.1186 -0.7165 -0.5262 0.4967 1.223 0.3165 -0.3247 -0.5717...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "error in comparing Berts\n SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.414 VALS [-5.65 -6.041 -6.096 -6.062 -5.945 -5.777 -5.977 -6.015 -6.028 -5.935...] \n!=\n SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/mlab/days/w2d1d2/bert.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmy_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbert_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_same_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlab/days/w2d1d2/bert_tests.py\u001b[0m in \u001b[0;36mtest_same_output\u001b[0;34m(your_bert, pretrained_bert)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myour_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     allclose(your_bert.eval()(input_ids),\n\u001b[0m\u001b[1;32m    170\u001b[0m              \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m              'comparing Berts', tol=1e-4)\n",
      "\u001b[0;32m~/mlab/days/w2d1d2/bert_tests.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(my_out, their_out, name, tol)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheir_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0merrstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'error in {name}\\n{tpeek(\"\", my_out, ret=True)} \\n!=\\n{tpeek(\"\", their_out, ret=True)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name} MATCH!!!!!!!!\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: error in comparing Berts\n SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.414 VALS [-5.65 -6.041 -6.096 -6.062 -5.945 -5.777 -5.977 -6.015 -6.028 -5.935...] \n!=\n SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]"
     ]
    }
   ],
   "source": [
    "%run bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert():\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "        num_heads=12, num_layers=12\n",
    "    )\n",
    "    pretrained_state_dict = pretrained_bert.state_dict()\n",
    "\n",
    "    #pretrained_keys = pretrained_state_dict.keys()\n",
    "    #my_keys = my_bert.state_dict().keys()\n",
    "    #print(set(my_keys) - set(pretrained_keys))\n",
    "\n",
    "    del pretrained_state_dict[\"classification_head.weight\"]\n",
    "    del pretrained_state_dict[\"classification_head.bias\"]\n",
    "\n",
    "    my_bert.load_state_dict(pretrained_state_dict)\n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 0.12), ('fish', 0.12), ('meat', 0.08), ('eggs', 0.05), ('food', 0.05)]\n",
      "[('fish', 0.07), ('meat', 0.05), ('eggs', 0.03), ('food', 0.02), ('bread', 0.02)]\n",
      "[('fish', 0.05), ('eggs', 0.04), ('meat', 0.04), ('fruit', 0.02), ('food', 0.02)]\n",
      "[('fish', 0.04), ('meat', 0.02), ('salt', 0.01), ('wine', 0.01), ('food', 0.01)]\n",
      "[('fish', 0.04), ('meat', 0.02), ('eggs', 0.02), ('birds', 0.02), ('food', 0.01)]\n",
      "[('fish', 0.04), ('food', 0.02), ('meat', 0.02), ('birds', 0.01), ('water', 0.01)]\n",
      "[('fish', 0.03), ('birds', 0.02), ('more', 0.02), ('honey', 0.01), ('food', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    bert = make_bert()\n",
    "    token_ids = torch.Tensor(tokenizer.encode(sentence)).long().unsqueeze(0)\n",
    "    mask_idxs = set()\n",
    "    for idx, token_id in enumerate(token_ids[0]):\n",
    "        if token_id == 103: # 103 == [MASK]\n",
    "            mask_idxs.add(idx)\n",
    "    unnormalized_output = bert(token_ids)\n",
    "    output = torch.log_softmax(unnormalized_output, dim=-1)\n",
    "    top_k = torch.topk(output, 5, dim=-1)\n",
    "    results = []\n",
    "    for seq_i, seq_top_k in enumerate(top_k.indices[0]):\n",
    "        if seq_i in mask_idxs:\n",
    "            results.append(list(zip(tokenizer.convert_ids_to_tokens(seq_top_k), [round(x.item(), 2) for x in top_k.values[0][seq_i].exp()])))\n",
    "    for l in results:\n",
    "        print(l)\n",
    "\n",
    "ascii_art_probs(\"The fish loves to eat [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationBERT():\n",
    "    def __init__(self, num_classes):\n",
    "        VOCAB_SIZE = 28996\n",
    "        DROPOUT=0.1\n",
    "        self.bert = Bert(\n",
    "            vocab_size=VOCAB_SIZE, hidden_size=768, max_position_embeddings=512, \n",
    "            type_vocab_size=2, dropout=DROPOUT, intermediate_size=3072, \n",
    "            num_heads=12, num_layers=12\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "        # TODO attach this to the last block of BERT as a hidden_size x num_classes linear layer,\n",
    "        # not on top of the vocab_size output of BERT\n",
    "        self.classification_layer = torch.nn.Linear(VOCAB_SIZE, num_classes)\n",
    "\n",
    "        pretrained_state_dict = pretrained_bert.state_dict()\n",
    "        del pretrained_state_dict[\"classification_head.weight\"]\n",
    "        del pretrained_state_dict[\"classification_head.bias\"]\n",
    "        self.bert.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classification_layer(self.dropout(self.bert(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train_list = list(data_train)\n",
    "random.shuffle(data_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(tensor([[  101.,   107., 12510.,   102.],\n          [  101.,  1258.,  3195.,   102.],\n          [  101.,   146.,  1821.,   102.],\n          [  101.,  1409.,  1128.,   102.],\n          [  101.,  1258.,   107.,   102.],\n          [  101.,  1332.,  1122.,   102.],\n          [  101., 22449.,   170.,   102.]]),\n  tensor([0, 1, 1, 1, 0, 1, 0], dtype=torch.int8)),\n (tensor([[  101.,   146.,  1138.,   102.],\n          [  101.,   146.,   112.,   102.],\n          [  101.,  2048.,   117.,   102.],\n          [  101.,  9498.,  1103.,   102.],\n          [  101., 12107.,  5253.,   102.],\n          [  101.,   178.,  8703.,   102.],\n          [  101.,   146.,  1108.,   102.]]),\n  tensor([0, 0, 1, 0, 1, 0, 0], dtype=torch.int8)),\n (tensor([[  101.,  1109., 22891.,   102.],\n          [  101.,   178.,  2140.,   102.],\n          [  101.,  2907.,  1227.,   102.],\n          [  101., 13411., 10197.,   102.],\n          [  101.,  1258., 10761.,   102.],\n          [  101., 15783.,   161.,   102.],\n          [  101.,  3100., 15198.,   102.]]),\n  tensor([0, 1, 1, 0, 0, 1, 0], dtype=torch.int8)),\n (tensor([[  101.,  1188.,  1110.,   102.],\n          [  101.,  1109., 13664.,   102.],\n          [  101.,  1188.,  1110.,   102.],\n          [  101.,   146.,  1525.,   102.],\n          [  101.,   146.,  2542.,   102.],\n          [  101.,   146.,  2566.,   102.],\n          [  101.,  5823.,  2542.,   102.]]),\n  tensor([0, 1, 1, 0, 0, 0, 0], dtype=torch.int8)),\n (tensor([[ 101.,  146., 1458.,  102.],\n          [ 101.,  146., 1108.,  102.]]),\n  tensor([0, 1], dtype=torch.int8))]"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_batches(data_train_list, batch_size, max_seq_len):\n",
    "    batches = []\n",
    "    for batch_start in range(0, len(data_train_list), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(data_train_list))\n",
    "        ratings, sentences = [list(tup) for tup in list(zip(*data_train_list[batch_start:batch_end]))]\n",
    "        tokens = tokenizer(sentences, padding='longest', max_length=max_seq_len, truncation=True)\n",
    "        ratings01 = torch.tensor([ rating == 'pos' for rating in ratings ], dtype=torch.int8)\n",
    "        tokens_tensor = torch.Tensor(tokens.input_ids)\n",
    "        batches.append((tokens_tensor, ratings01))\n",
    "    return batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}