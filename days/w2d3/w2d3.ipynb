{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "from torchtyping import TensorType\n",
    "import einops\n",
    "import gpt_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.linear_attn = torch.nn.Linear(hidden_size, 3 * hidden_size)\n",
    "        self.linear_output = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch_size\", \"seq_len\", \"hidden_size\"]):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        attn_concat = self.linear_attn(x)\n",
    "\n",
    "        q, k, v = torch.split(attn_concat, self.hidden_size, dim=-1)\n",
    "        q = einops.rearrange(\n",
    "            q,\n",
    "            \"batch_size seq_len (num_heads head_size) -> batch_size num_heads seq_len head_size\",\n",
    "            head_size=self.head_size,\n",
    "        )\n",
    "        k = einops.rearrange(\n",
    "            k,\n",
    "            \"batch_size seq_len (num_heads head_size) -> batch_size num_heads seq_len head_size\",\n",
    "            head_size=self.head_size,\n",
    "        )\n",
    "        v = einops.rearrange(\n",
    "            v,\n",
    "            \"batch_size seq_len (num_heads head_size) -> batch_size num_heads seq_len head_size\",\n",
    "            head_size=self.head_size,\n",
    "        )\n",
    "\n",
    "        attn_raw = torch.einsum(\"bhts, bhfs -> bhtf\", q, k)\n",
    "\n",
    "        neg_inf = torch.tensor(-1e4)\n",
    "        attn_mask = torch.triu(\n",
    "            torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1\n",
    "        )\n",
    "        attn_masked = torch.where(attn_mask, neg_inf, attn_raw) / math.sqrt(\n",
    "            self.head_size\n",
    "        )\n",
    "        attn_scores = torch.softmax(attn_masked, dim=-1)\n",
    "\n",
    "        attn = torch.einsum(\"bhtf, bhfs -> bhts\", attn_scores, v)\n",
    "        attn = einops.rearrange(\n",
    "            attn,\n",
    "            \"batch_size num_heads seq_len head_size -> batch_size seq_len (num_heads head_size)\",\n",
    "        )\n",
    "\n",
    "        return self.linear_output(attn)\n",
    "\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(MultiheadAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2Block(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        dropout: float,\n",
    "        layer_norm_epsilon: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_1 = torch.nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiheadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.norm_2 = torch.nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.linear_1 = torch.nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.linear_2 = torch.nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch_size\", \"seq_len\", \"hidden_size\"]):\n",
    "        normed_1 = self.norm_1(x)\n",
    "        attn = self.attn(normed_1)\n",
    "        attn_resid = x + attn\n",
    "        normed_2 = self.norm_2(attn_resid)\n",
    "        linear_1 = self.linear_1(normed_2)\n",
    "        gelu = torch.nn.functional.gelu(linear_1)\n",
    "        linear_2 = self.linear_2(gelu)\n",
    "        dropout = self.dropout(linear_2)\n",
    "        return dropout + attn_resid\n",
    "\n",
    "\n",
    "gpt_tests.test_gpt_block(GPT2Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        max_position_embeddings,\n",
    "        dropout,\n",
    "        layer_norm_epsilon,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embeddings = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.pos_embeddings = torch.nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.gpt_blocks = torch.nn.ModuleList([\n",
    "            GPT2Block(hidden_size=hidden_size, num_heads=num_heads, dropout=dropout, layer_norm_epsilon=layer_norm_epsilon)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = torch.nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, input_ids: TensorType[\"batch_size\", \"seq_len\"]):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        pos_ids = torch.arange(seq_len)\n",
    "        x = self.token_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.gpt_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        final_encoding = x[:, -1]\n",
    "        logits = final_encoding @ self.token_embeddings.weight.T\n",
    "\n",
    "        return GPT2Output(\n",
    "            logits=logits,\n",
    "            final_encoding=final_encoding,\n",
    "        )\n",
    "\n",
    "\n",
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3cfef2114ea42ecc59022793481ca7184625663f643bce446a46eafcd379519"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('mlab-gX83EaMB-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
