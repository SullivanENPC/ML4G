{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gpt_tests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, reduce, repeat\n",
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "import sys\n",
    "sys.path.append('../w2d1')\n",
    "from bert_sol import Bert\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n",
      "Checking encoding:\n",
      "Congrats! You've passed the test!\n",
      "Checking new key and value:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class UniMultiAttention(t.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.attention = t.nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.softmax = t.nn.Softmax(dim=-1)\n",
    "        self.projection = t.nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, past_key_values=None, return_key_values=False): # [batch, seq_len, hidden_size]\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = t.split(past_key_values, self.head_size, dim=-1)\n",
    "            x = self.attention(x)\n",
    "            q, k, v = t.split(x, self.hidden_size, dim=-1)\n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)[0]\n",
    "            k = rearrange(k, \"b n (h c) -> b h n c\", h = self.num_heads)[0]\n",
    "            v = rearrange(v, \"b n (h c) -> b h n c\", h = self.num_heads)[0]\n",
    "            new_kv = t.cat((k, v), dim=-1).unsqueeze(0)\n",
    "            k = t.cat((past_k, k), dim=-2)\n",
    "            v = t.cat((past_v, v), dim=-2)\n",
    "            x = t.einsum(\"hnc,hmc->hnm\", q, k)\n",
    "            x /= np.sqrt(self.head_size)\n",
    "            x = t.einsum(\"hnm,hmc->hnc\", self.softmax(x), v)\n",
    "            res = rearrange(x, \"h n c -> 1 n (h c)\")\n",
    "        else:\n",
    "            x = self.attention(x)\n",
    "            q, k, v = t.split(x, self.hidden_size, dim=-1)\n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            k = rearrange(k, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            v = rearrange(v, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            x = t.einsum(\"bhnc,bhmc->bhnm\", q, k)\n",
    "            x /= np.sqrt(self.head_size)\n",
    "\n",
    "            mask = t.arange(0, x.shape[-1]) <= t.arange(0, x.shape[-1]).unsqueeze(1)\n",
    "            x = t.where(mask, x, t.tensor(-1e4, dtype=t.float))\n",
    "            x = t.einsum(\"bhnm,bhmc->bhnc\", self.softmax(x), v)\n",
    "            res = rearrange(x, \"b h n c -> b n (h c)\")\n",
    "        if return_key_values:\n",
    "            return (self.projection(res), new_kv)\n",
    "        return self.projection(res)\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(UniMultiAttention)\n",
    "gpt_tests.test_attn_cache(UniMultiAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2Block(t.nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float, layer_norm_epsilon: float) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm = t.nn.LayerNorm(hidden_size, layer_norm_epsilon)\n",
    "        self.attention = UniMultiAttention(hidden_size, num_heads)\n",
    "        self.layers2 = t.nn.Sequential(\n",
    "            t.nn.LayerNorm(hidden_size, layer_norm_epsilon),\n",
    "            t.nn.Linear(hidden_size, hidden_size * 4),\n",
    "            t.nn.GELU(),\n",
    "            t.nn.Linear(hidden_size*4, hidden_size),\n",
    "            t.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, past_key_values=None, return_key_values=False):\n",
    "        y = self.layer_norm(x)\n",
    "        if return_key_values:\n",
    "            y, kv = self.attention(y, past_key_values, return_key_values)\n",
    "        else:\n",
    "            y = self.attention(y)\n",
    "        z = x + y\n",
    "        z = z + self.layers2(z)\n",
    "        if return_key_values:\n",
    "            return z, kv\n",
    "        return z\n",
    "gpt_tests.test_gpt_block(GPT2Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"seq_length\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n",
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 2.176s to generate a 500-token sentence without cache and 1.873s with cache.\n"
     ]
    }
   ],
   "source": [
    "class GPT2(t.nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size, max_position_embeddings, dropout, layer_norm_epsilon, use_cache = False, tokenizer=None) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_layer = t.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding_layer = t.nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.gpt_layers = [GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon) for x in range(num_layers)]\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        self.layers = t.nn.Sequential(\n",
    "            *self.gpt_layers,\n",
    "        )\n",
    "        self.layer_norm = t.nn.LayerNorm(hidden_size, layer_norm_epsilon)\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cached_kv = t.zeros((num_layers, num_heads, 0, 2 * hidden_size // num_heads))\n",
    "        self.cached_encodings = t.zeros((0, hidden_size))\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.cached_kv = t.zeros((self.num_layers, self.num_heads, 0, 2 * self.hidden_size // self.num_heads))\n",
    "        self.cached_encodings = t.zeros((0, self.hidden_size))\n",
    "\n",
    "    def forward(self, input_ids, use_cache = False): # [batch, seq_len]\n",
    "        input_ids = self.token_embedding_layer(input_ids) + self.position_embedding_layer(t.arange(input_ids.shape[-1]))\n",
    "        input_ids = self.dropout(input_ids)\n",
    "        if use_cache:\n",
    "            # if self.cached_kv.shape[2] == 0: # cache empty\n",
    "\n",
    "            temp = t.zeros((1, self.num_heads, 0, 2 * self.hidden_size // self.num_heads))\n",
    "            for i, layer in enumerate(self.gpt_layers):\n",
    "                input_ids, kv = layer(input_ids, past_key_values = self.cached_kv[i], return_key_values = True)\n",
    "                temp = t.cat((temp, kv), dim=-2)\n",
    "            temp = rearrange(temp, \"1 heads (layers seq) kv -> layers heads seq kv\", layers=self.num_layers)\n",
    "            self.cached_kv = t.cat((self.cached_kv, temp), dim=-2)\n",
    "        else:\n",
    "            input_ids = self.layers(input_ids)\n",
    "        self._enc = input_ids\n",
    "        if use_cache:\n",
    "            self.cached_encodings = t.cat((self.cached_encodings, self._enc[0]), dim=0)\n",
    "            # input_ids = self.cached_encodings\n",
    "        input_ids = self.layer_norm(input_ids)\n",
    "        x = input_ids @ self.token_embedding_layer.weight.T\n",
    "        return GPT2Output(x[...,x.shape[-2]-1,:], input_ids[...,input_ids.shape[-2]-1,:])\n",
    "    \n",
    "    def next_token(self, input_ids, temperature, freq_penalty=2.0): # input_ids : [seq_len]\n",
    "        out = self.forward(input_ids, use_cache=True)\n",
    "        logits = out.logits[-1]\n",
    "        id_frequencies = t.zeros(logits.shape)\n",
    "        id_frequencies[input_ids] += 1\n",
    "        probs = t.softmax(logits/temperature - id_frequencies * freq_penalty, dim=-1)\n",
    "        return t.argmax(probs)\n",
    "    \n",
    "    def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
    "        self.clear_cache()\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        for i in range(max_length):\n",
    "            next = self.next_token(t.tensor(tokens).unsqueeze(0), temperature, freq_penalty)\n",
    "            tokens.append(next)\n",
    "            if next == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(tokens)\n",
    "\n",
    "\n",
    "\n",
    "gpt_tests.test_gpt(GPT2)\n",
    "gpt_tests.test_gpt_cache(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, tokenizer=tokenizer, use_cache = True)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len([x for x in my_gpt.state_dict()]))\n",
    "print(len([x for x in pretrained_gpt.state_dict()]))\n",
    "mapping = {x: y for x, y in zip(my_gpt.state_dict(), pretrained_gpt.state_dict())}\n",
    "state_dict = pretrained_gpt.state_dict()\n",
    "final_dict = {}\n",
    "for x, y in mapping.items():\n",
    "    final_dict[x] = state_dict[y]\n",
    "my_gpt.load_state_dict(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_gpt.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_gpt.generate(\"my life motto is: \", max_length = 60, freq_penalty=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is?\"\\n\\n\\n\\n\\n\"I am the name\\n\"is it?\\n\" is \"the name\\nThe name\\nIs it\\n\"'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.generate(\"Hello, my name is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am  a man.\n",
      "I'm a man.\n",
      "I am a man. I am a man. I am a man. I am a man. I am a man. I am a man. I am a man. I am a man. I am a man. I am a man\n"
     ]
    }
   ],
   "source": [
    "print(my_gpt.generate(\"Hello, I am \", max_length = 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am  a man.\n",
      "I'm a woman. And this is the one of course it's not so, and that she was born to be my wife who are both men were all women in her husband as well for me.\"<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(my_gpt.generate(\"Hello, I am \", max_length = 60, freq_penalty = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Bert(\n",
    "    num_layers=12, num_heads=12, vocab_size=bert_tokenizer.vocab_size, hidden_size=768, intermediate_size=768,\n",
    "    max_position_embeddings=1024, dropout=0.1, type_vocab_size=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_gpt.eval()\n",
    "\n",
    "def gpt_encodings(text):\n",
    "    my_gpt(t.tensor(tokenizer.encode(text)).unsqueeze(0))\n",
    "    return t.argmax(t.softmax(my_gpt._enc, dim=-1))\n",
    "\n",
    "def bert_encodings(text):\n",
    "    bert(t.tensor(bert_tokenizer.encode(text)).unsqueeze(0))\n",
    "    return t.softmax(bert._enc, dim=-1)\n",
    "\n",
    "def gpt_pred(text):\n",
    "    logits = my_gpt(t.tensor(tokenizer.encode(text)).unsqueeze(0)).logits\n",
    "    logits = t.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    print(logits)\n",
    "    print(t.argmax(logits))\n",
    "    print(logits[t.argmax(logits)])\n",
    "    return tokenizer.decode(t.argmax(logits))\n",
    "\n",
    "#gpt_pred(\"My life motto: \\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print([\" \".join([str(y) for y in x]) for x in gpt_encodings(\"My life motto:\")])\n",
    "# print\n",
    "# print(gpt_encodings(\"motto\"))\n",
    "\n",
    "# print(bert_encodings(\"My life motto:\"))\n",
    "# print(bert_encodings(\"motto\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"My life motto:\"\n",
    "s2 = \"My life motto: Fortune favors\"\n",
    "motto1 = gpt_encodings(s1)[:,1]\n",
    "motto2 = gpt_encodings(s2)[:,1]\n",
    "bmotto1 = bert_encodings(s1)[:,3]\n",
    "bmotto2 = bert_encodings(s2)[:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'motto'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(bert_tokenizer.encode(s1)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT tensor(True)\n",
      "BERT tensor(False)\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT\", (motto1 == motto2).all())\n",
    "print(\"BERT\", (bmotto1 == bmotto2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5171,  0.6661, -0.6133,  ..., -0.3239, -1.1028,  0.7504],\n",
       "         [-0.2633,  0.4830, -0.1012,  ...,  0.0960, -0.0229,  0.0053],\n",
       "         [-0.7542,  0.5852, -0.5270,  ...,  0.6227, -0.1321, -0.0600],\n",
       "         [-0.3993,  0.9214, -0.8342,  ..., -0.3123, -0.5261,  0.4948],\n",
       "         [-0.2962,  1.1724, -0.3336,  ..., -0.1051, -0.3679,  0.2982]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(t.tensor(bert_tokenizer.encode(\"testing testing a\")).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert._enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5661, 318, 257, 33600]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"this is a motto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motto1 = gpt_encodings(\"hello world\")\n",
    "motto2 = gpt_encodings(\"hello world how\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
