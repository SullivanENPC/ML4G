{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "from torch import nn \n",
    "from einops import rearrange, reduce, repeat\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import transformers\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gpt_tests\n",
    "import bert_sol\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class UnidirectionalMultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads \n",
    "        self.head_size = hidden_size // num_heads\n",
    "        assert self.head_size * num_heads == hidden_size\n",
    "        self.attentionLL = nn.Linear(hidden_size, num_heads*self.head_size*3) \n",
    "        self.outputLL = nn.Linear(num_heads*self.head_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor): # [batch, seq_length, hidden_size]\n",
    "        # Shape: batch seq_len hidden_size*3\n",
    "        KQV = self.attentionLL(x)\n",
    "        KQV = rearrange(KQV, \"batch seq_len (three num_heads head_size) -> batch num_heads seq_len head_size three \", num_heads=self.num_heads, three=3)\n",
    "        Q = KQV[:, :, :, :, 0]\n",
    "        K = KQV[:, :, :, :, 1]\n",
    "        V = KQV[:, :, :, :, 2]\n",
    "        # Multiplying K and Q\n",
    "        attention_pattern = einsum('b n s h, b n t h -> b n s t', K, Q)\n",
    "        # Scale\n",
    "        attention_pattern = attention_pattern / math.sqrt(self.head_size)\n",
    "        # Key (row) must be less than Query (col), if not we set it to 1e-4\n",
    "        # print(torch.triu(attention_pattern))\n",
    "        # print((-1e4) * torch.tril(torch.ones_like(attention_pattern), diagonal=-1))\n",
    "        attention_pattern = torch.triu(attention_pattern) + (-1e4) * torch.tril(torch.ones_like(attention_pattern), diagonal=-1)        \n",
    "        # Softmax: batch num_heads key_len query_len, so we want to softmax over the keys\n",
    "        #  so dim=2\n",
    "        attention_pattern = torch.nn.Softmax(dim=2)(attention_pattern)\n",
    "        # Multiply by V\n",
    "        out = einsum('b n k q, b n k h -> b n q h', attention_pattern, V)\n",
    "        out = rearrange(out, 'batch num_heads seq_len head_size -> batch seq_len (num_heads head_size)')\n",
    "        out = self.outputLL(out) \n",
    "        #print(out)\n",
    "        return out\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(UnidirectionalMultiheadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, \n",
    "                dropout: float, layer_norm_epsilon: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = UnidirectionalMultiheadAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.dropout(self.linear2(torch.nn.functional.gelu(self.linear1(self.ln2(x)))))\n",
    "        return x \n",
    "\n",
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n",
      "Checking logits:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size,\n",
    "                hidden_size, max_position_embeddings, dropout, \n",
    "                layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.GPTBlocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon) \n",
    "                for i in range(num_layers)]\n",
    "        )\n",
    "        self.last_token_encodings = None\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, input_ids): # [batch, seq_len]\n",
    "        tokens = self.token_embedding(input_ids)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        position_ids = repeat(torch.arange(seq_len), 's -> b s', b = batch) \n",
    "        positions = self.position_embedding(position_ids)\n",
    "        embedding = tokens + positions\n",
    "        x = self.dropout(embedding)\n",
    "        x = self.GPTBlocks(x)\n",
    "        self.last_token_encodings = x\n",
    "        final_encodings = self.layer_norm(x)[:,-1,:]\n",
    "        logits = einsum('b c, v c -> b v', final_encodings, self.token_embedding.weight)\n",
    "        return GPT2Output(logits, final_encodings)\n",
    "\n",
    "gpt_tests.test_gpt(GPT2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, \n",
    "    hidden_size=768, max_position_embeddings=1024, dropout=0.1, \n",
    "    layer_norm_epsilon=1e-5)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = {}\n",
    "pretrained_dict = pretrained_gpt.state_dict()\n",
    "for a,b in zip(pretrained_dict.keys(), my_gpt.state_dict().keys()):\n",
    "    new_dict[b] = pretrained_dict[a] \n",
    "my_gpt.load_state_dict(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My life motto: \"\n",
      "My life motto: \"Don\n",
      "My life motto: \"Don't\n",
      "My life motto: \"Don't be\n",
      "My life motto: \"Don't be afraid\n",
      "My life motto: \"Don't be afraid to\n",
      "My life motto: \"Don't be afraid to be\n",
      "My life motto: \"Don't be afraid to be yourself\n",
      "My life motto: \"Don't be afraid to be yourself.\"\n",
      "My life motto: \"Don't be afraid to be yourself.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateText(model, text):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long).unsqueeze(0)\n",
    "    gpt_output = model(tokens)\n",
    "    predicted_word = gpt_output.logits.argmax(dim=-1)\n",
    "    new_text = text + tokenizer.decode(predicted_word.item())\n",
    "    return new_text\n",
    "\n",
    "text = \"My life motto:\"\n",
    "for _ in range(10):\n",
    "    text = generateText(my_gpt, text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bert = bert_sol.Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n",
    "mapped_params = {bert_sol.mapkey(k): v for k, v in pretrained_bert.state_dict().items()\n",
    "                if not k.startswith('classification_head')}\n",
    "my_bert.load_state_dict(mapped_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have my_bert, my_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1422,  1297, 13658,   131,   102,     0,     0,     0,     0],\n",
      "        [  101,  1422,  1297, 13658,   131, 14555,   102,     0,     0,     0],\n",
      "        [  101,  1422,  1297, 13658,   131, 14555, 24208,   102,     0,     0],\n",
      "        [  101,  1422,  1297, 13658,   131, 14555, 24208,  1103,   102,     0],\n",
      "        [  101,  1422,  1297, 13658,   131, 14555, 24208,  1103,  9009,   102]])\n",
      "BERT\n",
      "tensor([[ 1.0108,  1.0003, -0.0554,  ...,  0.4157,  0.4293,  0.1067],\n",
      "        [ 0.4482, -0.0443,  0.1862,  ..., -0.3393,  0.4696,  0.0145],\n",
      "        [ 0.0276, -0.0032,  0.2345,  ...,  0.3751,  0.4144,  0.0836],\n",
      "        [ 0.1360, -0.0919,  0.2080,  ...,  0.5545,  0.4430,  0.1225],\n",
      "        [ 0.0215, -0.0751,  0.1236,  ...,  0.6215,  0.4317,  0.2077]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([5, 10, 768])\n",
      "tensor([[ 10.6008,   0.4824,  -0.8409,  ...,   3.7044,   2.1718,  -1.7908],\n",
      "        [  2.2354,   4.2186,  -8.5184,  ...,  -1.7431,   6.8043, -18.0181],\n",
      "        [  2.2354,   4.2186,  -8.5184,  ...,  -1.7431,   6.8043, -18.0182],\n",
      "        [  2.2354,   4.2186,  -8.5184,  ...,  -1.7431,   6.8043, -18.0182],\n",
      "        [  2.2354,   4.2186,  -8.5184,  ...,  -1.7431,   6.8043, -18.0181]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# this is to demonstrate that in gpt the logits for a token \n",
    "# only depends on earlier tokens in the sequence, whereas that's not\n",
    "# true for BERT\n",
    "strings = \"\"\"My life motto:\n",
    "My life motto: Fortune\n",
    "My life motto: Fortune favors\n",
    "My life motto: Fortune favors the\n",
    "My life motto: Fortune favors the bold\"\"\"\n",
    "strings = strings.split(\"\\n\")\n",
    "strings = [bert_tokenizer.encode(s) for s in strings]\n",
    "pad_length = 10\n",
    "strings = torch.tensor([s + [0]*(pad_length-len(s)) for s in strings], dtype = torch.long)\n",
    "print(strings)\n",
    "\n",
    "my_bert.eval()\n",
    "my_bert(strings)\n",
    "print(\"BERT\")\n",
    "print(my_bert.last_token_encodings[:,3,:])\n",
    "\n",
    "my_gpt.eval()\n",
    "my_gpt(strings) \n",
    "print(\"GPT\")\n",
    "print(my_gpt.last_token_encodings[:,3,:])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
