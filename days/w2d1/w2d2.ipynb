{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run w2d1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional, List, Tuple\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19082, 1184, 112, 188, 1146, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 7592, 2054, 1005, 1055, 2039, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"[CLS] hello what's up [SEP]\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(\"hello what's up\"))\n",
    "uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(uncased_tokenizer([\"hello what's up\"]))\n",
    "coded = uncased_tokenizer([\"hello what's up\"])\n",
    "uncased_tokenizer.batch_decode(coded['input_ids'])\n",
    "tokenizer.batch_decode(coded['input_ids'])\n",
    "uncased_tokenizer.batch_decode(coded['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert, pretrained_bert = load_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish loves to eat [MASK].\n",
      "it fish them meat food eggs honey insects too rice everything water vegetables this fruit apples him there again here\n",
      "tensor([0.1738, 0.0980, 0.0947, 0.0410, 0.0336, 0.0251, 0.0134, 0.0130, 0.0126,\n",
      "        0.0119, 0.0092, 0.0090, 0.0088, 0.0083, 0.0072, 0.0069, 0.0063, 0.0060,\n",
      "        0.0058, 0.0054], grad_fn=<IndexBackward0>)\n",
      "\n",
      "The fish loves to eat [MASK]\n",
      ". ;!?..., : | and \" but - so ред because as [UNK]') with\n",
      "tensor([9.4125e-01, 4.6098e-02, 1.1822e-02, 4.5820e-04, 1.2235e-04, 5.4506e-05,\n",
      "        3.6213e-05, 1.6483e-05, 1.2279e-05, 9.2127e-06, 6.5461e-06, 4.6536e-06,\n",
      "        3.4753e-06, 3.3669e-06, 2.9931e-06, 2.4598e-06, 1.9791e-06, 1.7764e-06,\n",
      "        1.3952e-06, 1.0635e-06], grad_fn=<IndexBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def feed_bert(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    mask_idxs = [idx for idx, token in enumerate(input_ids) if token == 103]\n",
    "\n",
    "    all_logits = model(t.tensor([input_ids], dtype=t.long))[0]\n",
    "\n",
    "    print(text)\n",
    "    for mask_idx in mask_idxs:\n",
    "        logits = all_logits[mask_idx]\n",
    "        probs = t.softmax(logits, dim=0)\n",
    "\n",
    "        top_logit_idxs = t.argsort(logits, descending=True)[:top_k]\n",
    "        top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "\n",
    "        print(top_logit_words)\n",
    "        print(probs[top_logit_idxs])\n",
    "        print()\n",
    "\n",
    "my_bert.eval()\n",
    "feed_bert(my_bert, \"The fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "feed_bert(my_bert, \"The fish loves to eat [MASK]\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The vegetarian fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The meat-eating fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The tiny fish loves to eat [MASK].\", tokenizer, top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_classification(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train = list(data_train)[:10]\n",
    "data_test = list(data_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_collate_fn(\n",
    "    max_seq_length: int,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    device: str,\n",
    "):\n",
    "    def fn(raw_xs: List[Tuple[str, str]]) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        labels: List[str]\n",
    "        texts: List[str]\n",
    "        labels, texts = zip(*raw_xs)\n",
    "\n",
    "        xs = t.tensor(\n",
    "            tokenizer(\n",
    "                texts,\n",
    "                padding=\"longest\",\n",
    "                max_length=max_seq_length,\n",
    "                truncation=True,\n",
    "            ),\n",
    "            dtype=t.long,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        ys = t.tensor([int(l == \"pos\") for l in labels], dtype=t.long, device=device)\n",
    "\n",
    "        return xs, ys\n",
    "\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[('neg', 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'), ('neg', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl_train = DataLoader(data_train, batch_size=2, collate_fn=imdb_collate_fn)\n",
    "\n",
    "for x in dl_train:\n",
    "    print(len(x))\n",
    "    print(x)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg',\n",
       " 'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\' American Masters: Finding Lucy. If you want to see a docudrama, \"Before the Laughter\" would be a better choice. The casting of Lucille Ball and Desi Arnaz in \"Before the Laughter\" is much better compared to this. At least, a similar aspect is shown rather than nothing.')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
