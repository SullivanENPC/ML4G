{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_attention_pattern(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    project_query: Callable[[t.Tensor], t.Tensor],\n",
    "    project_key: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)]\n",
    "    project_query: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    project_key:   function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "    \"\"\"\n",
    "\n",
    "    queries = rearrange(\n",
    "        project_query(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "    keys = rearrange(\n",
    "        project_key(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    head_size = t.tensor(keys.shape[-1])\n",
    "    return einsum(\"bhid, bhjd -> bhij\", keys, queries) / t.sqrt(head_size)\n",
    "\n",
    "\n",
    "# bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_attention(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    attention_pattern: t.Tensor,\n",
    "    project_value: Callable[[t.Tensor], t.Tensor],\n",
    "    project_output: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "    num_heads: int,\n",
    "    attention_pattern: Tensor[batch_size,num_heads, seq_length, seq_length],\n",
    "    project_value: function( (Tensor[..., 768]) -> Tensor[..., 768] ),\n",
    "    project_output: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, seq_length, hidden_size]\n",
    "    \"\"\"\n",
    "\n",
    "    attention_prob = t.softmax(attention_pattern, dim=-2)  # dim: b head s s\n",
    "    values = rearrange(\n",
    "        project_value(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    output_by_head = einsum(\"bhis, bhid -> bhsd\", attention_prob, values)\n",
    "    concatenated = rearrange(output_by_head, \"b h s d -> b s (h d)\")\n",
    "\n",
    "    return project_output(concatenated)\n",
    "\n",
    "\n",
    "# bert_tests.test_attention_fn(bert_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_size: int,\n",
    "        attention_dim: int = 64,\n",
    "        per_head_output_dim: int = 64,\n",
    "        output_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if output_dim is None:\n",
    "            output_dim = hidden_size\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.per_head_output_dim = per_head_output_dim\n",
    "        self.output_dim: int = output_dim\n",
    "\n",
    "        self.Q = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.K = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.V = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * per_head_output_dim\n",
    "        )\n",
    "        self.O = nn.Linear(\n",
    "            in_features=num_heads * per_head_output_dim, out_features=output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        input: Tensor[batch_size, seq_length, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        attention_pattern = raw_attention_pattern(\n",
    "            input,\n",
    "            self.num_heads,\n",
    "            project_key=self.K,\n",
    "            project_query=self.Q,\n",
    "        )\n",
    "\n",
    "        return bert_attention(\n",
    "            token_activations=input,\n",
    "            num_heads=self.num_heads,\n",
    "            attention_pattern=attention_pattern,\n",
    "            project_value=self.V,\n",
    "            project_output=self.O,\n",
    "        )\n",
    "\n",
    "\n",
    "# bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa = MultiHeadedSelfAttention(\n",
    "    num_heads=17,\n",
    "    hidden_size=768,\n",
    "    attention_dim=37,\n",
    "    per_head_output_dim=89,\n",
    "    output_dim=2,\n",
    ")\n",
    "# mhsa(t.ones((10, 117, 768))).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_mlp(\n",
    "    token_activations: t.Tensor,\n",
    "    linear_1: nn.Module,\n",
    "    linear_2: nn.Module,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: torch.Tensor[batch_size,seq_length,768],\n",
    "    return: torch.Tensor[batch_size, seq_length, 768]\n",
    "    \"\"\"\n",
    "\n",
    "    x = linear_1(token_activations)\n",
    "    x = F.gelu(x)\n",
    "    x = linear_2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(\n",
    "            in_features=input_size, out_features=intermediate_size\n",
    "        )\n",
    "        self.linear_2 = nn.Linear(\n",
    "            in_features=intermediate_size, out_features=input_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        return bert_mlp(input, self.linear_1, self.linear_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input: t.Tensor):  # shape[..., normalized_dim]\n",
    "        m = t.mean(input, dim=-1, keepdim=True).detach()\n",
    "        v = t.var(input, dim=-1, keepdim=True, unbiased=False).detach()\n",
    "        input = (input - m) / t.sqrt(v + self.eps)\n",
    "        return input * self.weight + self.bias\n",
    "\n",
    "\n",
    "# bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size: int, intermediate_size: int, num_heads: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadedSelfAttention(\n",
    "            num_heads=num_heads, hidden_size=hidden_size\n",
    "        )\n",
    "        self.ln1 = LayerNorm(normalized_dim=hidden_size)\n",
    "        self.bmlp = BertMLP(input_size=hidden_size, intermediate_size=intermediate_size)\n",
    "        self.ln2 = LayerNorm(normalized_dim=hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        x1 = self.ln1(self.mha(input) + input)\n",
    "        return self.ln2(self.dropout(self.bmlp(x1)) + x1)\n",
    "\n",
    "# bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Parameter(t.randn((vocab_size, embed_size),))\n",
    "\n",
    "    def forward(self, input: t.LongTensor) -> t.FloatTensor:\n",
    "        \"\"\"\n",
    "        input: tensor[...]\n",
    "        return: tensor[..., embed_size]\n",
    "        \"\"\"\n",
    "        return self.embedding[input]\n",
    "\n",
    "# bert_tests.test_embedding(Embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding(\n",
    "    input_ids: t.Tensor,  # [batch, seqlen]\n",
    "    token_type_ids: t.Tensor,  # [batch, seqlen]\n",
    "    position_embedding: Embedding,\n",
    "    token_embedding: Embedding,\n",
    "    token_type_embedding: Embedding,\n",
    "    layer_norm: LayerNorm,\n",
    "    dropout: nn.Dropout,\n",
    ") -> t.Tensor:\n",
    "    seq_len = input_ids.shape[-1]\n",
    "    device = input_ids.device\n",
    "\n",
    "    inputs = token_embedding(input_ids)\n",
    "    tokens = token_type_embedding(token_type_ids)\n",
    "    positions = position_embedding(t.arange(seq_len, dtype=t.long, device=device))\n",
    "\n",
    "    return dropout(layer_norm(inputs + tokens + positions))\n",
    "\n",
    "# bert_tests.test_bert_embedding_fn(bert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        type_vocab_size: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids: t.Tensor, token_type_ids: t.Tensor) -> t.Tensor:\n",
    "        return bert_embedding(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_embedding=self.position_embedding,\n",
    "            token_embedding=self.token_embedding,\n",
    "            token_type_embedding=self.token_type_embedding,\n",
    "            layer_norm=self.layer_norm,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "\n",
    "# bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        type_vocab_size: int,\n",
    "        dropout: float,\n",
    "        intermediate_size: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        num_classes: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = BertEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                BertBlock(\n",
    "                    hidden_size=hidden_size,\n",
    "                    intermediate_size=intermediate_size,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # TODO: Tie unembedding weights\n",
    "        self.lm_head = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"mlp\",\n",
    "                        nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "                    ),\n",
    "                    (\"gelu\", nn.GELU()),\n",
    "                    (\"layer_norm\", LayerNorm(hidden_size)),\n",
    "                    (\n",
    "                        \"unembedding\",\n",
    "                        nn.Linear(in_features=hidden_size, out_features=vocab_size),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.classification_head = None if num_classes is None else nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def transformer_output(self, input_ids: t.Tensor) -> t.Tensor:\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        embed = self.embedding(input_ids, token_type_ids)\n",
    "        return self.transformer(embed)\n",
    "\n",
    "    def forward(self, input_ids: t.Tensor) -> Union[t.Tensor, Tuple[t.Tensor, t.Tensor]]:\n",
    "        transformer_output = self.transformer_output(input_ids)\n",
    "        lm_output = self.lm_head(transformer_output)\n",
    "        if self.classification_head is not None:\n",
    "            return (lm_output, self.classification_head(transformer_output[:, 0]))\n",
    "        return lm_output\n",
    "        \n",
    "\n",
    "\n",
    "# bert_tests.test_bert(Bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0252e-05, -5.1260e-06,  0.0000e+00,  5.1260e-06,  1.0252e-05])\n",
      "tensor([-2.0000, -1.0000,  0.0000,  1.0000,  2.0000])\n",
      "tensor([-1.0014e-05, -5.0068e-06,  0.0000e+00,  5.0068e-06,  1.0014e-05])\n"
     ]
    }
   ],
   "source": [
    "class ExperimentalLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,), dtype=t.float))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,), dtype=t.float))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input: t.Tensor):  # shape[..., normalized_dim]\n",
    "        m = t.mean(input, dim=-1, keepdim=True)  # .detach()\n",
    "        v = t.var(input - m, dim=-1, keepdim=True, unbiased=False)  # .detach()\n",
    "        input = (input - m) / t.sqrt(v + self.eps)\n",
    "        return input * self.weight + self.bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_grad(layernorm):\n",
    "    input = t.arange(N, dtype=t.float, requires_grad=True)\n",
    "\n",
    "    loss = t.sum(layernorm(input.reshape(1, N)) ** 2)\n",
    "    loss.backward()\n",
    "\n",
    "    return input.grad\n",
    "\n",
    "# https://stackoverflow.com/a/43615015\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    N = 5\n",
    "    torchs = nn.LayerNorm(N, eps=1e-5)\n",
    "    ours = LayerNorm(N)\n",
    "    exp = ExperimentalLayerNorm(N, eps=1e-5)\n",
    "\n",
    "    print(get_grad(torchs))\n",
    "    print(get_grad(ours))\n",
    "    print(get_grad(exp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.702 STD: 2.377 VALS [-5.679 -5.777 -5.893 -5.582 -5.954 -5.679 -6.112 -6.121 -6.171 -5.848...]\n"
     ]
    }
   ],
   "source": [
    "def hf_to_our_state_dict(hf_dict: Dict[str, t.Tensor]) -> Dict[str, t.Tensor]:\n",
    "    def include_key(key: str) -> bool:\n",
    "        if key.startswith(\"classification_head\"):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def transform_key(key: str) -> str:\n",
    "        subkeys = key.split(\".\")\n",
    "\n",
    "        if key.startswith(\"embedding\") and key.endswith(\"_embedding.weight\"):\n",
    "            subkeys[-1] = \"embedding\"\n",
    "            return \".\".join(subkeys)\n",
    "\n",
    "        if subkeys[0] == \"transformer\":\n",
    "            if subkeys[2] == \"attention\":\n",
    "                subkeys[2] = \"mha\"\n",
    "\n",
    "                if subkeys[3] == \"pattern\":\n",
    "                    subkeys.pop(3)\n",
    "\n",
    "                subkeys[3] = {\n",
    "                    \"project_value\": \"V\",\n",
    "                    \"project_query\": \"Q\",\n",
    "                    \"project_key\": \"K\",\n",
    "                    \"project_out\": \"O\",\n",
    "                }[subkeys[3]]\n",
    "\n",
    "                return \".\".join(subkeys)\n",
    "\n",
    "            if subkeys[2] == \"residual\":\n",
    "                if subkeys[3] != \"layer_norm\":\n",
    "                    subkeys[2] = \"bmlp\"\n",
    "                    subkeys[3] = {\"mlp1\": \"linear_1\", \"mlp2\": \"linear_2\"}[subkeys[3]]\n",
    "                    return \".\".join(subkeys)\n",
    "\n",
    "                if subkeys[3] == \"layer_norm\":\n",
    "                    subkeys.pop(2)\n",
    "                    subkeys[2] = \"ln2\"\n",
    "                    return \".\".join(subkeys)\n",
    "\n",
    "            if subkeys[2] == \"layer_norm\":\n",
    "                subkeys[2] = \"ln1\"\n",
    "                return \".\".join(subkeys)\n",
    "\n",
    "        return key\n",
    "\n",
    "    return {transform_key(k): v for k, v in hf_dict.items() if include_key(k)}\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/43615015\n",
    "\n",
    "\n",
    "def load_pretrained_bert(num_classes: Optional[int] = None):\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996,\n",
    "        hidden_size=768,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        dropout=0.1,\n",
    "        intermediate_size=3072,\n",
    "        num_heads=12,\n",
    "        num_layers=12,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    pretrained_bert = bert_tests.get_pretrained_bert()\n",
    "\n",
    "    load_res = my_bert.load_state_dict(\n",
    "        hf_to_our_state_dict(pretrained_bert.state_dict()), strict=False\n",
    "    )\n",
    "    assert len(load_res.unexpected_keys) == 0\n",
    "\n",
    "    return my_bert, pretrained_bert\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and \"__file__\" not in globals():\n",
    "    my_bert, pretrained_bert = load_pretrained_bert()\n",
    "    bert_tests.test_same_output(my_bert, pretrained_bert, tol=1e-4)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
