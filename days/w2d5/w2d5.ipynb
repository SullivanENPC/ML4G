{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch as t\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import transformers\n",
    "\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenization(json_corpus_path):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = t.load(\"gpt_trained\")\n",
    "# gpt = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device=device)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categoricalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_pred(prompt, max_length=100):\n",
    "    tokens = tokenizer(prompt).input_ids\n",
    "    for _ in range(max_length):\n",
    "        output = gpt(t.tensor([tokens],device=device))\n",
    "        next_logits = output.logits.squeeze(0)[-1,:]\n",
    "        sampler = t.distributions.categorical.Categorical(logits=next_logits)\n",
    "        next_token = sampler.sample()\n",
    "        tokens.append(next_token)\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less Wrong: The Solution to AI Alignment: Minddiscipline,\n",
      "science, economics, etc. The evidence continued on for longer\n",
      "basis, with improved\n",
      "aggregations, a dramatically different burden of documentation, and the volumes and analysis of numerology that306you already have. The reduction in grayish ink suggested that motivated modelling is not much more useful for solving the difficult tasks you're looking for. Previous studies had found deep adoption in cloud computing, and Salesforce for several, and now it\\'s\n",
      "more common for organizations\n"
     ]
    }
   ],
   "source": [
    "print(gpt_pred('Less Wrong: The Solution to AI Alignment:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = t.distributions.categorical.Categorical(\n",
    "    probs=t.tensor([[0.1, 0.2, 0.7], [0.7, 0.2, 0.1]])\n",
    ")\n",
    "s.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [ friend would'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([685, 1545, 561])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
    "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    self.clear_cache()\n",
    "    for _ in range(max_length):\n",
    "        next_token = self.next_token(\n",
    "            input_ids=t.tensor(input_ids, device=self.input_device()),\n",
    "            temperature=temperature,\n",
    "            freq_penalty=freq_penalty,\n",
    "        )\n",
    "        input_ids.append(next_token)\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    return tokenizer.decode(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
