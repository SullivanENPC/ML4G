{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    query = project_query(token_activations)\n",
    "    query = rearrange(query, \"b s (n h) -> b n s h\", n=num_heads)\n",
    "    key = project_key(token_activations)\n",
    "    key = rearrange(key, \"b s (n h) -> b n s h\", n=num_heads)\n",
    "    head_size = key.shape[-1]\n",
    "    dot_prod = t.einsum(\"bsnh,bsmh->bsmn\", query, key)\n",
    "    dot_prod /= np.sqrt(head_size)\n",
    "    return dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.0007749 STD: 0.1164 VALS [0.1197 0.02408 0.03616 0.01532 0.1504 -0.1547 -0.04518 0.08466 -0.1337 -0.09462...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    attention_score = t.softmax(attention_pattern, dim=-2)\n",
    "    value = project_value(token_activations)\n",
    "    value = rearrange(value, \"batch seq (head size) -> batch seq size head\", head=num_heads)\n",
    "    attention_value = t.einsum(\"bhkq,bksh->bshq\", attention_score, value)\n",
    "    attention_value = rearrange(attention_value, \"batch size head seq-> batch seq (head size)\", head=num_heads)\n",
    "    output = project_output(attention_value)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001968 STD: 0.1205 VALS [0.1741 -0.002676 -0.08375 0.2172 -0.1002 0.04842 0.04215 -0.1167 0.1185 -0.1826...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        attn_pattern = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "        bert_attn = bert_attention(input, self.num_heads, attn_pattern, self.value, self.output)\n",
    "        return bert_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    x = linear_1(token_activations)\n",
    "    y = t.nn.functional.gelu(x)\n",
    "    return linear_2(y)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.linear1 = t.nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = t.nn.Linear(intermediate_size, input_size)\n",
    "    def forward(self, input):\n",
    "        return bert_mlp(input, self.linear1, self.linear2)\n",
    "\n",
    "# bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5117 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mean = t.mean(input, dim=-1, keepdims=True).detach()\n",
    "        var = t.std(input, dim=-1, unbiased=False, keepdims=True).detach()\n",
    "        input = (input - mean) / var\n",
    "        input = input * self.weight + self.bias\n",
    "        return input\n",
    "    \n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.897e-09 STD: 1 VALS [0.007131 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.norm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.attn(input)\n",
    "        y = self.norm1(x + input)\n",
    "        x = self.mlp(y)\n",
    "        x = self.dropout(x)\n",
    "        return self.norm2(x + y)\n",
    "        \n",
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.embedding[inputs, :] #TODO look at solution\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):\n",
    "    x = position_embedding(t.arange(input_ids.shape[-1])) + token_embedding(input_ids) + token_type_embedding(token_type_ids)\n",
    "    x = layer_norm(x)\n",
    "    return dropout(x)\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.553e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_position_embeddings: int, type_vocab_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.token_em = Embedding(vocab_size, hidden_size)\n",
    "        self.pos_em = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_em = Embedding(2, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.pos_em, self.token_em, self.token_type_em, self.layer_norm, self.dropout)\n",
    "    \n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf92ecb8ba8c4929991c83d4efd53b8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7e65220dc38470e9fb7fc848c4b7902"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f6faa1ddd474fb89010229cc7d9e96d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8120f58082f4f0baf4d57cb9108bd79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.4321 0.1186 -0.7165 -0.5262 0.4967 1.223 0.3165 -0.3247 -0.5717...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "            max_position_embeddings: int, type_vocab_size: int, \n",
    "            dropout: float, intermediate_size: int, num_heads: int, \n",
    "            num_layers: int):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.layers = t.nn.Sequential(\n",
    "            t.nn.Sequential(*[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)]),\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "        )\n",
    "        self.linear = t.nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        return self.linear(self.layers(self.embedding(input_ids, t.zeros(1, dtype=t.long, device=input_ids.device))))\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83046c76098f43dbb094917ef138cc1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n",
    "pretrained_bert_dict = pretrained_bert.state_dict()\n",
    "del pretrained_bert_dict['classification_head.weight']\n",
    "del pretrained_bert_dict['classification_head.bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_weights = {v[0]: pretrained_bert_dict[v[1]] for v in zip(my_bert.state_dict(), pretrained_bert_dict)}\n",
    "mapping_replacements = {\n",
    "    'layers.3.weight' : 'linear.weight',\n",
    "    'linear.weight' : 'layers.3.weight',\n",
    "    'layers.3.bias' : 'linear.bias',\n",
    "    'linear.bias' : 'layers.3.bias',\n",
    "}\n",
    "\n",
    "# mapped_weights = {'a':2,'b':3}\n",
    "# mapping_replacements = {'a':'b','b':'a'}\n",
    "another_mapped_weights = {mapping_replacements[i] if i in mapping_replacements else i : mapped_weights[i] for i in mapped_weights}\n",
    "# another_mapped_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "print(len([v for v in my_bert.state_dict()]))\n",
    "print(len([v for v in pretrained_bert_dict]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_bert.load_state_dict(another_mapped_weights)\n",
    "my_bert.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.414 VALS [-5.65 -6.041 -6.096 -6.062 -5.945 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30b3487a97ca4119a89d6ec4ca87e486"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4574c94db7ab4a23ad52b667f2f80ee8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a048b26c16ac46638713e8cb69071308"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fafd8e91651e4e8f8d865dea0d2b98d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bad_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] colleges 天 largest happened smile donation [SEP]'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(bad_tokenizer.encode(\"Hi, my name is bert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 103, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "int"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.encode(\"The firetruck was painted bright [MASK].\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', tensor(0.1736, grad_fn=<UnbindBackward0>)), ('fish', tensor(0.0982, grad_fn=<UnbindBackward0>)), ('them', tensor(0.0946, grad_fn=<UnbindBackward0>)), ('meat', tensor(0.0410, grad_fn=<UnbindBackward0>))]\n",
      "tensor([0.1736, 0.0982, 0.0946, 0.0410], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoding = t.tensor([tokenizer.encode(\"The fish loves to eat [MASK].\")], dtype=t.long)\n",
    "mask_location = t.where(encoding == tokenizer.mask_token_id)[1]\n",
    "probs = t.softmax(my_bert(encoding)[0,mask_location,:], dim=-1)\n",
    "ids = t.topk(probs, 4).indices[0]\n",
    "print([x for x in zip(tokenizer.decode(ids).split(\" \"), probs[0, ids])])\n",
    "print(probs[0, ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_art_probs(input, tokenizer, model):\n",
    "    print(input.replace(\"[MASK]\", \"___\"))\n",
    "    encoding = t.tensor([tokenizer.encode(input)], dtype=t.long)\n",
    "    mask_location = t.where(encoding == tokenizer.mask_token_id)[1]\n",
    "    probs = t.softmax(model(encoding)[0,mask_location,:], dim=-1)\n",
    "    ids = t.topk(probs, 4).indices[0]\n",
    "    spaces = \" \"*input.index(\"[MASK]\")\n",
    "    for word, p in zip(tokenizer.decode(ids).split(\" \"), probs[0, ids]):\n",
    "        print(f\"{spaces}{word}\\t{p*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firetruck was painted bright ___.\n",
      "                                 red\t56.0%\n",
      "                                 yellow\t12.1%\n",
      "                                 white\t7.1%\n",
      "                                 blue\t6.9%\n"
     ]
    }
   ],
   "source": [
    "ascii_art_probs(\"The firetruck was painted bright [MASK].\", tokenizer, my_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentiment(t.nn.Module):\n",
    "    def __init__(self, bert_model, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = t.nn.Dropout(bert_model.dropout)\n",
    "        self.linear = t.nn.Linear(bert_model.hidden_size, num_classes)\n",
    "    def forward(self, input_ids):\n",
    "        return self.linear(self.dropout(self.bert_model.layers(self.bert_model.embedding(input_ids, t.zeros(1, dtype=t.long, device=input_ids.device)))))\n",
    "    def parameters(self):\n",
    "        return self.linear.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4187/3466353544.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdata_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIMDB\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'.data'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'test'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mdata_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import random\n",
    "\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train = list(data_train)\n",
    "random.shuffle(data_train)\n",
    "data_test = list(data_test)\n",
    "random.shuffle(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, tokenizer, max_seq_len = 2048):\n",
    "    assert len(data) > batch_size\n",
    "    n_batches = len(data) // batch_size\n",
    "    batched_data = t.zeros(n_batches, batch_size, max_seq_len, dtype=t.long)\n",
    "    batched_labels = t.zeros(n_batches, batch_size, dtype=t.long)\n",
    "    for batch in range(n_batches):\n",
    "        for i in range(batch_size):\n",
    "            label, text = data[batch*batch_size + i]\n",
    "            tokens = tokenizer.encode(text)\n",
    "            l = min(len(tokens), max_seq_len)\n",
    "            batched_data[batch,i,:l] = t.tensor(tokens)[:l]\n",
    "            batched_labels[batch,i] = int(label == 'pos')\n",
    "    return batched_data, batched_labels\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "res = get_batches(data_train, 16, tokenizer, max_seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1562, 16, 50])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiments(data_train, data_test, model, batch_size, tokenizer, max_seq_len=2048, lr=1e-5, epochs=1):\n",
    "    batched_data, batched_labels = get_batches(data_train, batch_size, tokenizer, max_seq_len=max_seq_len)\n",
    "    #print(batched_data.shape)\n",
    "    model = model.cuda()\n",
    "    optimizer = t.optim.Adam(model.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    try:\n",
    "        for _ in range(epochs):\n",
    "            for batch in tqdm(range(batched_data.shape[0])):\n",
    "                batched_data_cuda = batched_data[batch].cuda()\n",
    "                batched_labels_cuda = batched_labels[batch].cuda()\n",
    "                #print(batched_data_cuda.shape)\n",
    "                #print(batched_labels_cuda.shape)\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(batched_data_cuda)[:,0,:]\n",
    "                #print(pred.shape)\n",
    "                #print(batched_labels[batch].shape)\n",
    "                loss = t.nn.CrossEntropyLoss()(pred, batched_labels_cuda)\n",
    "\n",
    "                #print(loss)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    except KeyboardInterrupt:\n",
    "        return losses\n",
    "    return losses\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/781 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-41-c3abcf69ead3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# sentiment_model = BertSentiment(my_bert, 2)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0msentiment_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_sentiments\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentiment_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m512\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-39-8bf899b90db9>\u001B[0m in \u001B[0;36mtrain_sentiments\u001B[0;34m(data_train, data_test, model, batch_size, tokenizer, max_seq_len, lr, epochs)\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatched_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m                 \u001B[0mbatched_data_cuda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatched_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m                 \u001B[0mbatched_labels_cuda\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatched_labels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m                 \u001B[0;31m#print(batched_data_cuda.shape)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# sentiment_model = BertSentiment(my_bert, 2)\n",
    "sentiment_model.cuda()\n",
    "losses = train_sentiments(data_train, data_test, sentiment_model, 32, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-289d49315128>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data, batched_labels = get_batches(data_train, 64, tokenizer, max_seq_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data = batched_data.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = t.zeros(10)\n",
    "B = A.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}