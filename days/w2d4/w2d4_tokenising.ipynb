{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from typing import List\n",
    "import tokenizer as tok_tests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_common_tokens(corpus: List[str], most_common=30_000):\n",
    "    concated_corpus = \" \".join(corpus)\n",
    "    token_list = re.findall(r\"\\w+|[^\\w\\s]\", concated_corpus)\n",
    "    counts_dict = collections.Counter(token_list)\n",
    "    most_common_items = counts_dict.most_common(most_common)\n",
    "    return [k for k, v in most_common_items]\n",
    "\n",
    "tok_tests.test_tokenizer_from_corpus_fn(corpus_common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running their tokenizer\n",
      "running our tokenizer\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, token_list, unk_id=3) -> None:\n",
    "        self.str_to_id = {d['piece']:d['id'] for d in token_list}\n",
    "        self.id_to_str = {v: k for k, v in self.str_to_id.items()}\n",
    "        self.unk_id = unk_id\n",
    "    \n",
    "    def decode(self, input_ids) -> str:\n",
    "        decoded = [self.id_to_str[i] for i in input_ids]\n",
    "        return \" \".join(decoded)\n",
    "    \n",
    "    def tokenize(self, string) -> List[int]:\n",
    "        token_strs = re.findall(r\"\\w+|[^\\w\\s]\", string)\n",
    "        return [self.str_to_id.get(string, self.unk_id) for string in token_strs]\n",
    "\n",
    "tok_tests.test_tokenizer(Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aab', 'c']\n",
      "[5, 2]\n",
      "['he', 'll', 'o', ', my ', 'nam', 'e ', 'is', ' t', 'om', ' tru', 'n', 'd', 'le', 'wi', 'ch']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "got (671, 96, 53, 464, 807, 80, 168, 83, 125, 471, 46, 14, 725, 118, 134) instead of (27, 296, 53, 464, 807, 80, 168, 83, 125, 83, 17, 504, 725, 118, 134), theirs:hello, my name is tom trundlewich, ours:hello, my name is tom trundlewich, theirs:f , ell, ours:he, ll",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mstr_to_id\u001b[38;5;241m=\u001b[39mexample_dict\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maabc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtok_tests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_bpe_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBPETokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlab/days/w2d4/tokenizer.py:289\u001b[0m, in \u001b[0;36mtest_bpe_tokenizer\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    286\u001b[0m their_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(reference\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello, my name is tom trundlewich\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    287\u001b[0m our_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(yours\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello, my name is tom trundlewich\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    290\u001b[0m     their_tokens \u001b[38;5;241m==\u001b[39m our_tokens\n\u001b[1;32m    291\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mour_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheir_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, theirs:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode(their_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ours:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode(our_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, theirs:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m38\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m296\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ours:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m671\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;241m.\u001b[39mdecode([\u001b[38;5;241m96\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: got (671, 96, 53, 464, 807, 80, 168, 83, 125, 471, 46, 14, 725, 118, 134) instead of (27, 296, 53, 464, 807, 80, 168, 83, 125, 83, 17, 504, 725, 118, 134), theirs:hello, my name is tom trundlewich, ours:hello, my name is tom trundlewich, theirs:f , ell, ours:he, ll"
     ]
    }
   ],
   "source": [
    "example_dict = {\"a\": 0, \"b\": 1, \"c\":2, \"aa\": 4, \"aab\": 5}\n",
    "example_str = 'aabc'\n",
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, token_list):\n",
    "        super().__init__(token_list)\n",
    "\n",
    "    def tokenize(self, string) -> List[int]:\n",
    "\n",
    "        # [\"a\", \"a\", \"b\", \"c\"]\n",
    "        # [\"aa\", \"b\", \"c\"]\n",
    "        # [\"aab\", \"c\"]\n",
    "\n",
    "        char_list = list(string)\n",
    "        have_merged = True\n",
    "\n",
    "        while have_merged:\n",
    "            i = 1\n",
    "            have_merged = False\n",
    "\n",
    "            while i < len(char_list) :\n",
    "                token_pair = \"\".join(char_list[i -1: i + 1])\n",
    "                is_token = token_pair in self.str_to_id\n",
    "\n",
    "                if is_token:\n",
    "                    char_list[i-1] = token_pair\n",
    "                    char_list.pop(i)\n",
    "                    have_merged = True\n",
    "\n",
    "                i += 1\n",
    "        \n",
    "        print(char_list)\n",
    "\n",
    "        return [self.str_to_id[tok] for tok in char_list]\n",
    "\n",
    "\n",
    "tokenizer=BPETokenizer([{'piece':'a', 'id':0}])\n",
    "tokenizer.str_to_id=example_dict\n",
    "print(tokenizer.tokenize('aabc'))\n",
    "tok_tests.test_bpe_tokenizer(BPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
