{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from torch import nn \n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import transformers\n",
    "import torchtext\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psudeocode for single head\n",
    "\n",
    "raw_attention_pattern(activations, query_fn, key_fn):\n",
    "    K = key_fn(activations)\n",
    "    Q = query_fn(activations)\n",
    "    return QK^T / sqrt(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.008059 STD: 0.1069 VALS [0.004014 0.1631 0.1264 -0.02056 -0.15 -0.0257 0.04729 0.1093 -0.01661 0.0894...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int, \n",
    "    project_query,\n",
    "    project_key\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        token_activations: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "        num_heads: int, \n",
    "        project_query: function( (Tensor[b, n, c: 768]) -> Tensor[b, n, (hn hs) : 768] ),\n",
    "        project_key: function( (Tensor[..., 768]) -> Tensor[..., 768] \n",
    "    returns:\n",
    "        Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "    \"\"\"\n",
    "    Q = rearrange(project_query(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    K = rearrange(project_key(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    ans = einsum(\"bhji,bhki->bhjk\", K, Q)\n",
    "    head_size = Q.shape[3]\n",
    "    return ans / math.sqrt(head_size)\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001797 STD: 0.1205 VALS [-0.2488 0.12 -0.05243 0.1049 -0.04674 -0.1931 0.07625 -0.2776 0.02127 -0.1898...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int, \n",
    "    attention_pattern: t.Tensor,\n",
    "    project_value, \n",
    "    project_output,\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        token_activations: Tensor[batch_size, seq_length, hidden_size (768)], \n",
    "        num_heads: int, \n",
    "        attention_pattern: Tensor[batch_size,num_heads, seq_length, seq_length], \n",
    "        project_value: function( (Tensor[..., 768]) -> Tensor[..., 768] ), \n",
    "        project_output: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    output:\n",
    "        Tensor[batch_size, seq_length, hidden_size] \n",
    "    \"\"\"\n",
    "    attention_pattern = t.softmax(attention_pattern, dim = 2)\n",
    "    V = rearrange(project_value(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    A = rearrange(einsum('bhkq,bhki->bhqi',attention_pattern,V), \"b hn n hs -> b n (hn hs)\", hn = num_heads)\n",
    "    return project_output(A)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads = 12, hidden_size = 768):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.O = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_pattern = raw_attention_pattern(input, self.num_heads, self.Q, self.K)\n",
    "        return bert_attention(input, self.num_heads, attention_pattern, self.V, self.O)\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(\n",
    "    token_activations: t.Tensor,\n",
    "    linear_1: nn.Module,\n",
    "    linear_2: nn.Module\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        token_activations: torch.Tensor[batch_size,seq_length,768],\n",
    "        linear_1: nn.Module,\n",
    "        linear_2: nn.Module\n",
    "    output:\n",
    "        torch.Tensor[batch_size, seq_length, 768]\n",
    "    \"\"\"\n",
    "    return linear_2(nn.functional.gelu(linear_1(token_activations)))\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size:int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.linear1 = nn.Linear(self.input_size, self.intermediate_size)\n",
    "        self.linear2 = nn.Linear(self.intermediate_size, self.input_size)\n",
    "    \n",
    "    def forward(self, activations):\n",
    "        return bert_mlp(activations, self.linear1, self.linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super().__init__()\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.weight = nn.Parameter(t.ones([self.normalized_dim]))\n",
    "        self.bias = nn.Parameter(t.zeros([self.normalized_dim]))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mean = input.mean(dim=-1, keepdim=True).detach()\n",
    "        var = input.var(dim=-1, keepdim=True, unbiased=False).detach()\n",
    "        normed = (input-mean) / t.sqrt(var + self.eps)\n",
    "        return normed*self.weight + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size:int, intermediate_size:int, num_heads:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = MultiHeadedSelfAttention(self.num_heads, self.hidden_size)\n",
    "        self.ln1 = LayerNorm(self.hidden_size)\n",
    "        self.mlp = BertMLP(self.hidden_size, self.intermediate_size)\n",
    "        self.ln2 = LayerNorm(self.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.ln1(self.dropout1(self.attention(input)) + input)\n",
    "        return self.ln2(self.dropout2(self.mlp(x)) + x)\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.weight = nn.Parameter(t.randn((self.vocab_size, self.embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "    input_ids, \n",
    "    token_type_ids, \n",
    "    position_embedding: Embedding,\n",
    "    token_embedding: Embedding, \n",
    "    token_type_embedding: Embedding, \n",
    "    layer_norm: LayerNorm, \n",
    "    dropout: nn.Dropout\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_ids: [batch, seqlen], \n",
    "        token_type_ids: [batch, seqlen], \n",
    "        position_embedding: Embedding,\n",
    "        token_embedding: Embedding, \n",
    "        token_type_embedding: Embedding, \n",
    "        layer_norm: LayerNorm, \n",
    "        dropout: nn.Dropout)\n",
    "    returns:\n",
    "\n",
    "    \"\"\"\n",
    "    pos_ids = repeat(t.arange(input_ids.shape[1]), 's -> b s', b = input_ids.shape[0]).to(input_ids.device) \n",
    "    x = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(pos_ids)\n",
    "    return dropout(layer_norm(x))\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 2.07e-10 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size:int, hidden_size:int, max_position_embeddings:int, type_vocab_size:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.token_embedding = Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.position_embedding = Embedding(self.max_position_embeddings, self.hidden_size)\n",
    "        self.token_type_embedding = Embedding(self.type_vocab_size, self.hidden_size)\n",
    "        self.layer_norm = LayerNorm(self.hidden_size) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids,\n",
    "            self.position_embedding, self.token_embedding,\n",
    "            self.token_type_embedding, self.layer_norm, self.dropout) \n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "        max_position_embeddings: int, type_vocab_size: int, \n",
    "        dropout: float, intermediate_size: int, num_heads: int, \n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for i in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.Sequential(OrderedDict([\n",
    "            [\"mlp\", nn.Linear(hidden_size, hidden_size)],\n",
    "            [\"gelu\", nn.GELU()],\n",
    "            [\"layer_norm\" , LayerNorm(hidden_size)],\n",
    "            [\"unembedding\", nn.Linear(hidden_size, vocab_size)],\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.lm_head(self.transformer(self.embedding(input_ids, token_type_ids)))\n",
    "    \n",
    "\n",
    "bert_tests.test_bert(Bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Extra Keys  ----\n",
      "['classification_head', 'bias']\n",
      "['classification_head', 'weight']\n",
      "----  Missing Keys  ----\n"
     ]
    }
   ],
   "source": [
    "def key_map(old_key):\n",
    "    key_segments = old_key.split(\".\")\n",
    "    if key_segments[0] == \"transformer\":\n",
    "        if key_segments[2] == 'layer_norm':\n",
    "            key_segments[2] = 'ln1'\n",
    "        elif key_segments[2] == 'attention':\n",
    "            if key_segments[3] == 'pattern':\n",
    "                del key_segments[3]\n",
    "            key_segments[3] = {\n",
    "                \"project_query\": \"Q\",\n",
    "                \"project_key\": \"K\",\n",
    "                \"project_value\": \"V\",\n",
    "                \"project_out\": \"O\",\n",
    "            }[key_segments[3]]\n",
    "        elif key_segments[2] == 'residual':\n",
    "            if key_segments[3] == 'mlp1':\n",
    "                key_segments[2] = 'mlp'\n",
    "                key_segments[3] = 'linear1'\n",
    "            elif key_segments[3] == 'mlp2':\n",
    "                key_segments[2] = 'mlp'\n",
    "                key_segments[3] = 'linear2'\n",
    "            elif key_segments[3] == 'layer_norm':\n",
    "                del key_segments[2]\n",
    "                key_segments[2] = 'ln2'\n",
    "\n",
    "    return \".\".join(key_segments)\n",
    "\n",
    "def skip_key(old_key: str):\n",
    "    return old_key.startswith(\"classification_head\")\n",
    "\n",
    "remapped_keys = set([key_map(k) for k in pretrained_bert.state_dict().keys()])\n",
    "target_keys = my_bert.state_dict().keys()\n",
    "\n",
    "def split_and_filter(k):\n",
    "    segs = k.split(\".\")\n",
    "    # if segs[0] == \"transformer\" and segs[1] == '0':\n",
    "    print(segs)\n",
    "\n",
    "print(\"----  Extra Keys  ----\")\n",
    "\n",
    "for k in remapped_keys - target_keys:\n",
    "    split_and_filter(k)\n",
    "\n",
    "print(\"----  Missing Keys  ----\")\n",
    "\n",
    "for k in target_keys - remapped_keys:\n",
    "    split_and_filter(k)\n",
    "\n",
    "# for k in my_bert.state_dict().keys():\n",
    "#     key_segments = k.split(\".\")\n",
    "#     if key_segments[0] == \"transformer\":\n",
    "#         if key_segments[1] == '0':\n",
    "#             print(key_segments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(pretrained, ours, strict = True):\n",
    "    ours.load_state_dict({\n",
    "        key_map(k): v \n",
    "        for k, v in pretrained.state_dict().items()\n",
    "        if not skip_key(k)\n",
    "    }, strict = strict)\n",
    "\n",
    "transfer_weights(pretrained_bert, my_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def ascii_art_probs(inp, k = 5):\n",
    "    input_list = tokenizer(inp)['input_ids']\n",
    "    mask_id = input_list.index(103)\n",
    "    input_ids = t.tensor([input_list])\n",
    "    out = my_bert(input_ids)\n",
    "    soft = t.softmax(out[0,mask_id,:], dim = 0)\n",
    "    vals, inds = t.topk(soft, k, dim = 0)\n",
    "    print(inp.replace('[MASK]', '___'))\n",
    "    mask_location = inp.index('[MASK]')\n",
    "    preds = [f\"{vals[i]:.0%}\".rjust(mask_location-1) + ' ' + tokenizer.decode(inds[i]) for i in range(k)]\n",
    "    print('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 + 12 = ___.\n",
      "      10% 0\n",
      "      10% 1\n",
      "       6% 2\n",
      "       4% 3\n",
      "       4% 4\n"
     ]
    }
   ],
   "source": [
    "ascii_art_probs(\"58 + 12 = [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "class BertClassifier(Bert):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        trans_out = self.transformer(self.embedding(input_ids, token_type_ids))\n",
    "        class_logits = self.class_head(trans_out)[:, 0, :]\n",
    "        return self.lm_head(trans_out), class_logits\n",
    "\n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train = list(data_train)\n",
    "data_test = list(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_set: list, batch_size=16, max_seq_length=512, num_batches = None):\n",
    "    def process_element(el):\n",
    "        sentiment, review = el\n",
    "        numerical_sent = 1 if sentiment==\"pos\" else 0\n",
    "        tokens = tokenizer(review)['input_ids']\n",
    "        tokens = tokens[:max_seq_length - 1] + [102] if len(tokens) > max_seq_length - 1 else tokens\n",
    "        return numerical_sent, tokens\n",
    "\n",
    "    def pad(tokens, pad_length):\n",
    "        return tokens + [0]*(pad_length - len(tokens)) \n",
    "\n",
    "    def make_batch(start_idx, data_set):\n",
    "        batch_data =[process_element(el) for el in data_set[start_idx:start_idx+batch_size]]\n",
    "        longest = max([len(tokens) for _, tokens in batch_data])\n",
    "        token_tensor = t.tensor([pad(tokens, longest) for _, tokens in batch_data])\n",
    "        sent_tensor = t.tensor([sent for sent, _ in batch_data])\n",
    "        return (token_tensor, sent_tensor)\n",
    "\n",
    "    data_set.sort(key= lambda tup: len(tup[1]), reverse=True)\n",
    "    batch_start_idxs = list(range(0, len(data_set), batch_size))\n",
    "    random.shuffle(batch_start_idxs)\n",
    "    if num_batches is None:\n",
    "        return (make_batch(idx, data_set) for idx in batch_start_idxs)\n",
    "    else:\n",
    "        return (make_batch(idx, data_set) for idx in batch_start_idxs[:num_batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 47.54 GiB total capacity; 45.07 GiB already allocated; 10.75 MiB free; 45.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cedd54c7ab83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     num_heads=12, num_layers=12)\n\u001b[1;32m     45\u001b[0m \u001b[0mtransfer_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mour_bert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mour_bert_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtrain_sentiment_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_bert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 47.54 GiB total capacity; 45.07 GiB already allocated; 10.75 MiB free; 45.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def eval_classifier(model, data_test):\n",
    "    model.eval()\n",
    "    CE_Loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    batched_test = process_data(data_test)\n",
    "    test_losses = []\n",
    "    for batch_data, batch_sentiment in batched_test:\n",
    "        batch_data.to(device)\n",
    "        batch_sentiment.to(device)\n",
    "        _, pred_class_logits = model(batch_data) # pred_class_logits: [batch, classes]\n",
    "        loss = CE_Loss(pred_class_logits, batch_sentiment).detach().item()\n",
    "        test_losses.append(loss)\n",
    "    \n",
    "    return sum(test_losses)/len(test_losses)\n",
    "\n",
    "def train_sentiment_classifier(model, data_train: list, data_test: list, lr=1e-5, epochs=1):\n",
    "    opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    CE_Loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batched_train = process_data(data_train)\n",
    "\n",
    "        model.train()\n",
    "        for batch_data, batch_sentiment in tqdm(batched_train):\n",
    "            batch_data.to(device)\n",
    "            batch_sentiment.to(device)\n",
    "            opt.zero_grad()\n",
    "            _, pred_class_logits = model(batch_data) # pred_class_logits: [batch, classes]\n",
    "            loss = CE_Loss(pred_class_logits, batch_sentiment)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(t.cuda.memory_allocated(0))\n",
    "\n",
    "        print(\"epoch done\")\n",
    "        print(t.cuda.memory_allocated(0))\n",
    "        test_loss = eval_classifier(model, data_test)\n",
    "        print(f\"epoch {epoch}/{epochs}: {test_loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "our_bert_classifier = BertClassifier(num_classes = 2, vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12)\n",
    "transfer_weights(pretrained_bert, our_bert_classifier, strict = False)\n",
    "our_bert_classifier.to(device)\n",
    "\n",
    "train_sentiment_classifier(our_bert_classifier, data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def eval_classifier(model, data_test, batch_size):\n",
    "    model.eval()\n",
    "    CE_Loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    batched_test = process_data(data_test, batch_size=batch_size, num_batches = 10)\n",
    "    test_losses = []\n",
    "    acc_cnt = 0\n",
    "    tot_cnt = 0\n",
    "    for batch_data, batch_sentiment in batched_test:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_sentiment = batch_sentiment.to(device)\n",
    "        _, pred_class_logits = model(batch_data) # pred_class_logits: [batch, classes]\n",
    "        loss = CE_Loss(pred_class_logits, batch_sentiment).detach().item()\n",
    "        test_losses.append(loss)\n",
    "        acc_cnt += (t.argmax(pred_class_logits, dim = 1) == batch_sentiment).sum()\n",
    "        tot_cnt += batch_data.shape[0] \n",
    "    \n",
    "    return sum(test_losses)/len(test_losses), acc_cnt/tot_cnt \n",
    "\n",
    "def train_sentiment_classifier(model, data_train: list, data_test: list, lr=5e-6, epochs=30, batch_size = 16):\n",
    "    opt = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    CE_Loss = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    for epoch in range(epochs):\n",
    "        batched_train = process_data(data_train, batch_size=batch_size, num_batches = 100)\n",
    "\n",
    "        for batch_data, batch_sentiment in tqdm(batched_train):\n",
    "            model.train()\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_sentiment = batch_sentiment.to(device)\n",
    "            opt.zero_grad()\n",
    "            _, pred_class_logits = model(batch_data) # pred_class_logits: [batch, classes]\n",
    "            loss = CE_Loss(pred_class_logits, batch_sentiment)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        print(\"epoch done\")\n",
    "        test_loss, pred_acc = eval_classifier(model, data_test, batch_size)\n",
    "        losses.append(test_loss)\n",
    "        acc.append(pred_acc)\n",
    "        print(f\"epoch {epoch}/{epochs}: {test_loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    return losses, acc\n",
    "\n",
    "our_bert_classifier = BertClassifier(num_classes = 2, vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12)\n",
    "transfer_weights(pretrained_bert, our_bert_classifier, strict = False)\n",
    "our_bert_classifier.to(device)\n",
    "\n",
    "losses, acc = train_sentiment_classifier(our_bert_classifier, data_train, data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 28288, 196)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_data = data_test[22000] \n",
    "tokenized = process_data([fun_data], batch_size = 1)\n",
    "d, s = next(tokenized)\n",
    "our_bert_classifier.eval()\n",
    "print(f\"pos prob: {t.softmax(our_bert_classifier(d)[1], dim = 1)[0][1].item():.2%}\") \n",
    "print(fun_data[1], fun_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data_train, wiki_data_valid, wiki_data_test = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid', 'test'))\n",
    "wiki_data_train = list(wiki_data_train)\n",
    "wiki_data_valid = list(wiki_data_valid)\n",
    "wiki_data_test = list(wiki_data_test)\n",
    "random.shuffle(wiki_data_train)\n",
    "random.shuffle(wiki_data_valid)\n",
    "random.shuffle(wiki_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask_data(data_set: list, batch_size=16, max_seq_length=512, num_batches = None):\n",
    "    def process_element(el):\n",
    "        tokens = tokenizer(el)['input_ids']\n",
    "        tokens = tokens[:max_seq_length - 1] + [102] if len(tokens) > max_seq_length - 1 else tokens\n",
    "        tokens = t.tensor(tokens)\n",
    "        tmp_rand = t.rand(size = tokens.shape)\n",
    "        tmp_rand_int = t.randint(low = 0, high = 28996, size = tokens.shape)\n",
    "        predict = (tmp_rand < 0.15).bool()\n",
    "        original_tokens = tokens.detach().clone() \n",
    "        tokens = t.where(tmp_rand < 0.12, tokenizer.mask_token_id, tokens)\n",
    "        tokens = t.where((0.12 <= tmp_rand) & (tmp_rand < 0.135), tmp_rand_int, tokens)\n",
    "        return tokens, predict, original_tokens\n",
    "\n",
    "    def pad(tokens, pad_length):\n",
    "        return t.cat((tokens, t.zeros((pad_length - tokens.shape[0]), dtype = tokens.dtype)))\n",
    "\n",
    "    def make_batch(start_idx, data_set):\n",
    "        batch_data =[process_element(el) for el in data_set[start_idx:start_idx+batch_size]]\n",
    "        longest = max([tokens.shape[0] for tokens,_,__ in batch_data])\n",
    "        #print(longest, batch_data)\n",
    "        # print(longest)\n",
    "        # for tokens, predict, original in batch_data:\n",
    "        #     print(tokens[:10])\n",
    "        #     print(pad(tokens, longest))\n",
    "            \n",
    "        token_tensor = t.stack([pad(tokens, longest) for tokens, _p, _o in batch_data])\n",
    "        predict_tensor = t.stack([pad(predict, longest) for _t, predict, _o in batch_data])\n",
    "        original_tensor = t.stack([pad(original, longest) for _t, _p, original in batch_data])\n",
    "        return (token_tensor, predict_tensor, original_tensor)\n",
    "\n",
    "    data_set.sort(key = lambda s: len(s), reverse=True)\n",
    "    batch_start_idxs = list(range(0, len(data_set), batch_size))\n",
    "    random.shuffle(batch_start_idxs)\n",
    "    if num_batches is None:\n",
    "        return (make_batch(idx, data_set) for idx in batch_start_idxs)\n",
    "    else:\n",
    "        return (make_batch(idx, data_set) for idx in batch_start_idxs[:num_batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515814"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=384, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=1536, \n",
    "    num_heads=12, num_layers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10., 12., 37., 33., 22., 16., 12.,  9.,  8., 37.]),\n",
       " array([ 41. ,  88.1, 135.2, 182.3, 229.4, 276.5, 323.6, 370.7, 417.8,\n",
       "        464.9, 512. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def masked_loss(model, batch):\n",
    "    batch_token, batch_pred, batch_original = batch\n",
    "    logsoft = nn.LogSoftmax(dim = 2)\n",
    "    batch_token = batch_token.to(device)\n",
    "    batch_pred = batch_pred.to(device)\n",
    "    batch_original = batch_original.to(device)\n",
    "    if not t.any(batch_pred):\n",
    "        return None\n",
    "    out = model(batch_token)\n",
    "    ls = rearrange(logsoft(out), 'b n v -> v b n')[batch_original]\n",
    "    logprobs_of_correct_tokens = t.masked_select(ls, batch_pred)\n",
    "    return -logprobs_of_correct_tokens.mean()\n",
    "    \n",
    "\n",
    "def train_our_bert(model, data_train, data_test, lr = 0.001, epochs = 30):\n",
    "    opt = t.optim.Adam(model.parameters(), lr = lr)\n",
    "    for epoch in range(epochs):\n",
    "        batches = process_mask_data(data_train)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_cnt_batches = 0\n",
    "        for b in batches:\n",
    "            opt.zero_grad()\n",
    "            loss = masked_loss(model, b)\n",
    "            if loss is None:\n",
    "                continue \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.detach().item()\n",
    "            train_cnt_batches += 1\n",
    "        print(f\"train loss epoch {epoch+1}: {train_loss/train_cnt_batches}\")\n",
    "        \n",
    "        model.eval()\n",
    "        batches = process_mask_data(data_test, num_batches = 50)\n",
    "        tot_loss = 0\n",
    "        cnt_batches = 0\n",
    "        for b in batches:\n",
    "            loss = masked_loss(model, b)\n",
    "            if loss is None:\n",
    "                continue \n",
    "            tot_loss += loss\n",
    "            cnt_batches += 1\n",
    "        print(f\"test loss epoch {epoch+1}: {(tot_loss/cnt_batches).item()}\") \n",
    "        \n",
    "\n",
    "tiny_bert.to(device)\n",
    "train_our_bert(tiny_bert, wiki_data_train, wiki_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
