{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Optional\n",
    "import transformers\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import torchvision\n",
    "import einops\n",
    "\n",
    "DEVICES = [\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTJForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-j-6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=10, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Sequential(torch.nn.Linear(10, 10))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedGPTJBlock(torch.nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Taking output out from one-element tuple\n",
    "        [activations] = self.block(x)\n",
    "        return activations\n",
    "\n",
    "\n",
    "def save_gptj_blocks(model):\n",
    "    block_0 = torch.nn.Sequential(\n",
    "        model.transformer.wte,\n",
    "        model.transformer.drop,\n",
    "        *[WrappedGPTJBlock(model.transformer.h[i]) for i in range(7)]\n",
    "    )\n",
    "\n",
    "    block_1 = torch.nn.Sequential(\n",
    "        *[WrappedGPTJBlock(model.transformer.h[i]) for i in range(7, 7*2)]\n",
    "    )\n",
    "\n",
    "    block_2 = torch.nn.Sequential(\n",
    "        *[WrappedGPTJBlock(model.transformer.h[i]) for i in range(7*2, 7*3)]\n",
    "    )\n",
    "\n",
    "    block_3 = torch.nn.Sequential(\n",
    "        *[WrappedGPTJBlock(model.transformer.h[i]) for i in range(7*3, 7*4)],\n",
    "        model.transformer.ln_f,\n",
    "        model.score,\n",
    "    )\n",
    "\n",
    "    blocks = [block_0, block_1, block_2, block_3]\n",
    "\n",
    "    for i, block in enumerate(blocks):\n",
    "        torch.save(block, f\"gptj_block_{i}.pt\")\n",
    "\n",
    "\n",
    "# save_gptj_blocks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_our_model_to_theirs(our_model, their_model):\n",
    "    our_model.eval()\n",
    "    their_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        our_model = MultiGPUGPTJ(model)\n",
    "        inp = torch.randint(0, 100, (1, 2))\n",
    "        expected_outputs = their_model(inp).logits # shape: 1,2 -- batch num_class\n",
    "        actual_outputs = our_model(inp) # shape: 1,2,2 -- batch seq num_class\n",
    "\n",
    "        assert torch.allclose(expected_outputs, actual_outputs), f\"Got {actual_outputs} but expected {expected_outputs}\"\n",
    "\n",
    "\n",
    "# compare_our_model_to_theirs(our_model, their_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# TODO: find out why the following breaks `tokenizer(['hi how are you', 'something'], padding='longest').input_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From dataloader above, hardcoded\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train = list(data_train)\n",
    "data_test = list(data_test)\n",
    "print(len(data_train), len(data_test))\n",
    "\n",
    "def to_batches(data, batch_size=BATCH_SIZE, max_seq_len=MAX_SEQ_LEN):\n",
    "    sorted_data = sorted(data, key=lambda d: len(d[1]))\n",
    "    num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    batched_data = []\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch = sorted_data[batch_start:batch_end]\n",
    "        sentiments = torch.tensor([1 if s == 'pos' else 0 for s, r in batch])\n",
    "        reviews = [r for s, r in batch]\n",
    "        tokenization = tokenizer(reviews, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors=\"pt\")\n",
    "        review_tokens = tokenization.input_ids\n",
    "        batched_data.append((review_tokens, sentiments))\n",
    "    random.shuffle(batched_data)\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "data_batches = to_batches(data_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for i, (input, target) in enumerate(data_batches):\n",
    "        optimizer.zero_grad()\n",
    "        logits, class_logits = model(input.to(device))\n",
    "        loss = t.nn.functional.cross_entropy(class_logits, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"{i} {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_imdb(model, num_batches_to_use=None):\n",
    "    model.eval()\n",
    "    test_batches = to_batches(data_test, batch_size=8)\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    if num_batches_to_use is None:\n",
    "        num_batches_to_use = len(test_batches)\n",
    "    print(f'evaluating using {num_batches_to_use} batches')\n",
    "    for i, (input, target) in enumerate(test_batches):\n",
    "        logits, class_logits = model(input.to(device))\n",
    "        answers = t.argmax(class_logits, dim=-1)\n",
    "        total_correct += t.sum(answers == target.to(device))\n",
    "        total_samples += answers.shape[0]\n",
    "        print(i, total_correct, total_samples)\n",
    "        if i >= num_batches_to_use:\n",
    "            break\n",
    "    return total_correct / total_samples\n",
    "\n",
    "\n",
    "# eval_on_imdb(my_bert, num_batches_to_use=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARD_OPTIMIZER_STATE=False\n",
    "\"\"\"\n",
    "mini_batch_size 350: both pass\n",
    "mini_batch_size 375: sharding passes\n",
    "mini_batch_size 400: both fail\n",
    "\"\"\"\n",
    "\n",
    "LEADER = 0\n",
    "HIDDEN_SIZE = 4096\n",
    "\n",
    "\n",
    "def load_block(rank):\n",
    "    return torch.load(f\"block_{rank}.pt\")\n",
    "\n",
    "\n",
    "def run(rank, size, world_size):\n",
    "    \"\"\" Distributed function to be implemented later. \"\"\"\n",
    "    \n",
    "    device = DEVICES[rank]\n",
    "    block = load_block(rank).to(device)\n",
    "    \n",
    "    if rank == LEADER:\n",
    "        # If there's still data, fetch them\n",
    "        if data_batches:\n",
    "            inps, labels = next(data_batches)\n",
    "            inps.to(device)\n",
    "            labels.to(device)\n",
    "    else:\n",
    "        # Initialise inps: batch, seq_len, hidden_size (I'm guessing we don't consider num_heads, head_size??)\n",
    "        # We are fetching outputs from the last block\n",
    "        inps = torch.zeros(BATCH_SIZE, MAX_SEQ_LEN, HIDDEN_SIZE).to(device)\n",
    "        group = dist.new_group([rank-1, rank])\n",
    "        dist.broadcast(tensor=inps, src=rank-1, group=group)\n",
    "\n",
    "    # Put it through block\n",
    "    out = block(inps)\n",
    "    print(f\"Rank {rank} output {out.shape}\")\n",
    "    \n",
    "    # Send output on to next block\n",
    "    if rank < len(DEVICES) - 1:\n",
    "        group = dist.new_group([rank, rank+1])\n",
    "        dist.broadcast(tensor=out, src=rank, group=group)\n",
    "        if rank == LEADER:\n",
    "            # Send labels to last block\n",
    "            group = dist.new_group([rank, world_size - 1])\n",
    "            dist.broadcast(tensor=labels, src=rank, group=group)\n",
    "    else:\n",
    "        # Get labels from the first block\n",
    "        labels = torch.zeros(BATCH_SIZE).to(device)\n",
    "        group = dist.new_group([LEADER, world_size - 1])\n",
    "        dist.broadcast(tensor=labels, src=LEADER, group=group)\n",
    "\n",
    "        # Handle loss and backprop at last block\n",
    "        classification_logits = out[:, -1]\n",
    "        \n",
    "        # inputs [N, C] and targets [N]\n",
    "        loss = torch.nn.functional.cross_entropy(classification_logits, labels)\n",
    "        loss.backward()\n",
    "        print(loss.detach().item())\n",
    "\n",
    "#     if rank == 0:\n",
    "#         start_time = time.time()\n",
    "\n",
    "\n",
    "#     model = transformers.GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "#     if SHARD_OPTIMIZER_STATE:\n",
    "#         params_to_optimize = []\n",
    "#         for i, param in enumerate(model.parameters()):\n",
    "#             if i % size == 0:\n",
    "#                 params_to_optimize.append(param)\n",
    "#         optimizer = torch.optim.Adam(params_to_optimize, lr=1e-5)\n",
    "#     else:\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "#     # Iterate over minibatches\n",
    "#     model.train()\n",
    "#     for epoch in range(4):\n",
    "#         print(epoch)\n",
    "#         ddl = DistributedDataLoader(rank, len(DEVICES), 375, random_seed = epoch)\n",
    "#         for minibatch_data in ddl:\n",
    "#             optimizer.zero_grad()\n",
    "#             # Normal training loop\n",
    "#             # FIXME\n",
    "#             minibatch_data = {'input_ids': minibatch_data,\n",
    "#                             'attention_mask': torch.ones_like(minibatch_data, dtype=torch.long)}\n",
    "#             outputs = model(**minibatch_data, labels=minibatch_data['input_ids']) \n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             print(loss.detach())\n",
    "#             # All-reduce to share gradients, for each parameter\n",
    "#             for param in model.parameters():\n",
    "#                 old_grad = param.grad.detach().clone()\n",
    "#                 # Taking the mean over the gradients\n",
    "#                 dist.all_reduce(param.grad, dist.ReduceOp.SUM)\n",
    "#                 # assert not torch.allclose(old_grad, param.grad.detach())\n",
    "#                 param.grad = param.grad / size\n",
    "#             # Does it take real long? Maybe time optimizer.step() and dist.broadcast, and compare them\n",
    "#             optimizer.step()\n",
    "#             if SHARD_OPTIMIZER_STATE:\n",
    "#                 for i, param in enumerate(model.parameters()):\n",
    "#                     dist.broadcast(param.data, src=i % 3)\n",
    "\n",
    "#     print('Training completed')\n",
    "#     loss = 0.\n",
    "\n",
    "#     ddl = DistributedDataLoader(rank, len(DEVICES), 32, random_seed = epoch)\n",
    "\n",
    "#     if rank == 0:\n",
    "#         model.eval()\n",
    "#         test_data = ddl.test_dataloader\n",
    "#         c = 0\n",
    "#         for test_datum in test_data:\n",
    "#             test_datum = ddl.tokenize(test_datum)\n",
    "#             test_datum = {'input_ids': test_datum,\n",
    "#                         'attention_mask': torch.ones_like(test_datum, dtype=torch.long)}\n",
    "#             outputs = model(**test_datum, labels=test_datum['input_ids']) \n",
    "#             loss += outputs.loss.detach()\n",
    "#             c +=1\n",
    "#             if c % 10 == 0:\n",
    "#                 print(loss)\n",
    "#             if c > 100:\n",
    "#                 break    \n",
    "#         print(\"eval loss: \", loss / len(test_data) / c)  \n",
    "\n",
    "#         print(\"time: \", time.time() - start_time)          \n",
    "    # After 4 epochs evaluate on test set\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29503'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "    device = DEVICES[rank]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(DEVICES)\n",
    "processes = []\n",
    "# mp.set_start_method(\"spawn\")\n",
    "for rank in range(size):\n",
    "    p = mp.Process(target=init_process, args=(rank, size, run, \"gloo\"))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
