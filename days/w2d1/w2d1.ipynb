{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw attention pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.00134 STD: 0.1129 VALS [-0.01475 0.08565 -0.0173 0.08945 0.1001 -0.2143 -0.05152 -0.08566 0.03025 -0.003722...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    project_query: Callable[[t.Tensor], t.Tensor],\n",
    "    project_key: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)]\n",
    "    project_query: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    project_key:   function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "    \"\"\"\n",
    "\n",
    "    queries = rearrange(\n",
    "        project_query(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "    keys = rearrange(\n",
    "        project_key(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    head_size = t.tensor(keys.shape[-1])\n",
    "    return einsum(\"bhid, bhjd -> bhij\", keys, queries) / t.sqrt(head_size)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001894 STD: 0.1235 VALS [0.1045 -0.07578 0.009482 -0.2152 -0.0599 0.08476 -0.2925 -0.02358 -0.1737 0.05641...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    attention_pattern: t.Tensor,\n",
    "    project_value: Callable[[t.Tensor], t.Tensor],\n",
    "    project_output: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "    num_heads: int,\n",
    "    attention_pattern: Tensor[batch_size,num_heads, seq_length, seq_length],\n",
    "    project_value: function( (Tensor[..., 768]) -> Tensor[..., 768] ),\n",
    "    project_output: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, seq_length, hidden_size]\n",
    "    \"\"\"\n",
    "\n",
    "    attention_prob = t.softmax(attention_pattern, dim=-2)  # dim: b head s s\n",
    "    values = rearrange(\n",
    "        project_value(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    output_by_head = einsum(\"bhis, bhid -> bhsd\", attention_prob, values)\n",
    "    concatenated = rearrange(output_by_head, \"b h s d -> b s (h d)\")\n",
    "\n",
    "    return project_output(concatenated)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_size: int,\n",
    "        attention_dim: int = 64,\n",
    "        per_head_output_dim: int = 64,\n",
    "        output_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if output_dim is None:\n",
    "            output_dim = hidden_size\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.per_head_output_dim = per_head_output_dim\n",
    "        self.output_dim: int = output_dim\n",
    "\n",
    "        self.Q = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.K = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.V = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * per_head_output_dim\n",
    "        )\n",
    "        self.O = nn.Linear(\n",
    "            in_features=num_heads * per_head_output_dim, out_features=output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        input: Tensor[batch_size, seq_length, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        attention_pattern = raw_attention_pattern(\n",
    "            input,\n",
    "            self.num_heads,\n",
    "            project_key=self.K,\n",
    "            project_query=self.Q,\n",
    "        )\n",
    "\n",
    "        return bert_attention(\n",
    "            token_activations=input,\n",
    "            num_heads=self.num_heads,\n",
    "            attention_pattern=attention_pattern,\n",
    "            project_value=self.V,\n",
    "            project_output=self.O,\n",
    "        )\n",
    "\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 117, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhsa = MultiHeadedSelfAttention(\n",
    "    num_heads=17,\n",
    "    hidden_size=768,\n",
    ")\n",
    "mhsa(t.ones((10, 117, 768))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
