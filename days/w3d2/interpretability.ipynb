{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import w3d2_tests\n",
    "import transformers\n",
    "import torch\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = w3d2_tests.get_minigpt(\"model.pt\")\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer._add_tokens([\"[BEGIN]\", \"[END]\"])\n",
    "tokenizer.pad_token = \"[END]\"\n",
    "tokenizer.eos_token = \"[END]\"\n",
    "# 50258 is the pad_token_id\n",
    "# 50257 is the BEGIN token id\n",
    "\n",
    "with open(\"test_tokens_owt_subset.json\") as f:\n",
    "    test_tokens = torch.LongTensor(json.load(f))[:,:512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.project_qkv = nn.Linear(hidden_size, hidden_size * 3, bias=False)\n",
    "        self.project_output = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.n_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, pos_embedding):\n",
    "        batch, seq_len = x.shape[:2]\n",
    "        pos_ids = t.arange(x.shape[1]).unsqueeze(0).to(x.device)\n",
    "        pos_emb = pos_embedding(pos_ids)\n",
    "\n",
    "        q, k, _ = t.split(self.project_qkv(x + pos_emb), self.hidden_size, dim=-1)\n",
    "        _, _, v = t.split(self.project_qkv(x), self.hidden_size, dim=-1)\n",
    "        \n",
    "        q = einops.rearrange(q, 'b n (h l) -> b h n l', l=self.head_size)\n",
    "        k = einops.rearrange(k, 'b n (h l) -> b h n l', l=self.head_size)\n",
    "        v = einops.rearrange(v, 'b n (h l) -> b h n l', l=self.head_size)\n",
    "        \n",
    "        neg_inf = t.tensor(-1e4).to(x.device)\n",
    "        q_ind = t.arange(seq_len).unsqueeze(1)\n",
    "        k_ind = t.arange(seq_len).unsqueeze(0)\n",
    "        mask = (q_ind < k_ind).to(x.device)\n",
    "        attn_scores = t.einsum('bhql, bhkl -> bhqk', q, k) / math.sqrt(self.head_size)\n",
    "        attn_scores = t.where(mask, neg_inf, attn_scores)\n",
    "\n",
    "        self._attn_scores = attn_scores.detach()[0]\n",
    "        probs = attn_scores.softmax(dim=-1)\n",
    "        combined_v = t.einsum('bhqk, bhkl -> bhql', probs, v)\n",
    "        combined_v = einops.rearrange(combined_v, 'b h q l -> b q (h l)')\n",
    "        self._combined_v = combined_v\n",
    "        out = self.project_output(combined_v)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MiniGPT(nn.Module):\n",
    "#     def __init__(self, num_heads=8, vocab_size=50259, hidden_size=256,\n",
    "#                  max_position_embeddings=512):\n",
    "#         super().__init__()\n",
    "#         self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "#         self.pos_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "#         self.blocks = nn.Sequential(\n",
    "#             UniAttention(hidden_size, num_heads),\n",
    "#             UniAttention(hidden_size, num_heads),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, input_ids):\n",
    "#         emb = self.token_embedding(input_ids)\n",
    "#         for block in self.blocks:\n",
    "#             emb = emb + block(emb, self.pos_embedding)\n",
    "#         return t.einsum('bnl, vl -> bnv', emb, self.token_embedding.weight)\n",
    "\n",
    "    \n",
    "    \n",
    "def weight_matrix(model, qkvo: str, layer: int, head: int):\n",
    "    layer = model.blocks[layer]\n",
    "    if qkvo == 'o':\n",
    "        res = layer.project_output.weight[:, head*layer.head_size:(head+1)*layer.head_size]\n",
    "        return res\n",
    "    weights = layer.project_qkv.weight\n",
    "    q, k, v = t.split(weights, layer.hidden_size, dim=0)\n",
    "    # Shapes are: output, input\n",
    "    # QKV map to each head, that's why that's the head*layer... is on LHS\n",
    "    if qkvo == 'q':\n",
    "        return q[head*layer.head_size:(head+1)*layer.head_size, :]\n",
    "    elif qkvo == 'k':\n",
    "        return k[head*layer.head_size:(head+1)*layer.head_size, :]\n",
    "    elif qkvo == 'v':\n",
    "        return v[head*layer.head_size:(head+1)*layer.head_size, :]\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected qkvo value \"{qkvo}\"')\n",
    "\n",
    "# w3d2_tests.test_weight_matrix(weight_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def attention_presoftmax(model, input_ids, neg_inf=-1e4):\n",
    "    model(input_ids)\n",
    "    neg_inf = t.tensor(neg_inf).to(input_ids.device)\n",
    "    \n",
    "    batch, seq_len = input_ids.shape[:2]\n",
    "    q_ind = t.arange(seq_len).unsqueeze(1)\n",
    "    k_ind = t.arange(seq_len).unsqueeze(0)\n",
    "    \n",
    "    mask = (q_ind < k_ind).to(input_ids.device)\n",
    "    all_attn_scores = []\n",
    "    \n",
    "    for layer in model.blocks:\n",
    "        all_attn_scores.append(t.where(mask, neg_inf, layer._attn_scores))\n",
    "    \n",
    "    # layer._attn_scores shape: batch_size, hidden_size, seq_len, seq_len\n",
    "    result = t.stack(all_attn_scores, dim=0)\n",
    "    return result\n",
    "\n",
    "w3d2_tests.test_attention_presoftmax(attention_presoftmax)\n",
    "\n",
    "def attention_softmaxed(model, input_ids):\n",
    "    attn_scores = attention_presoftmax(model, input_ids)\n",
    "    return attn_scores.softmax(dim=-1)\n",
    "\n",
    "w3d2_tests.test_attention_softmaxed(attention_softmaxed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def layer0_embedding_contributions(model, input_ids):\n",
    "    token_embeddings = model.token_embedding(input_ids)\n",
    "    model(input_ids)\n",
    "    \n",
    "    # what's in the doc: batch_size, seq_len, hidden_size\n",
    "    # what we have? batch_size, num_heads, seq_len, head_size\n",
    "    block = model.blocks[0]\n",
    "    combined_v = block._combined_v\n",
    "    \n",
    "    split_v = einops.rearrange(combined_v, 'batch_size seq_len (num_heads head_size) -> num_heads batch_size seq_len head_size', \n",
    "                               num_heads=block.n_heads)\n",
    "\n",
    "    layer_head_outputs = [t.einsum('bsn, on -> bso', split_v[i], weight_matrix(model, 'o', 0, i)) for i in range (block.n_heads)]\n",
    "    return [token_embeddings] + layer_head_outputs\n",
    "        \n",
    "w3d2_tests.test_layer0_embedding_contributions(layer0_embedding_contributions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected[1][1]=tensor([[[-4.9771e-04, -9.7052e-03,  1.4165e-02,  ..., -7.1923e-03,\n",
      "           8.7951e-03,  1.2432e-02],\n",
      "         [-5.4336e-04, -1.4060e-02,  1.2033e-02,  ..., -1.4916e-02,\n",
      "           1.0521e-02,  4.1670e-03],\n",
      "         [-7.6825e-05, -1.4130e-02,  1.0433e-02,  ..., -1.6977e-02,\n",
      "           1.1078e-02,  1.5323e-03],\n",
      "         ...,\n",
      "         [-4.3942e-04, -1.0419e-02,  9.4183e-03,  ..., -1.8466e-02,\n",
      "           1.0260e-02,  7.1619e-06],\n",
      "         [ 2.1126e-04, -8.7242e-03,  7.5222e-03,  ..., -2.0696e-02,\n",
      "           9.9822e-03, -2.0267e-03],\n",
      "         [-5.2381e-05, -6.1574e-03,  6.6062e-03,  ..., -1.7786e-02,\n",
      "           8.5225e-03, -8.6275e-04]]], grad_fn=<ViewBackward0>)\n",
      "9\n",
      "torch.Size([1, 10, 256])\n",
      "actual[1][1]=tensor([[[-4.9771e-04, -9.7052e-03,  1.4165e-02,  ..., -7.1923e-03,\n",
      "           8.7951e-03,  1.2432e-02],\n",
      "         [-5.4559e-04, -1.4272e-02,  1.1930e-02,  ..., -1.5292e-02,\n",
      "           1.0605e-02,  3.7644e-03],\n",
      "         [ 7.5567e-05, -1.3327e-02,  1.0344e-02,  ..., -1.6149e-02,\n",
      "           1.0923e-02,  2.2854e-03],\n",
      "         ...,\n",
      "         [-3.9393e-04, -9.9825e-03,  8.2672e-03,  ..., -2.0986e-02,\n",
      "           1.0415e-02, -2.4102e-03],\n",
      "         [-1.8904e-04, -8.3843e-03,  7.6967e-03,  ..., -2.1251e-02,\n",
      "           9.9837e-03, -2.9499e-03],\n",
      "         [-4.1813e-06, -6.5841e-03,  6.7759e-03,  ..., -1.8634e-02,\n",
      "           8.8756e-03, -1.4473e-03]]], grad_fn=<ViewBackward0>)\n",
      "actual[1][1] - expected[1][1]=tensor([[[-1.3970e-09,  0.0000e+00, -4.6566e-09,  ...,  1.8626e-09,\n",
      "           1.8626e-09,  3.7253e-09],\n",
      "         [-2.2247e-06, -2.1211e-04, -1.0383e-04,  ..., -3.7614e-04,\n",
      "           8.4060e-05, -4.0255e-04],\n",
      "         [ 1.5239e-04,  8.0314e-04, -8.9386e-05,  ...,  8.2713e-04,\n",
      "          -1.5515e-04,  7.5304e-04],\n",
      "         ...,\n",
      "         [ 4.5492e-05,  4.3611e-04, -1.1511e-03,  ..., -2.5195e-03,\n",
      "           1.5437e-04, -2.4174e-03],\n",
      "         [-4.0030e-04,  3.3986e-04,  1.7449e-04,  ..., -5.5474e-04,\n",
      "           1.5283e-06, -9.2328e-04],\n",
      "         [ 4.8200e-05, -4.2661e-04,  1.6970e-04,  ..., -8.4866e-04,\n",
      "           3.5315e-04, -5.8452e-04]]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cefa222bd0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mw3d2_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_layer1_embedding_contributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_embedding_contributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/yulong-tim/mlab/days/w3d2/w3d2_tests.py\u001b[0m in \u001b[0;36mtest_layer1_embedding_contributions\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{actual[1][1]=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{actual[1][1] - expected[1][1]=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0m_check_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yulong-tim/mlab/days/w3d2/w3d2_tests.py\u001b[0m in \u001b[0;36m_check_equal\u001b[0;34m(tensor1, tensor2, print_congrats)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0m_check_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_congrats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yulong-tim/mlab/days/w3d2/w3d2_tests.py\u001b[0m in \u001b[0;36m_check_equal\u001b[0;34m(tensor1, tensor2, print_congrats)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0m_check_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_congrats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yulong-tim/mlab/days/w3d2/w3d2_tests.py\u001b[0m in \u001b[0;36m_check_equal\u001b[0;34m(tensor1, tensor2, print_congrats)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0m_check_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_congrats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_congrats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Congrats! You've passed the test!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def layer1_embedding_contributions(model, input_ids):\n",
    "    \n",
    "    token_embeddings = model.token_embedding(input_ids)\n",
    "    model(input_ids)\n",
    "    \n",
    "    layer0_contributions = layer0_embedding_contributions(model, input_ids)\n",
    "    print(len(layer0_contributions))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for layer0_contribution in layer0_contributions:\n",
    "        layer_outputs = [layer0_contribution]\n",
    "        block = model.blocks[1]\n",
    "        block(layer0_contribution, model.pos_embedding)\n",
    "        combined_v = block._combined_v\n",
    "    \n",
    "        split_v = einops.rearrange(combined_v, \n",
    "                                   'batch_size seq_len (num_heads head_size) -> num_heads batch_size seq_len head_size', \n",
    "                                   num_heads=block.n_heads)\n",
    "        layer_outputs += [t.einsum('bsn, on -> bso', split_v[i], weight_matrix(model, 'o', 1, i)) for i in range (block.n_heads)]\n",
    "        results.append(layer_outputs)\n",
    "    print(results[2][2].shape)\n",
    "    \n",
    "    # each item is: batch_size, seq_len, hidden_size\n",
    "    return results\n",
    "\n",
    "w3d2_tests.test_layer1_embedding_contributions(layer1_embedding_contributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.allclose(torch.tensor(1), torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_attention(model, layer, head, input_ids, tokenizer=tokenizer):\n",
    "    attentions = attention_softmaxed(model, input_ids)\n",
    "    plt.imshow(attentions[layer, head])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 17250,   703,   389,   345,    30, 50258],\n",
       "         [50257,    40,   716,  1804,   880,    13, 50258]]),\n",
       " torch.Size([2, 7]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(['[BEGIN] Hi how are you? [END]', '[BEGIN] I am doing well. [END]'], return_tensors='pt')['input_ids']\n",
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(2):\n",
    "    for head in range(8):\n",
    "        print(f\"{layer=}\", f\"{head=}\")\n",
    "        vis_attention(model, layer=layer, head=head, input_ids=input_ids)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "torch.Size([1, 12, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-bd0d440d3b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer1_contributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer1_embedding_contributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlayer1_contributions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(['[BEGIN] Hi how are you. Hi really good. Hi.'], return_tensors='pt')['input_ids']\n",
    "\n",
    "layer1_contributions = layer1_embedding_contributions(model, input_ids)\n",
    "layer1_contributions[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.7769,  0.7754, 18.6025, 21.0893, 20.2699, 16.5687, 15.5548, 29.6031,\n",
       "          13.2295, 15.0390, 16.8253, 30.0753]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.3200, 0.3199, 2.3385, 3.7999, 3.7689, 2.8225, 2.4293, 2.7209, 2.7246,\n",
       "          3.1062, 2.7983, 3.4419]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.3694, 0.3693, 0.4882, 1.1471, 1.1320, 0.8694, 1.6841, 0.6595, 1.1185,\n",
       "          0.9741, 2.3032, 0.8091]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.5374, 0.5370, 2.5654, 3.2426, 4.1631, 3.7899, 3.1187, 4.1566, 4.0884,\n",
       "          4.0029, 3.1161, 4.7299]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.4345, 0.4338, 3.5867, 4.7422, 4.6748, 4.1146, 4.2799, 4.6940, 4.1544,\n",
       "          3.7634, 4.3731, 4.8611]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.6660, 0.6660, 0.6424, 0.6541, 0.6765, 0.6750, 0.6819, 0.6618, 0.6563,\n",
       "          0.6743, 0.6728, 0.6685]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.5457, 0.5457, 0.9964, 1.5978, 1.4584, 1.5005, 1.7658, 2.2193, 2.5046,\n",
       "          2.7745, 2.9028, 3.4151]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.5306, 0.5304, 0.7729, 1.3099, 1.5071, 1.6909, 2.0641, 2.1725, 2.7351,\n",
       "          2.9779, 3.3001, 3.0967]], grad_fn=<SumBackward1>),\n",
       " tensor([[0.3589, 0.3589, 0.9040, 1.3295, 0.8455, 0.6048, 1.0359, 3.4871, 2.0359,\n",
       "          3.6593, 1.8180, 3.7908]], grad_fn=<SumBackward1>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.abs().sum(dim=-1) for i in layer1_contributions[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uctpenicceryolengressrigesticocateotic',\n",
       " 'uctpenicceryolengressrigesticocateotic',\n",
       " 'Civil Dull Appalach Asgardsecure CrowunningUrban Voldemortiant',\n",
       " ' Thiel GPS mainland Tibet WiSnowAllenCivil Dull Maps',\n",
       " ' TibetChina Thiel Gibraltar GPS mainland TibetanijingVPN Swansea',\n",
       " ' TibetChinaijingVPN Tibetan Thiel Beijing Snowden PruittPrivacy',\n",
       " ' TibetChinaijing BeijingPrivacy Tibetan China Thiel GPS Dalai',\n",
       " ' TibetChina BeijingPrivacyijing Tibetan Thiel GPS China Dalai',\n",
       " ' TibetChinaPrivacy Beijing Thielijing China Bezos Tibetan Dalai',\n",
       " ' TibetChinaPrivacy Beijing Dalaiijing Bezos China Tibetan Alibaba',\n",
       " ' TibetChina BeijingPrivacy China Bezos Dalai Tibetanijing Alibaba',\n",
       " 'China TibetPrivacy Beijing Bezos China Dalai Tibetan Thielijing']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unembed l0h0, l1h4 and check the tokens\n",
    "# emb = layer1_contributions[1][5] + layer1_contributions[0][0]\n",
    "emb = layer1_contributions[1][5] + layer1_contributions[1][7]\n",
    "\n",
    "logits = t.einsum('bnl, vl -> bnv', emb, model.token_embedding.weight)\n",
    "tokenizer.batch_decode(torch.topk(logits, k=10, dim=-1).indices.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.abs().sum() for i in layer1_contributions[1]]\n",
    "\n",
    "# Take unembedding of this, and see if there's copying behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.sum() for i in layer1_contributions[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.abs().sum() for i in layer1_contributions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.abs().sum() for i in layer1_contributions[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = attention_softmaxed(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_head(model, embedding, layer0_head_idx, layer1_head_idx, attn_precomputed):\n",
    "    probs = attn_precomputed\n",
    "\n",
    "    if layer0_head_idx == model.blocks[0].n_heads:\n",
    "        out = embedding\n",
    "    else:\n",
    "        Wv0 = weight_matrix(model, 'v', 0, layer0_head_idx)\n",
    "        Wo0 = weight_matrix(model, 'o', 0, layer0_head_idx)\n",
    "\n",
    "        v = t.einsum('bsi, ji -> bsj', embedding, Wv0)\n",
    "        combined_v = t.einsum('qk, bkl -> bql', probs[0, layer0_head_idx], v)\n",
    "        out = t.einsum('bsi, ji -> bsj', combined_v, Wo0)\n",
    "    \n",
    "    if layer1_head_idx == model.blocks[1].n_heads:\n",
    "        embeddings = embedding\n",
    "    else:\n",
    "        Wv1 = weight_matrix(model, 'v', 1, layer1_head_idx)\n",
    "        Wo1 = weight_matrix(model, 'o', 1, layer1_head_idx)\n",
    "\n",
    "        v = t.einsum('bsi, ji -> bsj', out, Wv1)\n",
    "        combined_v = t.einsum('qk, bkl -> bql', probs[1, layer1_head_idx], v)\n",
    "        embeddings = t.einsum('bsi, ji -> bsj', combined_v, Wo1)\n",
    "    return t.einsum('bnl, vl -> bnv', embeddings, model.token_embedding.weight)\n",
    "\n",
    "\n",
    "def all_virtual_attn_heads(model, sentence):\n",
    "    input_ids = tokenizer([sentence], return_tensors='pt')['input_ids']\n",
    "    token_embeddings = model.token_embedding(input_ids)\n",
    "    attn_precomputed = attention_softmaxed(model, input_ids)\n",
    "    \n",
    "    out = []\n",
    "    for i in range(model.blocks[0].n_heads+1):\n",
    "        row = []\n",
    "        for j in range(model.blocks[0].n_heads+1):\n",
    "            row.append(virtual_head(model, token_embeddings, i, j, attn_precomputed))\n",
    "        out.append(t.stack(row, dim=1))\n",
    "    x = t.stack(out, dim=1)\n",
    "    return einops.rearrange(x, 'b i j n h -> b n i j h')\n",
    "\n",
    "v_a_h = all_virtual_attn_heads(model, '[BEGIN] Hi how are you. Hi really good. Hi.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_a_h = v_a_h.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 9, 9, 50259])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_a_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd2db95940>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(v_a_h.sum(dim=[0,-1]).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
