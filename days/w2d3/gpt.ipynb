{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch\n",
    "from torch import einsum\n",
    "from torch import nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import mlab_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(t.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_size,hidden_size*3)\n",
    "        self.output_projection = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, past_key_values=None, return_key_values=False):\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            k, v = torch.tensor_split(past_key_values.unsqueeze(0), 2, 3)\n",
    "            out = self.attention(x)\n",
    "            q, k_final, v_final = out.tensor_split(3,dim=-1)\n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            k_final = rearrange(k_final, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            v_final = rearrange(v_final, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            \n",
    "            k = torch.cat((k, k_final), 2)\n",
    "            v = torch.cat((v, v_final), 2)\n",
    "            attention_score = einsum(\"bhtc,bhfc->bhft\", q, k)\n",
    "\n",
    "            \n",
    "        else:     \n",
    "            out = self.attention(x)\n",
    "            q, k, v = out.tensor_split(3,dim=-1)\n",
    "            \n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            k = rearrange(k, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            v = rearrange(v, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "\n",
    "            attention_score = einsum(\"bhtc,bhfc->bhft\", q, k)\n",
    "                        \n",
    "            attention_score = t.triu(attention_score) - 1e4*t.tril(t.ones_like(attention_score), diagonal = -1)\n",
    "                \n",
    "        scaled_attn_score = 1/torch.sqrt(t.tensor(self.head_size)) * attention_score\n",
    "                \n",
    "        attention_pattern = t.nn.Softmax(dim = -2)(scaled_attn_score)\n",
    "            \n",
    "        out = einsum(\"bhft,bhfc -> bhtc\", attention_pattern, v)\n",
    "                \n",
    "        out = rearrange(out, \"b h n c -> b n (h c)\")\n",
    "        \n",
    "        out = self.output_projection(out)\n",
    "        \n",
    "        if return_key_values and past_key_values is not None:\n",
    "            return out, torch.cat((k_final, v_final), 3)\n",
    "        else:\n",
    "            return out\n",
    "# gpt_tests.test_attn_cache(Attention)\n",
    "# gpt_tests.test_unidirectional_attn(Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our out tensor([[[-2.4267e-01, -6.7535e-01,  1.6882e-03,  1.5479e-01,  1.8791e-01,\n",
      "           1.0629e-01, -2.1500e-01,  5.4569e-01, -2.8674e-01,  1.5972e-01,\n",
      "           1.9140e-01, -4.1335e-02,  3.4724e-01, -4.7367e-01, -3.7063e-01,\n",
      "          -3.4441e-01, -1.9860e-01,  4.2505e-01,  4.3122e-01,  2.2667e-01,\n",
      "          -2.7387e-03,  2.3899e-02,  1.7695e-01,  2.5153e-01],\n",
      "         [-4.8123e-01, -2.6815e-01, -1.7251e-01,  9.6488e-02,  1.1933e-01,\n",
      "           7.1637e-03,  1.1151e-01,  3.4887e-01,  1.9976e-01,  7.5051e-02,\n",
      "           2.6465e-01, -1.3239e-01,  2.2056e-01, -1.8246e-01, -2.0666e-01,\n",
      "          -2.6338e-01, -2.2939e-01,  3.9007e-01,  1.1968e-01,  3.3997e-01,\n",
      "           4.3774e-01, -3.1492e-02,  5.4310e-02,  1.2460e-01],\n",
      "         [-3.3502e-01, -4.1158e-01,  3.3808e-02,  4.1109e-03,  1.8947e-01,\n",
      "           1.1793e-01, -1.7818e-02,  2.9381e-01,  7.5902e-02,  4.2964e-02,\n",
      "           9.8588e-02, -4.1910e-01,  2.6740e-01, -1.0821e-01,  3.8267e-03,\n",
      "          -9.4716e-02, -1.8388e-01,  3.1332e-01,  4.5938e-02,  5.0622e-01,\n",
      "           4.3415e-01,  1.3947e-01,  6.4570e-02,  1.0938e-01],\n",
      "         [-2.5416e-01, -2.1348e-01,  1.9283e-01, -3.1187e-03,  4.0976e-02,\n",
      "           3.0519e-02,  1.0291e-02,  1.3009e-01,  3.2573e-01, -1.1999e-01,\n",
      "           1.2030e-01, -2.3412e-01,  1.9310e-01, -1.1347e-01, -8.6918e-02,\n",
      "          -4.1033e-02, -1.6452e-01,  1.0412e-01, -1.7548e-01,  2.8392e-01,\n",
      "           2.8952e-01, -7.6336e-02,  1.4336e-01, -2.7292e-01],\n",
      "         [-2.2337e-01, -3.6899e-01,  1.2185e-01,  1.4037e-01,  1.4051e-01,\n",
      "           1.2121e-01, -1.4649e-01,  2.7230e-01,  2.0667e-01, -1.8062e-01,\n",
      "           8.9460e-02, -3.8905e-01,  2.3613e-01, -7.1509e-02, -2.3903e-02,\n",
      "           1.9587e-04, -1.0819e-01,  2.3449e-01, -2.2003e-02,  4.5285e-01,\n",
      "           2.5873e-01, -1.0152e-02,  1.5114e-01, -1.9349e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "their out tensor([[[-2.4267e-01, -6.7535e-01,  1.6882e-03,  1.5479e-01,  1.8791e-01,\n",
      "           1.0629e-01, -2.1500e-01,  5.4569e-01, -2.8674e-01,  1.5972e-01,\n",
      "           1.9140e-01, -4.1335e-02,  3.4724e-01, -4.7367e-01, -3.7063e-01,\n",
      "          -3.4441e-01, -1.9860e-01,  4.2505e-01,  4.3122e-01,  2.2667e-01,\n",
      "          -2.7387e-03,  2.3899e-02,  1.7695e-01,  2.5153e-01],\n",
      "         [-4.8123e-01, -2.6815e-01, -1.7251e-01,  9.6488e-02,  1.1933e-01,\n",
      "           7.1637e-03,  1.1151e-01,  3.4887e-01,  1.9976e-01,  7.5051e-02,\n",
      "           2.6465e-01, -1.3239e-01,  2.2056e-01, -1.8246e-01, -2.0666e-01,\n",
      "          -2.6338e-01, -2.2939e-01,  3.9007e-01,  1.1968e-01,  3.3997e-01,\n",
      "           4.3774e-01, -3.1492e-02,  5.4310e-02,  1.2460e-01],\n",
      "         [-3.3502e-01, -4.1158e-01,  3.3808e-02,  4.1109e-03,  1.8947e-01,\n",
      "           1.1793e-01, -1.7818e-02,  2.9381e-01,  7.5902e-02,  4.2964e-02,\n",
      "           9.8588e-02, -4.1910e-01,  2.6740e-01, -1.0821e-01,  3.8267e-03,\n",
      "          -9.4716e-02, -1.8388e-01,  3.1332e-01,  4.5938e-02,  5.0622e-01,\n",
      "           4.3415e-01,  1.3947e-01,  6.4570e-02,  1.0938e-01],\n",
      "         [-2.5416e-01, -2.1348e-01,  1.9283e-01, -3.1187e-03,  4.0976e-02,\n",
      "           3.0519e-02,  1.0291e-02,  1.3009e-01,  3.2573e-01, -1.1999e-01,\n",
      "           1.2030e-01, -2.3412e-01,  1.9310e-01, -1.1347e-01, -8.6918e-02,\n",
      "          -4.1033e-02, -1.6452e-01,  1.0412e-01, -1.7548e-01,  2.8392e-01,\n",
      "           2.8952e-01, -7.6336e-02,  1.4336e-01, -2.7292e-01],\n",
      "         [-2.2337e-01, -3.6899e-01,  1.2185e-01,  1.4037e-01,  1.4051e-01,\n",
      "           1.2121e-01, -1.4649e-01,  2.7230e-01,  2.0667e-01, -1.8062e-01,\n",
      "           8.9460e-02, -3.8905e-01,  2.3613e-01, -7.1509e-02, -2.3903e-02,\n",
      "           1.9587e-04, -1.0819e-01,  2.3449e-01, -2.2003e-02,  4.5285e-01,\n",
      "           2.5873e-01, -1.0152e-02,  1.5114e-01, -1.9349e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "import gpt_tests\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(self.hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attention = Attention(self.hidden_size, self.num_heads)\n",
    "        self.ln2 = nn.LayerNorm(self.hidden_size, eps=layer_norm_epsilon)\n",
    "        self.MLP = nn.Sequential(nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "                                   nn.GELU(),\n",
    "                                   nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "                                   nn.Dropout(self.dropout)\n",
    "                                  )\n",
    "        \n",
    "    def forward(self, x, past_key_values = None, return_key_values = False):\n",
    "        if return_key_values:\n",
    "            layer1, new_key_values = self.attention(self.ln1(x)) \n",
    "            layer1 = layer1 + x\n",
    "            return self.ML\n",
    "        # past_key_values = self.attention(self.ln2(layer1)) + layer1\n",
    "            \n",
    "        layer1 = self.attention(self.ln1(x), past_key_values) + x\n",
    "        return self.MLP(self.ln2(layer1)) + layer1\n",
    "\n",
    "# gpt_tests.test_gpt_block(Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "from collections import Counter\n",
    "import transformers\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "    # L_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size, \n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon, L=0, use_cache=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "                                      for _ in range(num_layers)])\n",
    "        \n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        \n",
    "        self.use_cache = use_cache\n",
    "        if self.use_cache:\n",
    "            self.cache = t.zeros((num_layers, num_heads, 0, 2*self.head_size))\n",
    "        \n",
    "    def forward(self, input_ids): # [batch, seq_len]\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.get_device() \n",
    "                                 if input_ids.get_device() >= 0 else 'cpu')\n",
    "        embeddings = self.token_embeddings(input_ids) + self.position_embeddings(positions)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        if self.use_cache and self.cache.shape[1] == 0:\n",
    "                        \n",
    "            nl, nh, _, hs = self.cache.shape\n",
    "            cache_updates = t.tensor((0, nh, 1, hs))\n",
    "            \n",
    "            if self.cache.shape[2] == 0:\n",
    "                tokens_to_process = range(input_ids.shape[1])\n",
    "            \n",
    "            else:\n",
    "                tokens_to_process = [-1]\n",
    "            \n",
    "            for i in tokens_to_process:\n",
    "                \n",
    "                out = embeddings[:,i]\n",
    "                for block_num, block in enumerate(self.blocks):\n",
    "                    out, new_keys = block(out, cache[block_num], True)\n",
    "                    cache_updates = t.cat((cache_updates,new_keys),0)\n",
    "                \n",
    "                self.cache = t.cat((self.cache, cache_updates), 2)\n",
    "        else:\n",
    "            out = embeddings\n",
    "            for block_num, block in enumerate(self.blocks):\n",
    "                out = block(out)        \n",
    "        \n",
    "        embeddings = self.ln(out)\n",
    "        output = GPT2Output(einsum(\"bnc, vc -> bnv\", embeddings, self.token_embeddings.weight)[:, -1],\n",
    "                            embeddings[:, -1])\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def next_token(self, input_ids, temperature, freq_penalty=2.0):\n",
    "        output = self.forward(torch.tensor(input_ids).unsqueeze(0))\n",
    "        highly_creative_counter = Counter(input_ids)\n",
    "        \n",
    "        frequencies = torch.zeros(self.vocab_size)\n",
    "        for word, count in highly_creative_counter.items():\n",
    "            frequencies[word] += count\n",
    "            \n",
    "        return nn.Softmax(dim=0)(output.logits.squeeze(0) / temperature - frequencies * freq_penalty).argmax(dim=0)\n",
    "    \n",
    "    def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
    "        self.cache = t.zeros((self.num_layers, self.num_heads, 0, 2*self.head_size))\n",
    "        tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        input_ids = tokenizer(text)[\"input_ids\"]\n",
    "        \n",
    "        length = len(input_ids)\n",
    "        cur_token = input_ids[-1]\n",
    "        while length < max_length and cur_token != tokenizer.eos_token_id:\n",
    "            length += 1\n",
    "            cur_token = self.next_token(input_ids, temperature, freq_penalty)\n",
    "            input_ids.append(cur_token)\n",
    "        \n",
    "        return tokenizer.decode(input_ids)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, \n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, L = 11, use_cache=True)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()\n",
    "pretrained_state_dict = pretrained_gpt.state_dict()\n",
    "\n",
    "zipped_params = [param for param in zip(pretrained_state_dict.keys(), bert.state_dict().keys())]\n",
    "new_state_dict = {}\n",
    "for pretrained, ours in zipped_params:\n",
    "    new_state_dict[ours] = pretrained_state_dict[pretrained]\n",
    "\n",
    "bert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.generate(\"\"\"What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I'm the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little \"clever\" comment was about to bring down upon you, maybe you would have held your fucking tongue.\n",
    "Spongbob: \"\"\", max_length=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert.generate(\"Here's my favorite joke: \", max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, \n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, L = 11)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()\n",
    "pretrained_state_dict = pretrained_gpt.state_dict()\n",
    "#my_gpt.load_state_dict(pretrained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_params = [param for param in zip(pretrained_state_dict.keys(), my_gpt.state_dict().keys())]\n",
    "new_state_dict = {}\n",
    "for pretrained, ours in zipped_params:\n",
    "    new_state_dict[ours] = pretrained_state_dict[pretrained]\n",
    "\n",
    "my_gpt.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Output' object has no attribute 'L_encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76157eda772e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-76157eda772e>\u001b[0m in \u001b[0;36mencoding\u001b[0;34m(texts, i)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmy_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgpt_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mbert_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Output' object has no attribute 'L_encoding'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from bert_sol import Bert\n",
    "\n",
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, L = 11,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "texts = [\"My life motto:\",\n",
    "         \"My life motto: Fortune\",\n",
    "         \"My life motto: Fortune favors\",\n",
    "         \"My life motto: Fortune favors the\",\n",
    "         \"My life motto: Fortune favors the bold\"]\n",
    "\n",
    "\n",
    "def encoding(texts, i):\n",
    "    # inputs = t.zeros((len(texts), len(texts[0])))\n",
    "    tokens = tokenizer(texts, padding = \"longest\")\n",
    "    \n",
    "    inputs = t.tensor(tokens[\"input_ids\"])\n",
    "    \n",
    "    my_gpt.eval()\n",
    "    my_bert.eval()\n",
    "    \n",
    "    gpt_encodings = my_gpt(inputs).L_encoding[:,i]\n",
    "    \n",
    "    bert_encodings = my_bert(inputs).L_encoding[:,i]\n",
    "    \n",
    "    print(gpt_encodings)\n",
    "    print(bert_encodings)\n",
    "    \n",
    "    \n",
    "encoding(texts,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
