{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "import tokenizer\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cool!\n"
     ]
    }
   ],
   "source": [
    "def corpus_common_tokens(str_list: List[str]):# -> List[{\"piece\": str, \"id\": int}]:\n",
    "    words = [word for text in str_list for word in re.findall(r\"\\w+|[^\\w\\s]\", text)]\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    return [token for token, _ in word_counts.most_common(30000)]\n",
    "tokenizer.test_tokenizer_from_corpus_fn(corpus_common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra cool\n",
      "dict_keys(['c', ' ', 'b', 'a', '  ', '   ', 'a  ', 'a   '])\n",
      "dict_keys(['c', ' ', 'b', 'a', '  ', '   ', 'a   ', 'a   b'])\n",
      "hello, my name is tom trundlewich\n",
      "dict_keys([':', 'm', '1', 'V', 'D', 'l', 'v', '2', '5', 'K', 'o', 'B', 'C', 'X', 'b', 'I', 'R', ';', ')', 'Y', 'h', 'G', '(', 'A', '?', 'q', 'e', ' ', 'F', '.', ',', '!', ']', 'W', 'M', 'U', 's', 'a', 'r', '0', 'p', '-', 'i', 'S', 'E', 'O', 'N', 'g', '<', \"'\", 'x', 'J', '4', '\\n', '3', '[', 'T', 'L', 'w', 'n', 'y', 'k', 't', 'H', 'u', 'j', '>', 'c', 'z', 'P', 'd', '9', 'f', '  ', '   ', 'e ', '    ', 'th', ' t', 't ', 's ', 'ou', ', ', 'er', 'in', 'd ', '. ', 'an', 'or', 'y ', 'o ', 'll', '.\\n', 'ha', 'on', 'hi', 'you', ' th', 'en', 'I ', '        ', 'ar', 'ea', 'of', 'es', ' s', 'll ', 'no', 'er ', 'ing', 'and ', 've ', 'a ', 'wi', 'is ', ',\\n', 'the ', 'r ', 'of ', 'he ', 'om', 'it', '; ', 're', 'for', ' to ', 'st', 'ER', 'se', 'ch', 'at ', 'at', ' the ', ' m', 'ir', 'ow', 'his ', 'OL', 'him', 'e, ', 'LO', \"'s \", '                ', 'LE', 'as ', 'her', 'li', 'am', 'DI', 'gh', 'you ', 'oo', 'ee', 'ain', 'hat ', 'th ', 'st ', 'T ', 'e\\n', '    T', 'ut ', 'have ', 'it ', 'is', 'on ', 'my ', 'ra', 'ld ', 'ing ', 'e.\\n', 'CO', 'RD', '?\\n', 'LORD', 'et', 'un', 'TR', 'not ', 'your ', 'be ', '. I ', '    A', '    W', '  F', '  FI', '  FIR', '  FIRS', '  FIRST ', 'BER', 'ri', 'ear', 'AM', 'es ', 'en ', 'BERTR', 'BERTRAM', 'la', 'AN', 'now', 'AR', 'ch ', 'lor', 'that ', \"'d \", ', and ', 'to ', 'our', 'be', 'LORD. ', 'D ', ', s', 'ke ', 'an ', 'OLLE', '  \\n', 'ce ', 'PAR', 'PAROLLE', 'PAROLLES', 'ma', 'ould ', 'di', 'me ', 'nd ', 'k ', '  BERTRAM', 'ur', 'ed ', 'em', 'ro', 's\\n', 'com', 'ver', 'in ', 'ay', 'SE', 'ther', 'lo', 'but ', 'for ', '  PAROLLES', 'SOL', 'SOLDI', 'SOLDIER', 'f ', '  BERTRAM. ', 'was ', 'le ', 'rea', 'our ', 'will ', 'are ', 'ru', 'ter', 'po', '  SE', '  SECO', '  SECON', '  SECOND ', ';\\n', 'hall ', 'know', 'LA', 'LAF', 'LAFE', 'LAFEU', 'ce', 'ti', 'd, ', '. W', 'all', 'sel', '. T', 'et ', '  FIRST SOLDIER', 'de', ', sir', 'him ', 'ter ', '  LAFEU', 'IN', 'al', 'el', ' to', 'con', 'wh', 'or ', '  SECOND LORD. ', 'ap', 'her ', 'with', 'ell', 'n ', 'ent', 'sh', 'mor', \"e's \", 'igh', '    And ', '. A', 'se ', 'd\\n', \"'d\", 'EN', 'DIAN', 'DIANA', 'ut', 'mo', 'by ', 'il', '. H', 'up', 'this ', 'KIN', 'KING', 'ou ', '. S', 'ther ', 'hon', 'me', 'what ', 'whi', '  KING', 'the', 'do ', 'oun', 'si', 'ess', 'with ', 'est', 'ol', 'would ', 'ay ', 'ere ', 'fr', ' my ', 'man', 'y, ', '    H', '  PAROLLES. ', 'fa', '  DIANA', 'wor', 'no ', 'lea', ' that ', 'we ', 'En', 'your', 'lord', 'pro', 'ive ', 'ring', 'am ', 'ly ', 'WN', 'e,\\n', 'p ', 'Ex', 'from', 'tain', 'ood ', 'qu', '    Wh', '    M', 'ag', '    B', 'grea', 's, ', 'ough', 'bo', 'y\\n', 'ess ', 'CLO', 'CLOWN', 'sp', 'au', 'ck', '    I', \"'ll \", 'af', '    I ', ' ta', 'ne', 'and', ', sir, ', 'not', 'ine ', 'id', 'su', 'more ', 'ow ', 'end', 'here ', 'mu', 'of the ', \"'t\", 'fi', 'ES', 'll s', '                                ', 'dea', 'vi', 'sw', 'we', 'ong', \"' \", ' this ', 'self', 'us', 'shall ', '    S', '  CLOWN', 'um', ', I ', 'E ', 'Enter ', 'ion', 'You ', 'you s', '!\\n', '? ', '    That ', 'so ', 'per', 'est ', 'in the ', 'which ', 'ave ', 'had ', 'ed', '    O', 'bl', 'do', 'ear ', 'aptain', 'good ', 'that', 'give ', 'a s', 'par', 'dis', '  FIRST LORD. ', 'ul', 'pr', 'ring ', 'kn', 'ort', 'know ', 'sc', 'ous ', '. I', 'ELE', 'aw', 'COU', 'COUN', 'COUNT', 'ran', 'No', ', my ', 'ad', 'hath ', 'e of ', 'again', 's.\\n', '    D', ' tru', 'as', 'nigh', '; and ', 'it.\\n', 'very ', 'ent ', 'foo', 'NA', 'ce.\\n', 'gu', 'Coun', 'Captain', '! ', 'fir', 'e to ', 'wif', 'str', 'great ', '    To ', '.  \\n', 'honour', 'hea', 'I have ', '. He ', 'ry ', 'iv', 'nat', 'upon ', 'ec', 'ble ', 'a p', 'to', 'und', 'if ', 'out ', 'HELE', 'HELENA', 'she ', 'liv', 'ts ', 'ir ', 'ce, ', 'all ', 't\\n', 'go', 'own', ' take ', 'pra', 'cha', 'own ', 'were ', \"' th\", 'eed', 'ink', ' the', 'his\\n', 'im', '    The ', 'a c', 'COUNTES', 'COUNTESS', '  LAFEU. ', 'GEN', 'GENT', 'GENTLE', 'GENTLEM', 'GENTLEMAN', '    Th', '    Whi', '. B', 'ill', 'Flor', '. The ', '; but ', 'has ', 'ure', 'lad', 'plea', 'gra', 'ence ', 'from ', 'shi', 'av', 'rep', 'My ', 'ver ', 'hat', '  HELENA', 'ose ', 'let ', 'ted ', 'Go', 'Rou', 'Rousi', 'Rousill', 'co', 'ff', \"I'll \", 'WI', 'S ', 'hou', 'lord, ', 'ck ', 'ster', 'your s', '; s', 'off', 'any ', ' of ', 'ion ', ' to the ', 'ure ', 'don', 'ju', 'I am ', 'ince ', 'let', 'King', 'cour', ' st', 'you, ', 'ir, ', 'ur ', 'ish', 'Du', 'can', 'fool', '  COUNTESS', 'pea', 'ome ', 'hor', 'will', 'DO', 'love ', 'min', 'a m', '    When ', 'you are ', '    As ', 'eet ', '    My ', 'did ', 'thee ', 'comp', '    But ', 'lie', 'sha', 'see ', 'car', 'end ', 'lif', 'es, ', 'hear', 'pa', 'though', 'heav', 'mar', 'letter', 'other ', 'well ', '. And ', 'of\\n', 'them', 'mean', 's of ', 'ful', 'ous', 'ret', 'ge', 'Let ', ', the ', 'mad', 'make ', 'lordshi', 'e-', 'er.\\n', 'does ', 'rem', '  FIRST SOLDIER. W', 'with the ', '  GENTLEMAN', 'Fort', 'Fortun', '  KING. ', 'cam', 'int', 'it, ', 'augh', 'ood', ' mu', 'ill ', ' them', 'selv', 'Exe', 'Exeun', 'AC', 'ence', 'WIDO', 'WIDOW', 'he', 'ition', ', but ', 't.\\n', 'pri', 'you have ', \"'T\", 'ath', 'pray ', 'but', 've\\n', 'fore ', 'ge ', 'ity ', 'des', ' su', 'anc', \"i' th\", 'me, ', 'ho', 'than', 'ays ', 'like ', 'it\\n', 'wo ', 'S\\n', 'thing', 'y.\\n', 'g ', '    the ', 'himself', ' to be ', 'night', 'd.\\n', 'ceiv', 'ness ', 'made ', 'could ', 'mat', 'vir', '.  ', \"'\\n\", 'this', 'oll', 'y; ', 'one ', 'loo', \"e'\", 'OR', '    Which ', 'thou ', '    C', '; th', 'Count ', 'le', 'us ', '-\\n', 'ACT ', 'CEN', 'CENE ', 'house', ' and ', '  DIANA. ', 'ian', '    You ', 'ervi', 'erve ', 'when ', 'tru', 'y,\\n', '. Th', 'ell ', ':\\n', 'whom', 'ds ', 'in my ', 'ain ', 'mine ', 'id ', ' shall ', 's are ', 'most ', 'strong', 'you shall ', '    Y', 'dead', 'just', 'LORD. I ', 'red ', '; for ', 'displea', 'you.\\n', 'wom', 'fl', 'think', ' they ', 'coun', 'of his ', 'e, s', 'fin', 'say ', 'pla', 'tu', 'pt ', '                    ', 'bus', ']  ', 'sto', 'knav', 'answ', 'fri', 'thing ', '  PAROLLES. I ', ' me ', '  FIRST SOLDIER. ', 'y t', 'rat', 'so', 'ex', 'well', ', to ', 'Rousillon', 'first ', 'e;\\n', 'ce\\n', 'daugh', 'pos', 'Nay', 'arde', 'in\\n', 'kee', 'A ', 'bet', 'selves', \"'d.\\n\", '                        ', 'nam', 'Dian', 'honest', 'should ', ', my lord', 'vow', 'the\\n', 'lov', 'ever', 'oath', 'hol', 'High', 'ness', 'en, ', 'poor ', 'ft', 'beg', 'ake ', 'great', 'der', 'mother', 'son', 'ken', 'deed', 'if', \"'t \", 'The ', 'nature', 'into ', '  SECOND LORD. H', 'pp', ' to\\n', \"'tis \", 'mon', 'God ', 'ac', 'pet', ' to his ', \"'d, \", 'ment', 'pres', 'of th', 'war', 'What ', 'Fran', 'and, ', 'under', 'st\\n', 'by the ', ', and the ', 'for\\n', 'virtu', 'shall be ', 'can ', 'busin', 'wr', 'it to ', 'yet', 'fear', 'and\\n', 'du', '.  [', 'conf', 'ush', '. What ', 'poor', 'upon', 'reput', 'Par', \"that's \", 'will s', 'out', 'hal', 'they ', 'Duk', 'Captain ', 'bes', 'besee', 'e; ', 'e?\\n', 'never ', '    F', 'look', 'Not ', 'I would ', 'itt', 'ey', 'hel', 'noble ', '                                        ', 'whose ', 'daughter', \"'st \", 'ON', 'OR ', 'ring was ', 'let me ', 'cre', \"'ll s\", 'Nay, ', 'peak', 'ith', 'fu', 'art ', 'bro', 'hear ', 'Exeunt\\n', 'Florence', \"'S \", '    In ', 'ame ', 'now ', 'weet ', 'self ', 'against ', 'es\\n', 'leave ', 'orn', 's to ', 'pray you', 'ly, ', 'belie', 'whom ', 'char', '  DIANA. I ', '. G', 'ower', 'long', 'ob', \"i' th' \", 'worl', '. Wh', ' truth', 'yet ', 'hour', 'ime ', 'eu', 'he s', 'me\\n', 'his wif', 'him\\n', 'ren', 'camp', ' two ', 'king ', 'who ', 'ev', 'youn', 'young ', '    and ', 'ow, ', 'comm', 'ab', 't d', 'ful ', 'after ', ' so ', 'must ', 'her, ', 'lish', 'offi', 'ig', 'ANT', '. His ', 'lordship ', 'pat', ': ', ' the\\n', 'l ', 'oldi', 'bring ', 'forth', 'inst', 'ast', 'as I ', '   Enter ', 'man ', 'ci', 'ener', 'eneral', 'emand ', ' thous', 'cat', 'reputat', 'reputation ', 'my lord', 'Paroll', 'his s', 'ell, ', 'ay.\\n', '. Let ', 'with the Duk', '    you', 'beseech ', 'a\\n', 'fall', 'Mar', 'Count'])\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, token_list) -> None:\n",
    "        self.token_list = token_list\n",
    "        self.piece_to_id = {token['piece']: token['id'] for token in token_list}\n",
    "        self.vocab = {token['piece']: token for token in token_list}\n",
    "        self.id_to_piece = {token['id']: token['piece'] for token in token_list}\n",
    "        self.unk_token_id = 3 # [UNK]\n",
    "    \n",
    "    def decode(self, id_list: List[int]) -> str:\n",
    "        return ' '.join(self.id_to_piece[i] for i in id_list)\n",
    "    \n",
    "    def tokenize(self, string: str) -> List[int]:\n",
    "        token_split = re.findall(r\"\\w+|[^\\w\\s]\", string)\n",
    "        return [self.piece_to_id.get(token, self.unk_token_id) for token in token_split]\n",
    "        \n",
    "tokenizer.test_tokenizer(Tokenizer)\n",
    "\n",
    "f = open('bpe_tokens.json')\n",
    "token_list = json.load(f)\n",
    "\n",
    "t = Tokenizer(token_list)\n",
    "t.decode(t.tokenize(\"Hello there. POSIdfjpso $ $\"))\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, token_list):\n",
    "        super().__init__(token_list)\n",
    "        \n",
    "    def tokenize(self, string: str):\n",
    "        # token_split = re.findall(r\"\\w+|[^\\w\\s]\", string)\n",
    "        tokens = list(string)\n",
    "        print(string)\n",
    "        print(self.piece_to_id.keys())\n",
    "        \n",
    "        for piece in self.piece_to_id.keys():\n",
    "            new_tokens = []\n",
    "            while len(tokens) > 1:\n",
    "                tok1, tok2 = tokens[0], tokens[1]\n",
    "                if tok1 + tok2 == piece:\n",
    "                    # delete i, i+1 from tokens\n",
    "                    # insert tok1 + tok2\n",
    "                    tokens.pop(0)\n",
    "                    tokens.pop(0)\n",
    "                    new_tokens.append(piece)\n",
    "                else:\n",
    "                    new_tokens.append(tokens.pop(0))\n",
    "            if len(tokens) == 1:\n",
    "                new_tokens.append(tokens.pop(0))\n",
    "            tokens = new_tokens\n",
    "        return [self.piece_to_id[token] for token in tokens]\n",
    "              \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, num_tokens=1000):\n",
    "        # BPETokenizer.from_corpus(minicorpus)\n",
    "        # tokens = \n",
    "        unique_tokens = {token: i+5 for i, token in enumerate(set(list(\"\".join(corpus))))} # token: id\n",
    "        \n",
    "        corpus = [list(tokens) for tokens in corpus]\n",
    "        while len(unique_tokens) < num_tokens:\n",
    "            \n",
    "            pair_count = Counter((tokens[i], tokens[i+1]) for tokens in corpus for i in range(len(tokens)-1) )\n",
    "            \n",
    "            tok1, tok2 = pair_count.most_common(1)[0][0]\n",
    "            # pair_count[tok1, tok2] = 0\n",
    "            unique_tokens[tok1 + tok2] = len(unique_tokens) + 5\n",
    "            \n",
    "            new_corpus = []\n",
    "            for tokens in corpus:\n",
    "                new_tokens = []\n",
    "                while len(tokens) > 1:\n",
    "                    if tokens[0] == tok1 and tokens[1] == tok2:\n",
    "                        tokens.pop(0)\n",
    "                        tokens.pop(0)\n",
    "                        new_tokens.append(tok1 + tok2)\n",
    "\n",
    "                        # pair_count.subtract({(new_tokens[-1], tok1): 1, (tok2, tokens[0]): 1, (tok1 + tok2, tokens[0]): -1})\n",
    "                    else:\n",
    "                        new_tokens.append(tokens.pop(0))\n",
    "                if len(tokens) == 1:\n",
    "                    new_tokens.append(tokens.pop(0))\n",
    "                new_corpus.append( new_tokens)\n",
    "            corpus = new_corpus\n",
    "        return cls([{'piece': k, 'id': v} for k, v in unique_tokens.items()])\n",
    "\n",
    "# tokenizer.test_bpe_tokenizer_from_corpus(BPETokenizer)\n",
    "\n",
    "print(tokenizer.BPETokenizer.from_corpus([\"a   b   c\"], num_tokens = 8).vocab.keys())\n",
    "print(BPETokenizer.from_corpus([\"a   b   c\"], num_tokens = 8).vocab.keys())\n",
    "\n",
    "tokenizer.test_bpe_tokenizer(BPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
