{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert():\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996, hidden_size=384, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=1536, \n",
    "        num_heads=6, num_layers=2, num_classes=2\n",
    "    )\n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "data_train, data_valid, data_test = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid', 'test'))\n",
    "\n",
    "data_train_list = list(data_train)\n",
    "\n",
    "train_dataloader = DataLoader(data_train_list, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(data_test, batch_size=16)\n",
    "valid_dataloader = DataLoader(data_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = make_bert().cuda()\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(bert.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import datetime\n",
    "def train(model, optimizer, data, lossfn, epochs=1, max_seq_len=512):\n",
    "    model.train()\n",
    "    avg_losses = []\n",
    "    loss_buffer = []\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for X in tqdm(data):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # token processing\n",
    "            tokens = tokenizer(X, padding='longest', max_length=max_seq_len, truncation=True)\n",
    "            # the original input\n",
    "            unmasked_tokens = torch.tensor(tokens.input_ids, dtype=torch.long).cuda()\n",
    "            zero_tokens = unmasked_tokens == 0\n",
    "            rand_nums = torch.rand(unmasked_tokens.shape).cuda() <= 0.15\n",
    "\n",
    "            masked_tokens = unmasked_tokens.clone()\n",
    "            masked_tokens[rand_nums] = tokenizer.mask_token_id\n",
    "            masked_tokens[zero_tokens] = 0\n",
    "\n",
    "            output, _classifications = model(masked_tokens)\n",
    "            masked_mask = masked_tokens == tokenizer.mask_token_id\n",
    "            #expected_output_at_masks = torch.masked_select(unmasked_tokens,    masked_mask)\n",
    "            expected_output_at_masks = unmasked_tokens[masked_mask]\n",
    "            unnormed_probs_at_masks = output[masked_mask]\n",
    "            loss = lossfn(unnormed_probs_at_masks, expected_output_at_masks) \n",
    "\n",
    "            \n",
    "            #print(unmasked_tokens)\n",
    "            #print(masked_tokens)\n",
    "            #print(output)\n",
    "            #print(expected_output_at_masks)\n",
    "            #print(unnormed_probs_at_masks)\n",
    "            #break\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_buffer.append(loss)\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(loss_buffer) / len(loss_buffer)\n",
    "                avg_losses.append(avg_loss)\n",
    "                loss_buffer.clear()\n",
    "                if i % 5000 == 0:\n",
    "                    time_string = datetime.datetime.now().strftime('%y%m%d-%H-%M')\n",
    "                    torch.save(bert, f\"saved_tiny_bert/{time_string}_loss{avg_loss}.pt\")          \n",
    "                \n",
    "                print(f\"{i=} {epoch=} loss={avg_loss}\")\n",
    "                plt.plot(avg_losses)\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                             \n",
    "\n",
    "train(bert, optimizer, train_dataloader, lossfn, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('released', 0.02), ('nominated', 0.01), ('used', 0.01), ('available', 0.01), ('successful', 0.01)]\n",
      "[('released', 0.03), ('used', 0.02), ('known', 0.01), ('described', 0.01), ('produced', 0.01)]\n",
      "[('built', 0.02), ('destroyed', 0.01), ('located', 0.01), ('the', 0.01), ('completed', 0.01)]\n",
      "[('St', 0.02), ('No', 0.01), ('unknown', 0.01), ('Dr', 0.01), ('L', 0.01)]\n",
      "[('the', 0.38), ('his', 0.04), ('a', 0.04), ('her', 0.03), ('their', 0.02)]\n",
      "[('years', 0.02), ('##s', 0.01), ('##0', 0.0), ('city', 0.0), ('time', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(model, sentence):\n",
    "    model.eval()\n",
    "    token_ids = torch.Tensor(tokenizer.encode(sentence)).long().unsqueeze(0)\n",
    "    mask_idxs = set()\n",
    "    for idx, token_id in enumerate(token_ids[0]):\n",
    "        if token_id == 103: # 103 == [MASK]\n",
    "            mask_idxs.add(idx)\n",
    "    unnormalized_output, _classifications = bert(token_ids.cuda())\n",
    "    output = torch.log_softmax(unnormalized_output, dim=-1)\n",
    "    top_k = torch.topk(output, 5, dim=-1)\n",
    "    results = []\n",
    "    for seq_i, seq_top_k in enumerate(top_k.indices[0]):\n",
    "        if seq_i in mask_idxs:\n",
    "            results.append(list(zip(tokenizer.convert_ids_to_tokens(seq_top_k), [round(x.item(), 2) for x in top_k.values[0][seq_i].exp()])))\n",
    "    for l in results:\n",
    "        print(l)\n",
    "\n",
    "ascii_art_probs(\n",
    "    bert, \n",
    "    \"MLAB stands for 'Machine Learning Alignment Bootcamp.' \"\\\n",
    "    \"The best part about MLAB so far has been [MASK]. \"\\\n",
    "    \"The worst part about MLAB so far has been [MASK]. \"\\\n",
    "    \"What I would personally like to get ouf of MLAB is [MASK]. \"\\\n",
    "    \"Overall, MLAB is [MASK]. \"\\\n",
    "    \"You could make MLAB better by [MASK] [MASK]. \"\\\n",
    "    \"I would prefer that we spend [MASK] time on lecture. \"\\\n",
    "    \"My interactions with the teaching assistants has been [MASK].\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
