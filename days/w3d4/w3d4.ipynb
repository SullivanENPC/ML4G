{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\n",
    "    'gpt2', \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "ref_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:6'\n",
    "PROMPT = tokenizer(\"<|endoftext|>\", return_tensors='pt')['input_ids'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_periods(s: str) -> int:\n",
    "    k = 0\n",
    "    for c in s:\n",
    "        if c == '.': k += 1\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(generated_tokens):\n",
    "    strings = tokenizer.batch_decode(generated_tokens)\n",
    "    return t.tensor(\n",
    "        [count_periods(string) for string in strings],\n",
    "        dtype=t.float, \n",
    "        device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, eps=1e-6):\n",
    "    return (x - x.mean()) / (x.std() + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_kl(\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    optim_fn=t.optim.Adam,\n",
    "    experiment=None,\n",
    "    length: int = 20, \n",
    "    num_epochs: int = 200,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 3e-5,\n",
    "    temperature: float = 0.6,\n",
    "    kl_coef: float = 0.,\n",
    "    print_every: int = 20\n",
    "):\n",
    "    # set up model and optimizer for training\n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "    ref_model.eval()\n",
    "    ref_model.to(DEVICE)\n",
    "    optim = optim_fn(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        generated_tokens = model.generate(\n",
    "            PROMPT,\n",
    "            min_length=(length + len(PROMPT)),\n",
    "            max_length=(length + len(PROMPT)),\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=len(tokenizer),\n",
    "            top_p=1.0,\n",
    "            num_return_sequences=batch_size\n",
    "        )\n",
    "        returns = get_returns(generated_tokens) \n",
    "        normalized_returns = normalize(returns) / length\n",
    "        logits = model(generated_tokens).logits[:, :-len(PROMPT), :] # ignore terminal logits\n",
    "        logits /= temperature # temperature changes logprobs\n",
    "        logprobs = F.log_softmax(logits, dim=-1)\n",
    "        actions = generated_tokens[..., len(PROMPT):] # ignore prompt\n",
    "        # policy[b, s] = logits[b, s, actions[b, s]]\n",
    "        policy = logprobs.gather(dim=-1, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = - t.einsum('b s, b ->', policy, normalized_returns) / batch_size\n",
    "        with t.no_grad():\n",
    "            ref_logits = ref_model(generated_tokens).logits[:, :-len(PROMPT), :]\n",
    "            ref_probs = t.softmax(ref_logits, dim=-1)\n",
    "            ref_logprobs = F.log_softmax(ref_logits, dim=-1)\n",
    "        kl_divs = reduce(ref_probs * (ref_logprobs - logprobs), 'b s a -> b s', 'sum')\n",
    "        kl_loss = reduce(kl_divs, 'b s ->', 'mean')\n",
    "        loss = policy_loss + kl_coef * kl_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if experiment is not None:\n",
    "            experiment.log_metric('policy_loss', policy_loss)\n",
    "            experiment.log_metric('kl_loss', kl_loss)\n",
    "            experiment.log_metric('episode_return', returns.mean())\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch #{epoch}\")\n",
    "            print(f\"Policy loss: {policy_loss:.2f}\")\n",
    "            print(f\"KL loss: {kl_loss:.2f}\")\n",
    "            print(f\"Total loss: {loss:.2f}\")\n",
    "            print(f\"Episodic return: {returns.mean():.2f}\")\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1) \n",
    "        optim.step()\n",
    "    \n",
    "    if experiment is not None:\n",
    "        fname = f'{experiment.get_name()}.pt'\n",
    "        t.save(model, fname)\n",
    "        experiment.log_model('model', fname)\n",
    "        experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/guillecosta/gpt-rl/8b0450a7322743be8fd302fc8455d86b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"OiNBEOeeT9IFDdHDHRLeEe5hb\",\n",
    "    project_name=\"gpt-rl\",\n",
    "    workspace=\"guillecosta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Policy loss: -0.05\n",
      "KL loss: 1.30\n",
      "Total loss: -0.05\n",
      "Episodic return: 0.66\n",
      "Epoch #20\n",
      "Policy loss: -0.79\n",
      "KL loss: 2.35\n",
      "Total loss: -0.79\n",
      "Episodic return: 18.25\n",
      "Epoch #40\n",
      "Policy loss: -1.32\n",
      "KL loss: 6.03\n",
      "Total loss: -1.32\n",
      "Episodic return: 19.73\n",
      "Epoch #60\n",
      "Policy loss: -0.78\n",
      "KL loss: 8.37\n",
      "Total loss: -0.78\n",
      "Episodic return: 19.84\n",
      "Epoch #80\n",
      "Policy loss: -1.60\n",
      "KL loss: 21.81\n",
      "Total loss: -1.60\n",
      "Episodic return: 19.84\n",
      "Epoch #100\n",
      "Policy loss: -2.00\n",
      "KL loss: 17.75\n",
      "Total loss: -2.00\n",
      "Episodic return: 19.77\n",
      "Epoch #120\n",
      "Policy loss: -5.90\n",
      "KL loss: 26.76\n",
      "Total loss: -5.90\n",
      "Episodic return: 19.91\n",
      "Epoch #140\n",
      "Policy loss: -8.77\n",
      "KL loss: 28.76\n",
      "Total loss: -8.77\n",
      "Episodic return: 19.91\n",
      "Epoch #160\n",
      "Policy loss: -13.04\n",
      "KL loss: 30.12\n",
      "Total loss: -13.04\n",
      "Episodic return: 19.89\n",
      "Epoch #180\n",
      "Policy loss: -15.80\n",
      "KL loss: 26.89\n",
      "Total loss: -15.80\n",
      "Episodic return: 19.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/guillecosta/gpt-rl/8b0450a7322743be8fd302fc8455d86b\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     episode_return [200] : (0.65625, 20.046875)\n",
      "COMET INFO:     kl_loss [200]        : (1.2950834035873413, 53.30078125)\n",
      "COMET INFO:     loss [20]            : (-15.796425819396973, -0.04717282950878143)\n",
      "COMET INFO:     policy_loss [200]    : (-24.604307174682617, 0.07764101028442383)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (20.69 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model-element            : 1 (486.78 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish before aborting...\n",
      "COMET INFO: Still uploading 1 file(s), remaining 447.43 MB/486.79 MB\n",
      "COMET INFO: Still uploading 1 file(s), remaining 26.96 MB/486.79 MB, Throughput 27.99 MB/s, ETA ~1s\n"
     ]
    }
   ],
   "source": [
    "reinforce_kl(model, experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_readable(\n",
    "    model,\n",
    "    prompt=PROMPT,\n",
    "    length: int = 20,\n",
    "    temperature: float = 0.6,\n",
    "    batch_size: int = 64\n",
    "):\n",
    "    tokens = model.generate(\n",
    "        PROMPT,\n",
    "        min_length=(length + len(PROMPT)),\n",
    "        max_length=(length + len(PROMPT)),\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=len(tokenizer),\n",
    "        top_p=1.0,\n",
    "        num_return_sequences=batch_size\n",
    "    )\n",
    "    return tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................',\n",
       " '<|endoftext|>....................']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_human_readable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
