{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.001059 STD: 0.1083 VALS [-0.02093 -0.07866 0.1381 0.06736 -0.1423 0.101 -0.03359 -0.04506 -0.1214 -0.08011...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    K = rearrange(project_key(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "    Q = rearrange(project_query(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "\n",
    "    KbyQ = t.einsum(\"bsnh,btnh -> bnst\", K, Q)\n",
    "\n",
    "    d_k = token_activations.shape[2]/num_heads\n",
    "\n",
    "    out = KbyQ/t.sqrt(t.tensor([d_k]))\n",
    "    return out\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0004419 STD: 0.1213 VALS [-0.1009 -0.1021 -0.1148 0.1132 -0.0114 -0.04146 -0.068 -0.1139 -0.1022 0.01644...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    s = t.nn.Softmax(dim=2)\n",
    "\n",
    "    out = s(attention_pattern) # batch_size, head_num, key_token, query_token\n",
    "\n",
    "    out = rearrange(out, \"b n k q -> b n q k 1\")\n",
    "\n",
    "    V = rearrange(project_value(token_activations), \"b k (n h) -> b n 1 k h\", n = num_heads) # batch_size, num_heads, 1, key, head_size\n",
    "\n",
    "    out = einsum(\"bnqkh,bnqkh -> bnqh\", out, V)\n",
    "\n",
    "    out = rearrange(out, \"b n q h -> b q (n h)\")\n",
    "\n",
    "    return project_output(out)\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "\n",
    "        hidden_dim = num_heads * hidden_size\n",
    "\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_scores = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "\n",
    "        attention = bert_attention(input, self.num_heads, attention_scores, self.value, self.output)\n",
    "\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    out = linear_1(token_activations)\n",
    "    out = t.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear_1 = t.nn.Linear(input_size,intermediate_size)\n",
    "        self.linear_2 = t.nn.Linear(intermediate_size,input_size)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return bert_mlp(input,self.linear_1,self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        eps = 1e-05\n",
    "        mean = input.mean(-1).unsqueeze(-1)\n",
    "        mean.detach()\n",
    "        stdev = input.std(-1,unbiased = False).unsqueeze(-1)\n",
    "        stdev.detach()\n",
    "        out = (input - mean)/t.sqrt(stdev**2 + eps) \n",
    "        return out*self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.484e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm1 = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.mhsa(input)\n",
    "        out = self.layer_norm1(input + out)\n",
    "        residual = out\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm2(residual + out)\n",
    "        return out\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):        \n",
    "    positions = repeat(t.arange(input_ids.shape[1]), \"p -> b p\", b = input_ids.shape[0])\n",
    "    \n",
    "    if input_ids.is_cuda:\n",
    "        device = input_ids.get_device()\n",
    "        positions.to(device = device)\n",
    "    \n",
    "    out = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(positions)\n",
    "    out = layer_norm(out)\n",
    "    return dropout(out)\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 2.69e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = t.zeros_like(input_ids)\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = t.nn.Sequential(\n",
    "            BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout),\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "            t.nn.Linear(hidden_size, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.model(input_ids)\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassification(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = Bert(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        self.classification_head = t.nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.bert(input)\n",
    "        logits = self.bert\n",
    "        out = out[:,0,:]\n",
    "        out = self.dropout(out)\n",
    "        return (logits, self.classification_head(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = BertClassification(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes = 2\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding.layer_norm.weight',\n",
       " 'embedding.layer_norm.bias',\n",
       " 'transformer.0.layer_norm.weight',\n",
       " 'transformer.0.layer_norm.bias',\n",
       " 'transformer.0.residual.layer_norm.weight',\n",
       " 'transformer.0.residual.layer_norm.bias',\n",
       " 'transformer.1.layer_norm.weight',\n",
       " 'transformer.1.layer_norm.bias',\n",
       " 'transformer.1.residual.layer_norm.weight',\n",
       " 'transformer.1.residual.layer_norm.bias',\n",
       " 'transformer.2.layer_norm.weight',\n",
       " 'transformer.2.layer_norm.bias',\n",
       " 'transformer.2.residual.layer_norm.weight',\n",
       " 'transformer.2.residual.layer_norm.bias',\n",
       " 'transformer.3.layer_norm.weight',\n",
       " 'transformer.3.layer_norm.bias',\n",
       " 'transformer.3.residual.layer_norm.weight',\n",
       " 'transformer.3.residual.layer_norm.bias',\n",
       " 'transformer.4.layer_norm.weight',\n",
       " 'transformer.4.layer_norm.bias',\n",
       " 'transformer.4.residual.layer_norm.weight',\n",
       " 'transformer.4.residual.layer_norm.bias',\n",
       " 'transformer.5.layer_norm.weight',\n",
       " 'transformer.5.layer_norm.bias',\n",
       " 'transformer.5.residual.layer_norm.weight',\n",
       " 'transformer.5.residual.layer_norm.bias',\n",
       " 'transformer.6.layer_norm.weight',\n",
       " 'transformer.6.layer_norm.bias',\n",
       " 'transformer.6.residual.layer_norm.weight',\n",
       " 'transformer.6.residual.layer_norm.bias',\n",
       " 'transformer.7.layer_norm.weight',\n",
       " 'transformer.7.layer_norm.bias',\n",
       " 'transformer.7.residual.layer_norm.weight',\n",
       " 'transformer.7.residual.layer_norm.bias',\n",
       " 'transformer.8.layer_norm.weight',\n",
       " 'transformer.8.layer_norm.bias',\n",
       " 'transformer.8.residual.layer_norm.weight',\n",
       " 'transformer.8.residual.layer_norm.bias',\n",
       " 'transformer.9.layer_norm.weight',\n",
       " 'transformer.9.layer_norm.bias',\n",
       " 'transformer.9.residual.layer_norm.weight',\n",
       " 'transformer.9.residual.layer_norm.bias',\n",
       " 'transformer.10.layer_norm.weight',\n",
       " 'transformer.10.layer_norm.bias',\n",
       " 'transformer.10.residual.layer_norm.weight',\n",
       " 'transformer.10.residual.layer_norm.bias',\n",
       " 'transformer.11.layer_norm.weight',\n",
       " 'transformer.11.layer_norm.bias',\n",
       " 'transformer.11.residual.layer_norm.weight',\n",
       " 'transformer.11.residual.layer_norm.bias',\n",
       " 'lm_head.layer_norm.weight',\n",
       " 'lm_head.layer_norm.bias']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def mapkey(key):\n",
    "\n",
    "    # embedding = model.0\n",
    "    key = re.sub(r'^embedding', 'bert.model.0', key)\n",
    "\n",
    "    # transformer.i = model.(i+1)\n",
    "    key = re.sub(r'transformer\\.(\\d+)', lambda expr: \"bert.model.{}\".format(int(expr.groups()[0]) + 1), key)\n",
    "\n",
    "    # attention = mhsa\n",
    "    key = re.sub('.attention.', '.mhsa.', key)\n",
    "\n",
    "    key = re.sub('lm_head.mlp', 'model.13', key)\n",
    "    key = re.sub('lm_head.unembedding', 'model.15', key)\n",
    "    key = re.sub('lm_head.layer_norm', 'model.16', key)\n",
    "\n",
    "    key = re.sub('_embedding.weight', '_embedding.embedding', key)\n",
    "\n",
    "    key = re.sub(r'pattern\\.project_(key|query)', lambda expr: \"{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    key = re.sub('project_out', 'output', key)\n",
    "    key = re.sub('project_value', 'value', key)\n",
    "\n",
    "    key = re.sub(r'residual\\.mlp(1|2)', lambda expr: \"mlp.linear_{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    # key = re.sub(r'^((?!residual).)*\\.layer_norm', 'layer_norm1', key)\n",
    "    key = re.sub(r'residual\\.layer_norm', 'layer_norm2', key)\n",
    "    key = re.sub(r'layer_norm\\.', 'layer_norm1.', key)\n",
    "\n",
    "    key = re.sub(r'model\\.0\\.layer_norm1', 'model.0.layer_norm', key)\n",
    "\n",
    "    key = re.sub(r'model\\.15', 'model.temp', key)\n",
    "    key = re.sub(r'model\\.16', 'model.15', key)\n",
    "    key = re.sub(r'model\\.temp', 'model.16',key)\n",
    "\n",
    "    key = re.sub(r'^model', \"bert.model\", key)\n",
    "\n",
    "\n",
    "    return key\n",
    "\n",
    "mapkey('embedding gesgse')\n",
    "mapkey('transformer.1')\n",
    "mapkey('.attention.')\n",
    "mapkey('lm_head.layer_norm')\n",
    "\n",
    "# False 15 model.1.mlp.linear_1.weight model.1.residual.mlp1.weight\n",
    "# False 16 model.1.mlp.linear_1.bias model.1.residual.mlp1.bias\n",
    "\n",
    "\n",
    "def matching(xs, pattern):\n",
    "    return [s for s in xs if re.search(pattern, s)]\n",
    "\n",
    "matching(list(pretrained_bert.state_dict()), 'layer_norm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pretrained_params = list(pretrained_bert.state_dict())\n",
    "\n",
    "# for i, our_param in enumerate(my_bert.state_dict()):\n",
    "#     print(our_param == mapkey(pretrained_params[i]), i, our_param, mapkey(pretrained_params[i]))\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    if mapkey(pretrained_param) not in my_bert.state_dict():\n",
    "        print(mapkey(pretrained_param))\n",
    "\n",
    "# for i, our_param in enumerate(my_bert.state_dict()):\n",
    "#     print(our_param, pretrained_params[i])\n",
    "\n",
    "\n",
    "# print(list(pretrained_bert.state_dict()))\n",
    "# print(list(my_bert.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    new_state_dict[mapkey(pretrained_param)] = pretrained_bert.state_dict()[pretrained_param]\n",
    "\n",
    "my_bert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
