{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ = W_Q @ input\\nK = W_K @ input\\nV = W_V @ input\\nattn_pat = normalised_softmax(Q @ K^T)\\nattention = attn_patn @ V\\nO = W_O @ attention\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q = W_Q @ input\n",
    "K = W_K @ input\n",
    "V = W_V @ input\n",
    "attn_pat = normalised_softmax(Q @ K^T)\n",
    "attention = attn_patn @ V\n",
    "O = W_O @ attention\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.007844 STD: 0.1089 VALS [0.1368 0.02227 0.01142 -0.06201 -0.04411 0.09608 -0.04278 -0.04786 0.05769 -0.1033...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    dk = t.tensor(64) # num heads\n",
    "    # print(token_activations.shape) # [batch_size, (num_heads), input_length, hidden_size]\n",
    "    Q = project_query(token_activations) # W_Q: [hidden_size, num_heads * head_size], [batch_size, input_length, num_heads * head_size]\n",
    "    K = project_key(token_activations)\n",
    "    # print(project_query, project_key)\n",
    "    # print(Q.shape, K.shape)\n",
    "    Q = rearrange(Q, \"... n (h s) -> ... h n s\", h = num_heads)\n",
    "    K = rearrange(K, \"... n (h s) -> ... h n s\", h = num_heads)\n",
    "    res = t.einsum('...qc,...kc -> ...kq', Q, K)/t.sqrt(dk)\n",
    "    # print(res.shape)\n",
    "    return res\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001597 STD: 0.1131 VALS [-0.1304 -0.002212 0.04429 0.1036 0.1437 -0.000659 -0.2454 0.0493 0.04145 -0.09295...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    softmaxed = t.nn.functional.softmax(attention_pattern, dim=-2)\n",
    "    V = project_value(token_activations)\n",
    "    V = rearrange(V, \"... n (h s) -> ... h n s\", h=num_heads)\n",
    "    #print(V.shape, softmaxed.shape)\n",
    "    #print((softmaxed @ V).shape)\n",
    "    res = project_output(rearrange(t.einsum(\"...htf,...hts->...hfs\", softmaxed, V), \"... h n s -> ... n (h s)\"))\n",
    "    #print(res.shape)\n",
    "    return res\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pattern = nn.ModuleDict({\n",
    "            'project_query': nn.Linear(hidden_size, hidden_size),\n",
    "            'project_key': nn.Linear(hidden_size, hidden_size)\n",
    "        })\n",
    "        self.project_value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_out = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape, self.num_heads, self.hidden_size)\n",
    "        attn_pattern = raw_attention_pattern(x, self.num_heads, self.pattern['project_query'], self.pattern['project_key'])\n",
    "        return bert_attention(x, self.num_heads, attn_pattern, self.project_value, self.project_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.002911 STD: 0.1037 VALS [0.1037 -0.06468 -0.1908 0.1056 -0.1703 -0.1911 0.001357 -0.008986 0.09983 0.07543...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    return linear_2(t.nn.functional.gelu(linear_1(token_activations)))\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Linear(input_size, intermediate_size)\n",
    "        self.mlp2 = nn.Linear(intermediate_size, input_size)\n",
    "        self.layer_norm = LayerNorm(input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(bert_mlp(x, self.mlp1, self.mlp2) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: 4.768e-09 STD: 1.003 VALS [1.399 -0.9391 -1.153 -0.4013 -0.01391 0.9618 -0.8159 1.842 -0.8274 -0.05183...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    EPS = 1e-5\n",
    "    def __init__(self, normalized_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(t.ones(normalized_dim))\n",
    "        self.bias = nn.Parameter(t.zeros(normalized_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = x - x.mean(dim=-1, keepdim=True).detach()\n",
    "        x = x/(x.var(dim=-1, unbiased=False, keepdim=True).detach() + self.EPS).sqrt()\n",
    "        # print(self.weight.shape, self.bias.shape, x.shape)\n",
    "        # print(self.weight)\n",
    "        # print(self.bias)\n",
    "        # print(x)\n",
    "        # print(t.einsum('...i,i->...i', x, self.weight) + self.bias)\n",
    "        return t.einsum('...i,i->...i', x, self.weight) + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size : int, intermediate_size : int, num_heads : int, dropout : float):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.residual = BertMLP(hidden_size, intermediate_size)\n",
    "        self.dropout_dist = t.distributions.bernoulli.Bernoulli(1 - self.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        y = self.layer_norm(y + x)\n",
    "        z = self.residual(y)\n",
    "        if self.training == True:\n",
    "            z = z * self.dropout_dist(z.shape[-1]) / (1 - self.dropout)\n",
    "        return z\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.weight = nn.Parameter(t.randn(vocab_size, embed_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        onehot = t.nn.functional.one_hot(input, self.vocab_size).float()\n",
    "        return onehot @ self.weight\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 4.967e-09 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):\n",
    "    embeddings = token_embedding(input_ids)\n",
    "    embeddings += position_embedding(t.arange(0, input_ids.shape[-1]).to(input_ids.device))\n",
    "    embeddings += token_type_embedding(token_type_ids)\n",
    "    return dropout(layer_norm(embeddings))\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.897e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_position_embeddings: int, type_vocab_size: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "max_position_embeddings: int, type_vocab_size: int, \n",
    "dropout: float, intermediate_size: int, num_heads: int, \n",
    "num_layers: int\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.ModuleDict({\n",
    "            'mlp': nn.Linear(hidden_size, hidden_size),\n",
    "            'gelu': nn.GELU(),\n",
    "            'unembedding': nn.Linear(hidden_size, vocab_size),\n",
    "            'layer_norm': LayerNorm(hidden_size),\n",
    "        })\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids, t.zeros_like(input_ids))\n",
    "        x = self.transformer(x)\n",
    "        x = self.lm_head['mlp'](x)\n",
    "        x = self.lm_head['gelu'](x)\n",
    "        x = self.lm_head['layer_norm'](x)\n",
    "        x = self.lm_head['unembedding'](x)\n",
    "        return x\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "pretrained_state_dict = pretrained_bert.state_dict()\n",
    "del pretrained_state_dict['classification_head.weight']\n",
    "del pretrained_state_dict['classification_head.bias']\n",
    "my_bert.load_state_dict(pretrained_state_dict)\n",
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] colleges 天 largest happened smile donation [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoded = uncased_tokenizer.encode(\"Hi, my name is bert\")\n",
    "tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish loves to eat ---.\n",
      "                  17% it\n",
      "                   9% fish\n",
      "                   9% them\n",
      "                   4% meat\n",
      "                   3% food\n",
      "                   2% eggs\n",
      "                   1% honey\n",
      "                   1% insects\n",
      "                   1% too\n",
      "                   1% rice\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    mask_encoding = tokenizer.encode('[MASK]')[1]\n",
    "\n",
    "    my_bert.eval()\n",
    "\n",
    "    encoding = t.tensor(tokenizer.encode(sentence))\n",
    "\n",
    "    logits = my_bert(encoding)[encoding == mask_encoding]\n",
    "\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    probs, word_indices = probs.sort(descending=True, dim=-1)\n",
    "\n",
    "    probs = probs[:, :10]\n",
    "    word_indices = word_indices[:, :10]\n",
    "\n",
    "    words = [[tokenizer.decode(word) for word in word_options] for word_options in word_indices]\n",
    "    if len(words) > 1:\n",
    "        print(\"please don't double mask\")\n",
    "    words_with_probs = zip(words[0], probs[0])\n",
    "    \n",
    "    sentence_for_display = sentence.replace('[MASK]', '---')\n",
    "    print(sentence_for_display)\n",
    "    for word, prob in words_with_probs:\n",
    "        print(f\"%{sentence_for_display.index('---') - 2}.1d%% %s\" % (prob * 100, word))\n",
    "ascii_art_probs(\"The fish loves to eat [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "max_position_embeddings: int, type_vocab_size: int, \n",
    "dropout: float, intermediate_size: int, num_heads: int, \n",
    "num_layers: int, num_classes : int\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.ModuleDict({\n",
    "            'mlp': nn.Linear(hidden_size, hidden_size),\n",
    "            'gelu': nn.GELU(),\n",
    "            'unembedding': nn.Linear(hidden_size, vocab_size),\n",
    "            'layer_norm': LayerNorm(hidden_size),\n",
    "        })\n",
    "        self.classification_head = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids, t.zeros_like(input_ids))\n",
    "        x = self.transformer(x)\n",
    "        # x = self.lm_head['mlp'](x)\n",
    "        # x = self.lm_head['gelu'](x)\n",
    "        # x = self.lm_head['layer_norm'](x)\n",
    "        # x = self.lm_head['unembedding'](x)\n",
    "        x = self.classification_head(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "#bert_tests.test_bert(CBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84.1M/84.1M [00:02<00:00, 30.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "data_train, data_test = torchtext.datasets.IMDB(\n",
    "    root='.data',\n",
    "    split=('train', 'test')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.datasets_utils._RawTextIterableDataset at 0x7fa6051bd850>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
