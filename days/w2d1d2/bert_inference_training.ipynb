{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.002689 STD: 0.1037 VALS [-0.0166 0.0759 -0.0623 -0.0958 0.04385 0.1269 -0.02935 -0.169 0.04702 0.04498...]\n",
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.004806 STD: 0.1195 VALS [0.1319 -0.01158 -0.05224 0.2031 -0.01778 0.1567 0.01941 -0.01889 0.02216 0.07382...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n",
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n",
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n",
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n",
      "True\n",
      "True\n",
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [0.2316 0.08455 -0.5146 1.436 2.029 -1.117 2.775 -0.5305 -0.4485 -0.2485...]\n",
      "130611270 130611270\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.796 STD: 2.407 VALS [-5.172 -5.74 -5.461 -5.619 -5.017 -5.26 -5.695 -5.955 -5.663 -5.635...]\n"
     ]
    }
   ],
   "source": [
    "%run bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert():\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "        num_heads=12, num_layers=12\n",
    "    )\n",
    "    pretrained_state_dict = pretrained_bert.state_dict()\n",
    "\n",
    "    #pretrained_keys = pretrained_state_dict.keys()\n",
    "    #my_keys = my_bert.state_dict().keys()\n",
    "    #print(set(my_keys) - set(pretrained_keys))\n",
    "\n",
    "    # we could load these but actually we want to be fine-tuning this stuff in w2d2\n",
    "    # so let's not take these \n",
    "    del pretrained_state_dict[\"classification_head.weight\"]\n",
    "    del pretrained_state_dict[\"classification_head.bias\"]\n",
    "\n",
    "    my_bert.load_state_dict(pretrained_state_dict, strict=False)\n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fish', 0.16), ('meat', 0.09), ('eggs', 0.07), ('food', 0.04), ('apples', 0.03)]\n",
      "[('fish', 0.11), ('eggs', 0.07), ('meat', 0.04), ('fruit', 0.02), ('bread', 0.02)]\n",
      "[('fish', 0.08), ('meat', 0.06), ('eggs', 0.05), ('bread', 0.02), ('vegetables', 0.02)]\n",
      "[('fish', 0.05), ('meat', 0.03), ('eggs', 0.02), ('apples', 0.01), ('fruit', 0.01)]\n",
      "[('fish', 0.06), ('meat', 0.03), ('eggs', 0.03), ('salt', 0.02), ('fruit', 0.02)]\n",
      "[('fish', 0.06), ('eggs', 0.03), ('meat', 0.03), ('birds', 0.02), ('fruit', 0.02)]\n",
      "[('fish', 0.04), ('more', 0.03), ('birds', 0.02), ('eggs', 0.01), ('honey', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    bert = make_bert()\n",
    "    token_ids = torch.Tensor(tokenizer.encode(sentence)).long().unsqueeze(0)\n",
    "    mask_idxs = set()\n",
    "    for idx, token_id in enumerate(token_ids[0]):\n",
    "        if token_id == 103: # 103 == [MASK]\n",
    "            mask_idxs.add(idx)\n",
    "    unnormalized_output = bert(token_ids)\n",
    "    output = torch.log_softmax(unnormalized_output, dim=-1)\n",
    "    top_k = torch.topk(output, 5, dim=-1)\n",
    "    results = []\n",
    "    for seq_i, seq_top_k in enumerate(top_k.indices[0]):\n",
    "        if seq_i in mask_idxs:\n",
    "            results.append(list(zip(tokenizer.convert_ids_to_tokens(seq_top_k), [round(x.item(), 2) for x in top_k.values[0][seq_i].exp()])))\n",
    "    for l in results:\n",
    "        print(l)\n",
    "\n",
    "ascii_art_probs(\"The fish loves to eat [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationBERT():\n",
    "#     def __init__(self, num_classes):\n",
    "#         VOCAB_SIZE = 28996\n",
    "#         DROPOUT=0.1\n",
    "#         self.bert = Bert(\n",
    "#             vocab_size=VOCAB_SIZE, hidden_size=768, max_position_embeddings=512, \n",
    "#             type_vocab_size=2, dropout=DROPOUT, intermediate_size=3072, \n",
    "#             num_heads=12, num_layers=12\n",
    "#         )\n",
    "#         self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "#         # TODO attach this to the last block of BERT as a hidden_size x num_classes linear layer,\n",
    "#         # not on top of the vocab_size output of BERT\n",
    "#         self.classification_head = torch.nn.Linear(VOCAB_SIZE, num_classes)\n",
    "\n",
    "#         pretrained_state_dict = pretrained_bert.state_dict()\n",
    "#         del pretrained_state_dict[\"classification_head.weight\"]\n",
    "#         del pretrained_state_dict[\"classification_head.bias\"]\n",
    "#         self.bert.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         return self.classification_head(self.dropout(self.bert(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train_list = list(data_train)\n",
    "random.shuffle(data_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batches(data_train_list, batch_size, max_seq_len):\n",
    "    batches = []\n",
    "    for batch_start in range(0, len(data_train_list), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(data_train_list))\n",
    "        ratings, sentences = [list(tup) for tup in list(zip(*data_train_list[batch_start:batch_end]))]\n",
    "        tokens = tokenizer(sentences, padding='longest', max_length=max_seq_len, truncation=True)\n",
    "        ratings01 = torch.tensor([ rating == 'pos' for rating in ratings ], dtype=torch.long)\n",
    "        tokens_tensor = torch.tensor(tokens.input_ids, dtype=torch.long)\n",
    "        batches.append((tokens_tensor, ratings01))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(tensor([[ 101,  146, 4417, 1137, 1120, 1655, 2824, 1451, 3017,  102],\n          [ 101, 1409, 1103, 6621, 1104, 1109, 4537, 1622, 4499,  102]]),\n  tensor([0, 1])),\n (tensor([[  101,   146,  1660,  1142,  2523,   170,   124,  1112,  1122,   102],\n          [  101,   120,   115,  6812,   188,  5674, 25614,  1116,   115,   102]]),\n  tensor([0, 1]))]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batches(data_train_list[:4], 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = make_bert().cuda()\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(bert.parameters(), lr)\n",
    "data = make_batches(data_train_list, 128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213ce8cfc4e740edad362a7d8d3f2468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "  0%|          | 0/196 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, data, lossfn):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for X, y in tqdm(data):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.cuda()\n",
    "        y_hat = model(X, classification_mode=True)[:,0].softmax(dim=-1)\n",
    "        loss = lossfn(y_hat, y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        if i % 30 == 0:\n",
    "            print(loss)\n",
    "train(bert, optimizer, data, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}