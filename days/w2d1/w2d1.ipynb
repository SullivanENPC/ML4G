{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "attention pattern raw MATCH!!!!!!!!\n SHAPE (2, 12, 3, 3) MEAN: -0.0001405 STD: 0.1122 VALS [0.05109 -0.1041 0.1956 -0.02055 -0.06346 0.05726 0.0002961 0.004551 -0.1318 -0.07806...]\n"
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    K = rearrange(project_key(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "    Q = rearrange(project_query(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "\n",
    "    KbyQ = t.einsum(\"bsnh,btnh -> bnst\", K, Q)\n",
    "\n",
    "    d_k = token_activations.shape[2]/num_heads\n",
    "\n",
    "    out = KbyQ/t.sqrt(t.tensor([d_k]))\n",
    "    return out\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "attention MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.001522 STD: 0.118 VALS [0.167 0.06539 0.1114 0.2801 -0.006858 -0.258 0.09339 -0.165 0.3178 -0.04242...]\n"
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    s = t.nn.Softmax(dim=2)\n",
    "\n",
    "    out = s(attention_pattern) # batch_size, head_num, key_token, query_token\n",
    "\n",
    "    out = rearrange(out, \"b n k q -> b n q k 1\")\n",
    "\n",
    "    V = rearrange(project_value(token_activations), \"b k (n h) -> b n 1 k h\", n = num_heads) # batch_size, num_heads, 1, key, head_size\n",
    "\n",
    "    out = einsum(\"bnqkh,bnqkh -> bnqh\", out, V)\n",
    "\n",
    "    out = rearrange(out, \"b n q h -> b q (n h)\")\n",
    "\n",
    "    return project_output(out)\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "\n",
    "        hidden_dim = num_heads * hidden_size\n",
    "\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_scores = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "\n",
    "        attention = bert_attention(input, self.num_heads, attention_scores, self.value, self.output)\n",
    "\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert mlp MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    out = linear_1(token_activations)\n",
    "    out = t.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear_1 = t.nn.Linear(input_size,intermediate_size)\n",
    "        self.linear_2 = t.nn.Linear(intermediate_size,input_size)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return bert_mlp(input,self.linear_1,self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "layer norm MATCH!!!!!!!!\n SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        eps = 1e-05\n",
    "        mean = input.mean(-1).unsqueeze(-1)\n",
    "        mean.detach()\n",
    "        stdev = input.std(-1,unbiased = False).unsqueeze(-1)\n",
    "        stdev.detach()\n",
    "        out = (input - mean)/t.sqrt(stdev**2 + eps) \n",
    "        return out*self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: -2.484e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm1 = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.mhsa(input)\n",
    "        out = self.layer_norm1(input + out)\n",
    "        residual = out\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm2(residual + out)\n",
    "        return out\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "embedding MATCH!!!!!!!!\n SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert embedding MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):        \n",
    "    positions = repeat(t.arange(input_ids.shape[1]), \"p -> b p\", b = input_ids.shape[0])\n",
    "    \n",
    "    if input_ids.is_cuda:\n",
    "        device = input_ids.get_device()\n",
    "        positions.to(device = device)\n",
    "    \n",
    "    out = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(positions)\n",
    "    out = layer_norm(out)\n",
    "    return dropout(out)\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert embedding MATCH!!!!!!!!\n SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-0.7424 -0.3008 -0.8746 0.4806 -0.4385 0.7631 0.6443 0.0507 0.9098 -0.3144...]\n"
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = t.zeros_like(input_ids)\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert MATCH!!!!!!!!\n SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = t.nn.Sequential(\n",
    "            BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout),\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "            t.nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.model(input_ids)\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['embedding.layer_norm.weight',\n 'embedding.layer_norm.bias',\n 'transformer.0.layer_norm.weight',\n 'transformer.0.layer_norm.bias',\n 'transformer.0.residual.layer_norm.weight',\n 'transformer.0.residual.layer_norm.bias',\n 'transformer.1.layer_norm.weight',\n 'transformer.1.layer_norm.bias',\n 'transformer.1.residual.layer_norm.weight',\n 'transformer.1.residual.layer_norm.bias',\n 'transformer.2.layer_norm.weight',\n 'transformer.2.layer_norm.bias',\n 'transformer.2.residual.layer_norm.weight',\n 'transformer.2.residual.layer_norm.bias',\n 'transformer.3.layer_norm.weight',\n 'transformer.3.layer_norm.bias',\n 'transformer.3.residual.layer_norm.weight',\n 'transformer.3.residual.layer_norm.bias',\n 'transformer.4.layer_norm.weight',\n 'transformer.4.layer_norm.bias',\n 'transformer.4.residual.layer_norm.weight',\n 'transformer.4.residual.layer_norm.bias',\n 'transformer.5.layer_norm.weight',\n 'transformer.5.layer_norm.bias',\n 'transformer.5.residual.layer_norm.weight',\n 'transformer.5.residual.layer_norm.bias',\n 'transformer.6.layer_norm.weight',\n 'transformer.6.layer_norm.bias',\n 'transformer.6.residual.layer_norm.weight',\n 'transformer.6.residual.layer_norm.bias',\n 'transformer.7.layer_norm.weight',\n 'transformer.7.layer_norm.bias',\n 'transformer.7.residual.layer_norm.weight',\n 'transformer.7.residual.layer_norm.bias',\n 'transformer.8.layer_norm.weight',\n 'transformer.8.layer_norm.bias',\n 'transformer.8.residual.layer_norm.weight',\n 'transformer.8.residual.layer_norm.bias',\n 'transformer.9.layer_norm.weight',\n 'transformer.9.layer_norm.bias',\n 'transformer.9.residual.layer_norm.weight',\n 'transformer.9.residual.layer_norm.bias',\n 'transformer.10.layer_norm.weight',\n 'transformer.10.layer_norm.bias',\n 'transformer.10.residual.layer_norm.weight',\n 'transformer.10.residual.layer_norm.bias',\n 'transformer.11.layer_norm.weight',\n 'transformer.11.layer_norm.bias',\n 'transformer.11.residual.layer_norm.weight',\n 'transformer.11.residual.layer_norm.bias',\n 'lm_head.layer_norm.weight',\n 'lm_head.layer_norm.bias']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def mapkey(key):\n",
    "\n",
    "    # embedding = model.0\n",
    "    key = re.sub(r'^embedding', 'model.0', key)\n",
    "\n",
    "    # transformer.i = model.(i+1)\n",
    "    key = re.sub(r'transformer\\.(\\d+)', lambda expr: \"model.{}\".format(int(expr.groups()[0]) + 1), key)\n",
    "\n",
    "    # attention = mhsa\n",
    "    key = re.sub('.attention.', '.mhsa.', key)\n",
    "\n",
    "    key = re.sub('lm_head.mlp', 'model.13', key)\n",
    "    key = re.sub('lm_head.unembedding', 'model.15', key)\n",
    "    key = re.sub('lm_head.layer_norm', 'model.16', key)\n",
    "\n",
    "    key = re.sub('_embedding.weight', '_embedding.embedding', key)\n",
    "\n",
    "    key = re.sub(r'pattern\\.project_(key|query)', lambda expr: \"{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    key = re.sub('project_out', 'output', key)\n",
    "    key = re.sub('project_value', 'value', key)\n",
    "\n",
    "    key = re.sub(r'residual\\.mlp(1|2)', lambda expr: \"mlp.linear_{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    # key = re.sub(r'^((?!residual).)*\\.layer_norm', 'layer_norm1', key)\n",
    "    key = re.sub(r'residual\\.layer_norm', 'layer_norm2', key)\n",
    "    key = re.sub(r'layer_norm\\.', 'layer_norm1.', key)\n",
    "\n",
    "    key = re.sub(r'model\\.0\\.layer_norm1', 'model.0.layer_norm', key)\n",
    "\n",
    "    key = re.sub(r'model\\.15', 'model.temp', key)\n",
    "    key = re.sub(r'model\\.16', 'model.15', key)\n",
    "    key = re.sub(r'model\\.temp', 'model.16',key)\n",
    "\n",
    "\n",
    "    return key\n",
    "\n",
    "mapkey('embedding gesgse')\n",
    "mapkey('transformer.1')\n",
    "mapkey('.attention.')\n",
    "mapkey('lm_head.layer_norm')\n",
    "\n",
    "# False 15 model.1.mlp.linear_1.weight model.1.residual.mlp1.weight\n",
    "# False 16 model.1.mlp.linear_1.bias model.1.residual.mlp1.bias\n",
    "\n",
    "\n",
    "def matching(xs, pattern):\n",
    "    return [s for s in xs if re.search(pattern, s)]\n",
    "\n",
    "matching(list(pretrained_bert.state_dict()), 'layer_norm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True 0 model.0.token_embedding.embedding model.0.token_embedding.embedding\nTrue 1 model.0.position_embedding.embedding model.0.position_embedding.embedding\nTrue 2 model.0.token_type_embedding.embedding model.0.token_type_embedding.embedding\nTrue 3 model.0.layer_norm.weight model.0.layer_norm.weight\nTrue 4 model.0.layer_norm.bias model.0.layer_norm.bias\nFalse 5 model.1.mhsa.query.weight model.1.layer_norm1.weight\nFalse 6 model.1.mhsa.query.bias model.1.layer_norm1.bias\nFalse 7 model.1.mhsa.key.weight model.1.mhsa.query.weight\nFalse 8 model.1.mhsa.key.bias model.1.mhsa.query.bias\nFalse 9 model.1.mhsa.value.weight model.1.mhsa.key.weight\nFalse 10 model.1.mhsa.value.bias model.1.mhsa.key.bias\nFalse 11 model.1.mhsa.output.weight model.1.mhsa.value.weight\nFalse 12 model.1.mhsa.output.bias model.1.mhsa.value.bias\nFalse 13 model.1.layer_norm1.weight model.1.mhsa.output.weight\nFalse 14 model.1.layer_norm1.bias model.1.mhsa.output.bias\nTrue 15 model.1.mlp.linear_1.weight model.1.mlp.linear_1.weight\nTrue 16 model.1.mlp.linear_1.bias model.1.mlp.linear_1.bias\nTrue 17 model.1.mlp.linear_2.weight model.1.mlp.linear_2.weight\nTrue 18 model.1.mlp.linear_2.bias model.1.mlp.linear_2.bias\nTrue 19 model.1.layer_norm2.weight model.1.layer_norm2.weight\nTrue 20 model.1.layer_norm2.bias model.1.layer_norm2.bias\nFalse 21 model.2.mhsa.query.weight model.2.layer_norm1.weight\nFalse 22 model.2.mhsa.query.bias model.2.layer_norm1.bias\nFalse 23 model.2.mhsa.key.weight model.2.mhsa.query.weight\nFalse 24 model.2.mhsa.key.bias model.2.mhsa.query.bias\nFalse 25 model.2.mhsa.value.weight model.2.mhsa.key.weight\nFalse 26 model.2.mhsa.value.bias model.2.mhsa.key.bias\nFalse 27 model.2.mhsa.output.weight model.2.mhsa.value.weight\nFalse 28 model.2.mhsa.output.bias model.2.mhsa.value.bias\nFalse 29 model.2.layer_norm1.weight model.2.mhsa.output.weight\nFalse 30 model.2.layer_norm1.bias model.2.mhsa.output.bias\nTrue 31 model.2.mlp.linear_1.weight model.2.mlp.linear_1.weight\nTrue 32 model.2.mlp.linear_1.bias model.2.mlp.linear_1.bias\nTrue 33 model.2.mlp.linear_2.weight model.2.mlp.linear_2.weight\nTrue 34 model.2.mlp.linear_2.bias model.2.mlp.linear_2.bias\nTrue 35 model.2.layer_norm2.weight model.2.layer_norm2.weight\nTrue 36 model.2.layer_norm2.bias model.2.layer_norm2.bias\nFalse 37 model.3.mhsa.query.weight model.3.layer_norm1.weight\nFalse 38 model.3.mhsa.query.bias model.3.layer_norm1.bias\nFalse 39 model.3.mhsa.key.weight model.3.mhsa.query.weight\nFalse 40 model.3.mhsa.key.bias model.3.mhsa.query.bias\nFalse 41 model.3.mhsa.value.weight model.3.mhsa.key.weight\nFalse 42 model.3.mhsa.value.bias model.3.mhsa.key.bias\nFalse 43 model.3.mhsa.output.weight model.3.mhsa.value.weight\nFalse 44 model.3.mhsa.output.bias model.3.mhsa.value.bias\nFalse 45 model.3.layer_norm1.weight model.3.mhsa.output.weight\nFalse 46 model.3.layer_norm1.bias model.3.mhsa.output.bias\nTrue 47 model.3.mlp.linear_1.weight model.3.mlp.linear_1.weight\nTrue 48 model.3.mlp.linear_1.bias model.3.mlp.linear_1.bias\nTrue 49 model.3.mlp.linear_2.weight model.3.mlp.linear_2.weight\nTrue 50 model.3.mlp.linear_2.bias model.3.mlp.linear_2.bias\nTrue 51 model.3.layer_norm2.weight model.3.layer_norm2.weight\nTrue 52 model.3.layer_norm2.bias model.3.layer_norm2.bias\nFalse 53 model.4.mhsa.query.weight model.4.layer_norm1.weight\nFalse 54 model.4.mhsa.query.bias model.4.layer_norm1.bias\nFalse 55 model.4.mhsa.key.weight model.4.mhsa.query.weight\nFalse 56 model.4.mhsa.key.bias model.4.mhsa.query.bias\nFalse 57 model.4.mhsa.value.weight model.4.mhsa.key.weight\nFalse 58 model.4.mhsa.value.bias model.4.mhsa.key.bias\nFalse 59 model.4.mhsa.output.weight model.4.mhsa.value.weight\nFalse 60 model.4.mhsa.output.bias model.4.mhsa.value.bias\nFalse 61 model.4.layer_norm1.weight model.4.mhsa.output.weight\nFalse 62 model.4.layer_norm1.bias model.4.mhsa.output.bias\nTrue 63 model.4.mlp.linear_1.weight model.4.mlp.linear_1.weight\nTrue 64 model.4.mlp.linear_1.bias model.4.mlp.linear_1.bias\nTrue 65 model.4.mlp.linear_2.weight model.4.mlp.linear_2.weight\nTrue 66 model.4.mlp.linear_2.bias model.4.mlp.linear_2.bias\nTrue 67 model.4.layer_norm2.weight model.4.layer_norm2.weight\nTrue 68 model.4.layer_norm2.bias model.4.layer_norm2.bias\nFalse 69 model.5.mhsa.query.weight model.5.layer_norm1.weight\nFalse 70 model.5.mhsa.query.bias model.5.layer_norm1.bias\nFalse 71 model.5.mhsa.key.weight model.5.mhsa.query.weight\nFalse 72 model.5.mhsa.key.bias model.5.mhsa.query.bias\nFalse 73 model.5.mhsa.value.weight model.5.mhsa.key.weight\nFalse 74 model.5.mhsa.value.bias model.5.mhsa.key.bias\nFalse 75 model.5.mhsa.output.weight model.5.mhsa.value.weight\nFalse 76 model.5.mhsa.output.bias model.5.mhsa.value.bias\nFalse 77 model.5.layer_norm1.weight model.5.mhsa.output.weight\nFalse 78 model.5.layer_norm1.bias model.5.mhsa.output.bias\nTrue 79 model.5.mlp.linear_1.weight model.5.mlp.linear_1.weight\nTrue 80 model.5.mlp.linear_1.bias model.5.mlp.linear_1.bias\nTrue 81 model.5.mlp.linear_2.weight model.5.mlp.linear_2.weight\nTrue 82 model.5.mlp.linear_2.bias model.5.mlp.linear_2.bias\nTrue 83 model.5.layer_norm2.weight model.5.layer_norm2.weight\nTrue 84 model.5.layer_norm2.bias model.5.layer_norm2.bias\nFalse 85 model.6.mhsa.query.weight model.6.layer_norm1.weight\nFalse 86 model.6.mhsa.query.bias model.6.layer_norm1.bias\nFalse 87 model.6.mhsa.key.weight model.6.mhsa.query.weight\nFalse 88 model.6.mhsa.key.bias model.6.mhsa.query.bias\nFalse 89 model.6.mhsa.value.weight model.6.mhsa.key.weight\nFalse 90 model.6.mhsa.value.bias model.6.mhsa.key.bias\nFalse 91 model.6.mhsa.output.weight model.6.mhsa.value.weight\nFalse 92 model.6.mhsa.output.bias model.6.mhsa.value.bias\nFalse 93 model.6.layer_norm1.weight model.6.mhsa.output.weight\nFalse 94 model.6.layer_norm1.bias model.6.mhsa.output.bias\nTrue 95 model.6.mlp.linear_1.weight model.6.mlp.linear_1.weight\nTrue 96 model.6.mlp.linear_1.bias model.6.mlp.linear_1.bias\nTrue 97 model.6.mlp.linear_2.weight model.6.mlp.linear_2.weight\nTrue 98 model.6.mlp.linear_2.bias model.6.mlp.linear_2.bias\nTrue 99 model.6.layer_norm2.weight model.6.layer_norm2.weight\nTrue 100 model.6.layer_norm2.bias model.6.layer_norm2.bias\nFalse 101 model.7.mhsa.query.weight model.7.layer_norm1.weight\nFalse 102 model.7.mhsa.query.bias model.7.layer_norm1.bias\nFalse 103 model.7.mhsa.key.weight model.7.mhsa.query.weight\nFalse 104 model.7.mhsa.key.bias model.7.mhsa.query.bias\nFalse 105 model.7.mhsa.value.weight model.7.mhsa.key.weight\nFalse 106 model.7.mhsa.value.bias model.7.mhsa.key.bias\nFalse 107 model.7.mhsa.output.weight model.7.mhsa.value.weight\nFalse 108 model.7.mhsa.output.bias model.7.mhsa.value.bias\nFalse 109 model.7.layer_norm1.weight model.7.mhsa.output.weight\nFalse 110 model.7.layer_norm1.bias model.7.mhsa.output.bias\nTrue 111 model.7.mlp.linear_1.weight model.7.mlp.linear_1.weight\nTrue 112 model.7.mlp.linear_1.bias model.7.mlp.linear_1.bias\nTrue 113 model.7.mlp.linear_2.weight model.7.mlp.linear_2.weight\nTrue 114 model.7.mlp.linear_2.bias model.7.mlp.linear_2.bias\nTrue 115 model.7.layer_norm2.weight model.7.layer_norm2.weight\nTrue 116 model.7.layer_norm2.bias model.7.layer_norm2.bias\nFalse 117 model.8.mhsa.query.weight model.8.layer_norm1.weight\nFalse 118 model.8.mhsa.query.bias model.8.layer_norm1.bias\nFalse 119 model.8.mhsa.key.weight model.8.mhsa.query.weight\nFalse 120 model.8.mhsa.key.bias model.8.mhsa.query.bias\nFalse 121 model.8.mhsa.value.weight model.8.mhsa.key.weight\nFalse 122 model.8.mhsa.value.bias model.8.mhsa.key.bias\nFalse 123 model.8.mhsa.output.weight model.8.mhsa.value.weight\nFalse 124 model.8.mhsa.output.bias model.8.mhsa.value.bias\nFalse 125 model.8.layer_norm1.weight model.8.mhsa.output.weight\nFalse 126 model.8.layer_norm1.bias model.8.mhsa.output.bias\nTrue 127 model.8.mlp.linear_1.weight model.8.mlp.linear_1.weight\nTrue 128 model.8.mlp.linear_1.bias model.8.mlp.linear_1.bias\nTrue 129 model.8.mlp.linear_2.weight model.8.mlp.linear_2.weight\nTrue 130 model.8.mlp.linear_2.bias model.8.mlp.linear_2.bias\nTrue 131 model.8.layer_norm2.weight model.8.layer_norm2.weight\nTrue 132 model.8.layer_norm2.bias model.8.layer_norm2.bias\nFalse 133 model.9.mhsa.query.weight model.9.layer_norm1.weight\nFalse 134 model.9.mhsa.query.bias model.9.layer_norm1.bias\nFalse 135 model.9.mhsa.key.weight model.9.mhsa.query.weight\nFalse 136 model.9.mhsa.key.bias model.9.mhsa.query.bias\nFalse 137 model.9.mhsa.value.weight model.9.mhsa.key.weight\nFalse 138 model.9.mhsa.value.bias model.9.mhsa.key.bias\nFalse 139 model.9.mhsa.output.weight model.9.mhsa.value.weight\nFalse 140 model.9.mhsa.output.bias model.9.mhsa.value.bias\nFalse 141 model.9.layer_norm1.weight model.9.mhsa.output.weight\nFalse 142 model.9.layer_norm1.bias model.9.mhsa.output.bias\nTrue 143 model.9.mlp.linear_1.weight model.9.mlp.linear_1.weight\nTrue 144 model.9.mlp.linear_1.bias model.9.mlp.linear_1.bias\nTrue 145 model.9.mlp.linear_2.weight model.9.mlp.linear_2.weight\nTrue 146 model.9.mlp.linear_2.bias model.9.mlp.linear_2.bias\nTrue 147 model.9.layer_norm2.weight model.9.layer_norm2.weight\nTrue 148 model.9.layer_norm2.bias model.9.layer_norm2.bias\nFalse 149 model.10.mhsa.query.weight model.10.layer_norm1.weight\nFalse 150 model.10.mhsa.query.bias model.10.layer_norm1.bias\nFalse 151 model.10.mhsa.key.weight model.10.mhsa.query.weight\nFalse 152 model.10.mhsa.key.bias model.10.mhsa.query.bias\nFalse 153 model.10.mhsa.value.weight model.10.mhsa.key.weight\nFalse 154 model.10.mhsa.value.bias model.10.mhsa.key.bias\nFalse 155 model.10.mhsa.output.weight model.10.mhsa.value.weight\nFalse 156 model.10.mhsa.output.bias model.10.mhsa.value.bias\nFalse 157 model.10.layer_norm1.weight model.10.mhsa.output.weight\nFalse 158 model.10.layer_norm1.bias model.10.mhsa.output.bias\nTrue 159 model.10.mlp.linear_1.weight model.10.mlp.linear_1.weight\nTrue 160 model.10.mlp.linear_1.bias model.10.mlp.linear_1.bias\nTrue 161 model.10.mlp.linear_2.weight model.10.mlp.linear_2.weight\nTrue 162 model.10.mlp.linear_2.bias model.10.mlp.linear_2.bias\nTrue 163 model.10.layer_norm2.weight model.10.layer_norm2.weight\nTrue 164 model.10.layer_norm2.bias model.10.layer_norm2.bias\nFalse 165 model.11.mhsa.query.weight model.11.layer_norm1.weight\nFalse 166 model.11.mhsa.query.bias model.11.layer_norm1.bias\nFalse 167 model.11.mhsa.key.weight model.11.mhsa.query.weight\nFalse 168 model.11.mhsa.key.bias model.11.mhsa.query.bias\nFalse 169 model.11.mhsa.value.weight model.11.mhsa.key.weight\nFalse 170 model.11.mhsa.value.bias model.11.mhsa.key.bias\nFalse 171 model.11.mhsa.output.weight model.11.mhsa.value.weight\nFalse 172 model.11.mhsa.output.bias model.11.mhsa.value.bias\nFalse 173 model.11.layer_norm1.weight model.11.mhsa.output.weight\nFalse 174 model.11.layer_norm1.bias model.11.mhsa.output.bias\nTrue 175 model.11.mlp.linear_1.weight model.11.mlp.linear_1.weight\nTrue 176 model.11.mlp.linear_1.bias model.11.mlp.linear_1.bias\nTrue 177 model.11.mlp.linear_2.weight model.11.mlp.linear_2.weight\nTrue 178 model.11.mlp.linear_2.bias model.11.mlp.linear_2.bias\nTrue 179 model.11.layer_norm2.weight model.11.layer_norm2.weight\nTrue 180 model.11.layer_norm2.bias model.11.layer_norm2.bias\nFalse 181 model.12.mhsa.query.weight model.12.layer_norm1.weight\nFalse 182 model.12.mhsa.query.bias model.12.layer_norm1.bias\nFalse 183 model.12.mhsa.key.weight model.12.mhsa.query.weight\nFalse 184 model.12.mhsa.key.bias model.12.mhsa.query.bias\nFalse 185 model.12.mhsa.value.weight model.12.mhsa.key.weight\nFalse 186 model.12.mhsa.value.bias model.12.mhsa.key.bias\nFalse 187 model.12.mhsa.output.weight model.12.mhsa.value.weight\nFalse 188 model.12.mhsa.output.bias model.12.mhsa.value.bias\nFalse 189 model.12.layer_norm1.weight model.12.mhsa.output.weight\nFalse 190 model.12.layer_norm1.bias model.12.mhsa.output.bias\nTrue 191 model.12.mlp.linear_1.weight model.12.mlp.linear_1.weight\nTrue 192 model.12.mlp.linear_1.bias model.12.mlp.linear_1.bias\nTrue 193 model.12.mlp.linear_2.weight model.12.mlp.linear_2.weight\nTrue 194 model.12.mlp.linear_2.bias model.12.mlp.linear_2.bias\nTrue 195 model.12.layer_norm2.weight model.12.layer_norm2.weight\nTrue 196 model.12.layer_norm2.bias model.12.layer_norm2.bias\nTrue 197 model.13.weight model.13.weight\nTrue 198 model.13.bias model.13.bias\nFalse 199 model.15.weight model.16.weight\nFalse 200 model.15.bias model.16.bias\nFalse 201 model.16.weight model.15.weight\nFalse 202 model.16.bias model.15.bias\n"
    }
   ],
   "source": [
    "pretrained_params = list(pretrained_bert.state_dict())\n",
    "\n",
    "for i, our_param in enumerate(my_bert.state_dict()):\n",
    "    print(our_param == mapkey(pretrained_params[i]), i, our_param, mapkey(pretrained_params[i]))\n",
    "\n",
    "# for pretrained_param in pretrained_bert.state_dict():\n",
    "#     if mapkey(pretrained_param) not in my_bert.state_dict():\n",
    "#         print(mapkey(pretrained_param))\n",
    "\n",
    "# print(list(pretrained_bert.state_dict()))\n",
    "# print(list(my_bert.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    if pretrained_param != \"classification_head.weight\" and pretrained_param != \"classification_head.bias\":\n",
    "        new_state_dict[mapkey(pretrained_param)] = pretrained_bert.state_dict()[pretrained_param]\n",
    "\n",
    "my_bert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "comparing Berts MATCH!!!!!!!!\n SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}