{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.8/site-packages\")\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.9/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import rl_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import Video\n",
    "from video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v1\n",
      "INFO: Starting new video recorder writing to videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Starting ffmpeg with \"ffmpeg -nostats -loglevel error -y -f rawvideo -s:v 600x400 -pix_fmt rgb24 -framerate 50 -i - -vf scale=trunc(iw/2)*2:trunc(ih/2)*2 -vcodec libx264 -pix_fmt yuv420p -r 50 videos/test_0.mp4\"\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "DEBUG: Capturing video frame: path=videos/test_0.mp4\n",
      "total reward: 36.0\n",
      "num steps taken: 36\n",
      "DEBUG: Closing video encoder: path=videos/test_0.mp4\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "record = True\n",
    "video_name = \"test_0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "states = 0\n",
    "# Ensure that VideoRecorder is initialized outside\n",
    "video_recorder = VideoRecorder(env,\n",
    "    f\"videos/{video_name}.mp4\"\n",
    ")    \n",
    "while not done:\n",
    "    states += 1\n",
    "    if record:\n",
    "        video_recorder.capture_frame()\n",
    "    else:  \n",
    "        show_state(env)\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # Take a random action\n",
    "    # print(f\"{reward=}\")\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"total reward: {total_reward}\")\n",
    "print(f\"num steps taken: {states}\")\n",
    "if record:\n",
    "    video_recorder.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEBUG',\n",
       " 'DISABLED',\n",
       " 'ERROR',\n",
       " 'INFO',\n",
       " 'MIN_LEVEL',\n",
       " 'WARN',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'colorize',\n",
       " 'debug',\n",
       " 'error',\n",
       " 'info',\n",
       " 'setLevel',\n",
       " 'set_level',\n",
       " 'warn',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gym.logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(gym.logger.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dqn_pg.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'basic_rl_template.ipynb',\n",
       " 'videos',\n",
       " '__pycache__',\n",
       " 'breakout.mp4',\n",
       " 'rl_tests.py',\n",
       " 'video_recorder.py',\n",
       " 'earth.mp4',\n",
       " 'rl_env',\n",
       " 'solution']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/test_0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a saved video \n",
    "Video(\"videos/test_0.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_q_net MATCH!!!!!!!!\n",
      " SHAPE (16, 23) MEAN: 0.01303 STD: 0.1371 VALS [0.04156 -0.1465 -0.1255 0.186 -0.0694 -0.03668 0.01025 -0.07389 -0.2283 0.3127...]\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size: int, hidden_size: int, out_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size),\n",
    "        )\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Possibly in a batched manner:\n",
    "            Takes an observation vector (e.g. flattened image),\n",
    "            outputs a vector of rewards for each action\n",
    "        \"\"\"\n",
    "        return self.mlp(obs)\n",
    "    \n",
    "rl_tests.test_q_net(QNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_choice(env: gym.Env, eps: float, net: nn.Module, \n",
    "                obs: torch.Tensor, device: str\n",
    "            ) -> int:\n",
    "    batch_size = obs.shape[0]\n",
    "    # rewards_for_each_action shape: batch_size, num_actions\n",
    "\n",
    "    rewards_for_each_action = net(obs)\n",
    "    actions = torch.where(torch.rand(batch_size) < eps, \n",
    "                    # TODO: Make this batches\n",
    "                    torch.tensor([env.action_space.sample() for _ in range(batch_size)], dtype=torch.long), \n",
    "                    torch.argmax(rewards_for_each_action, dim=-1))\n",
    "    # print(f\"{obs=}\")\n",
    "    # print(f\"{actions=}\")\n",
    "    return actions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnetwork.mlp[0].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, env: gym.Env, eps: float = 0.05, video_name = \"test_1\") -> float:\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    states = 0\n",
    "    # Ensure that VideoRecorder is initialized outside\n",
    "    video_recorder = VideoRecorder(env,\n",
    "        f\"videos/{video_name}.mp4\"\n",
    "    )    \n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            states += 1\n",
    "            if record:\n",
    "                video_recorder.capture_frame()\n",
    "            else:  \n",
    "                show_state(env)\n",
    "            # Batch the observation\n",
    "            obs = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            state, reward, done, _ = env.step(make_choice(env, eps, \n",
    "                                                          model, obs, 'cuda:0').item())\n",
    "            # print(f\"{reward=}\")\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"total reward: {total_reward}\")\n",
    "        print(f\"num steps taken: {states}\")\n",
    "        if record:\n",
    "            video_recorder.close()\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhttps://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\\nDescription:\\n    A pole is attached by an un-actuated joint to a cart, which moves along\\n    a frictionless track. The pendulum starts upright, and the goal is to\\n    prevent it from falling over by increasing and reducing the cart's\\n    velocity.\\nSource:\\n    This environment corresponds to the version of the cart-pole problem\\n    described by Barto, Sutton, and Anderson\\nObservation:\\n    Type: Box(4)\\n    Num     Observation               Min                     Max\\n    0       Cart Position             -2.4                    2.4\\n    1       Cart Velocity             -Inf                    Inf\\n    2       Pole Angle                -0.209 rad (-12 deg)    0.209 rad (12 deg)\\n    3       Pole Angular Velocity     -Inf                    Inf\\nActions:\\n    Type: Discrete(2)\\n    Num   Action\\n    0     Push cart to the left\\n    1     Push cart to the right\\n    Note: The amount the velocity that is reduced or increased is not\\n    fixed; it depends on the angle the pole is pointing. This is because\\n    the center of gravity of the pole increases the amount of energy needed\\n    to move the cart underneath it\\nReward:\\n    Reward is 1 for every step taken, including the termination step\\nStarting State:\\n    All observations are assigned a uniform random value in [-0.05..0.05]\\nEpisode Termination:\\n    Pole Angle is more than 12 degrees.\\n    Cart Position is more than 2.4 (center of the cart reaches the edge of\\n    the display).\\n    Episode length is greater than 200.\\n    Solved Requirements:\\n    Considered solved when the average return is greater than or equal to\\n    195.0 over 100 consecutive trials.\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -2.4                    2.4\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.209 rad (-12 deg)    0.209 rad (12 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (35, 5, -30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-451128cf0c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_randbelow_with_getrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange() (%d, %d, %d)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Non-unit step argument supplied.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (35, 5, -30)"
     ]
    }
   ],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward: 10.0\n",
      "num steps taken: 10\n",
      "episode reward: 10.0\n",
      "num steps taken: 20\n",
      "episode reward: 10.0\n",
      "num steps taken: 30\n",
      "episode reward: 10.0\n",
      "num steps taken: 40\n",
      "episode reward: 10.0\n",
      "num steps taken: 50\n",
      "episode reward: 10.0\n",
      "num steps taken: 60\n",
      "episode reward: 8.0\n",
      "num steps taken: 68\n",
      "episode reward: 10.0\n",
      "num steps taken: 78\n",
      "episode reward: 10.0\n",
      "num steps taken: 88\n",
      "episode reward: 10.0\n",
      "num steps taken: 98\n",
      "episode reward: 8.0\n",
      "num steps taken: 106\n",
      "episode reward: 10.0\n",
      "num steps taken: 116\n",
      "episode reward: 10.0\n",
      "num steps taken: 126\n",
      "episode reward: 9.0\n",
      "num steps taken: 135\n",
      "q.mlp[0].weight[0,0].item()=0.2603098750114441\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2603098750114441\n",
      "episode reward: 11.0\n",
      "num steps taken: 146\n",
      "q.mlp[0].weight[0,0].item()=0.26082485914230347\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26082485914230347\n",
      "episode reward: 10.0\n",
      "num steps taken: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-5d26bf5aaa2c>:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cur_state = torch.tensor(cur_state, dtype=torch.float)\n",
      "<ipython-input-118-5d26bf5aaa2c>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(next_state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.mlp[0].weight[0,0].item()=0.26045557856559753\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26045557856559753\n",
      "episode reward: 10.0\n",
      "num steps taken: 166\n",
      "q.mlp[0].weight[0,0].item()=0.2599823772907257\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2599823772907257\n",
      "episode reward: 9.0\n",
      "num steps taken: 175\n",
      "q.mlp[0].weight[0,0].item()=0.2595096528530121\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2595096528530121\n",
      "episode reward: 10.0\n",
      "num steps taken: 185\n",
      "q.mlp[0].weight[0,0].item()=0.2590581178665161\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2590581178665161\n",
      "episode reward: 11.0\n",
      "num steps taken: 196\n",
      "q.mlp[0].weight[0,0].item()=0.25868338346481323\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25868338346481323\n",
      "episode reward: 9.0\n",
      "num steps taken: 205\n",
      "q.mlp[0].weight[0,0].item()=0.25836995244026184\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25836995244026184\n",
      "episode reward: 10.0\n",
      "num steps taken: 215\n",
      "q.mlp[0].weight[0,0].item()=0.25808462500572205\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25808462500572205\n",
      "q.mlp[0].weight[0,0].item()=0.25791260600090027\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25791260600090027\n",
      "episode reward: 10.0\n",
      "num steps taken: 225\n",
      "q.mlp[0].weight[0,0].item()=0.25790125131607056\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25790125131607056\n",
      "episode reward: 10.0\n",
      "num steps taken: 235\n",
      "q.mlp[0].weight[0,0].item()=0.25803112983703613\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25803112983703613\n",
      "episode reward: 10.0\n",
      "num steps taken: 245\n",
      "q.mlp[0].weight[0,0].item()=0.25845444202423096\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25845444202423096\n",
      "episode reward: 10.0\n",
      "num steps taken: 255\n",
      "q.mlp[0].weight[0,0].item()=0.2590506076812744\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2590506076812744\n",
      "episode reward: 10.0\n",
      "num steps taken: 265\n",
      "q.mlp[0].weight[0,0].item()=0.25968313217163086\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.25968313217163086\n",
      "episode reward: 10.0\n",
      "num steps taken: 275\n",
      "q.mlp[0].weight[0,0].item()=0.2604343593120575\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2604343593120575\n",
      "episode reward: 9.0\n",
      "num steps taken: 284\n",
      "q.mlp[0].weight[0,0].item()=0.261273056268692\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.261273056268692\n",
      "episode reward: 10.0\n",
      "num steps taken: 294\n",
      "q.mlp[0].weight[0,0].item()=0.26218000054359436\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26218000054359436\n",
      "episode reward: 11.0\n",
      "num steps taken: 305\n",
      "q.mlp[0].weight[0,0].item()=0.263141930103302\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.263141930103302\n",
      "q.mlp[0].weight[0,0].item()=0.2640601694583893\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2640601694583893\n",
      "episode reward: 10.0\n",
      "num steps taken: 315\n",
      "q.mlp[0].weight[0,0].item()=0.26503321528434753\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26503321528434753\n",
      "episode reward: 10.0\n",
      "num steps taken: 325\n",
      "q.mlp[0].weight[0,0].item()=0.2660582959651947\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2660582959651947\n",
      "episode reward: 11.0\n",
      "num steps taken: 336\n",
      "q.mlp[0].weight[0,0].item()=0.26711755990982056\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26711755990982056\n",
      "episode reward: 10.0\n",
      "num steps taken: 346\n",
      "q.mlp[0].weight[0,0].item()=0.26818662881851196\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26818662881851196\n",
      "episode reward: 8.0\n",
      "num steps taken: 354\n",
      "q.mlp[0].weight[0,0].item()=0.26919493079185486\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.26919493079185486\n",
      "episode reward: 13.0\n",
      "num steps taken: 367\n",
      "q.mlp[0].weight[0,0].item()=0.27022355794906616\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27022355794906616\n",
      "episode reward: 9.0\n",
      "num steps taken: 376\n",
      "q.mlp[0].weight[0,0].item()=0.27129170298576355\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27129170298576355\n",
      "episode reward: 10.0\n",
      "num steps taken: 386\n",
      "q.mlp[0].weight[0,0].item()=0.27229517698287964\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27229517698287964\n",
      "episode reward: 9.0\n",
      "num steps taken: 395\n",
      "q.mlp[0].weight[0,0].item()=0.2732243239879608\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2732243239879608\n",
      "episode reward: 9.0\n",
      "num steps taken: 404\n",
      "q.mlp[0].weight[0,0].item()=0.2741709351539612\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2741709351539612\n",
      "episode reward: 9.0\n",
      "num steps taken: 413\n",
      "q.mlp[0].weight[0,0].item()=0.27519503235816956\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27519503235816956\n",
      "episode reward: 8.0\n",
      "num steps taken: 421\n",
      "q.mlp[0].weight[0,0].item()=0.27625489234924316\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27625489234924316\n",
      "q.mlp[0].weight[0,0].item()=0.2773444652557373\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2773444652557373\n",
      "episode reward: 11.0\n",
      "num steps taken: 432\n",
      "q.mlp[0].weight[0,0].item()=0.27846986055374146\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27846986055374146\n",
      "episode reward: 12.0\n",
      "num steps taken: 444\n",
      "q.mlp[0].weight[0,0].item()=0.27965715527534485\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.27965715527534485\n",
      "episode reward: 10.0\n",
      "num steps taken: 454\n",
      "q.mlp[0].weight[0,0].item()=0.28086215257644653\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28086215257644653\n",
      "episode reward: 9.0\n",
      "num steps taken: 463\n",
      "q.mlp[0].weight[0,0].item()=0.2821129560470581\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2821129560470581\n",
      "episode reward: 10.0\n",
      "num steps taken: 473\n",
      "q.mlp[0].weight[0,0].item()=0.28329676389694214\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28329676389694214\n",
      "episode reward: 8.0\n",
      "num steps taken: 481\n",
      "q.mlp[0].weight[0,0].item()=0.28447988629341125\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28447988629341125\n",
      "episode reward: 9.0\n",
      "num steps taken: 490\n",
      "q.mlp[0].weight[0,0].item()=0.28571709990501404\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28571709990501404\n",
      "episode reward: 10.0\n",
      "num steps taken: 500\n",
      "q.mlp[0].weight[0,0].item()=0.28701111674308777\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28701111674308777\n",
      "episode reward: 10.0\n",
      "num steps taken: 510\n",
      "q.mlp[0].weight[0,0].item()=0.28832000494003296\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28832000494003296\n",
      "episode reward: 8.0\n",
      "num steps taken: 518\n",
      "q.mlp[0].weight[0,0].item()=0.28957489132881165\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.28957489132881165\n",
      "episode reward: 9.0\n",
      "num steps taken: 527\n",
      "q.mlp[0].weight[0,0].item()=0.2908162474632263\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2908162474632263\n",
      "episode reward: 9.0\n",
      "num steps taken: 536\n",
      "q.mlp[0].weight[0,0].item()=0.2920314073562622\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2920314073562622\n",
      "episode reward: 10.0\n",
      "num steps taken: 546\n",
      "q.mlp[0].weight[0,0].item()=0.2932755947113037\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2932755947113037\n",
      "episode reward: 10.0\n",
      "num steps taken: 556\n",
      "q.mlp[0].weight[0,0].item()=0.2945243716239929\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2945243716239929\n",
      "episode reward: 10.0\n",
      "num steps taken: 566\n",
      "q.mlp[0].weight[0,0].item()=0.29560595750808716\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.29560595750808716\n",
      "episode reward: 9.0\n",
      "num steps taken: 575\n",
      "q.mlp[0].weight[0,0].item()=0.2967369854450226\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2967369854450226\n",
      "episode reward: 9.0\n",
      "num steps taken: 584\n",
      "q.mlp[0].weight[0,0].item()=0.29789939522743225\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.29789939522743225\n",
      "episode reward: 9.0\n",
      "num steps taken: 593\n",
      "q.mlp[0].weight[0,0].item()=0.2989336848258972\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2989336848258972\n",
      "episode reward: 9.0\n",
      "num steps taken: 602\n",
      "q.mlp[0].weight[0,0].item()=0.2999702990055084\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.2999702990055084\n",
      "episode reward: 9.0\n",
      "num steps taken: 611\n",
      "q.mlp[0].weight[0,0].item()=0.301127552986145\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.301127552986145\n",
      "q.mlp[0].weight[0,0].item()=0.3020627498626709\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3020627498626709\n",
      "episode reward: 11.0\n",
      "num steps taken: 622\n",
      "q.mlp[0].weight[0,0].item()=0.30282074213027954\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.30282074213027954\n",
      "episode reward: 9.0\n",
      "num steps taken: 631\n",
      "q.mlp[0].weight[0,0].item()=0.3037475347518921\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3037475347518921\n",
      "episode reward: 8.0\n",
      "num steps taken: 639\n",
      "q.mlp[0].weight[0,0].item()=0.3045733869075775\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3045733869075775\n",
      "episode reward: 10.0\n",
      "num steps taken: 649\n",
      "q.mlp[0].weight[0,0].item()=0.3054587244987488\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3054587244987488\n",
      "episode reward: 11.0\n",
      "num steps taken: 660\n",
      "q.mlp[0].weight[0,0].item()=0.3061657249927521\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3061657249927521\n",
      "episode reward: 10.0\n",
      "num steps taken: 670\n",
      "q.mlp[0].weight[0,0].item()=0.3069673180580139\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3069673180580139\n",
      "episode reward: 8.0\n",
      "num steps taken: 678\n",
      "q.mlp[0].weight[0,0].item()=0.30780860781669617\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.30780860781669617\n",
      "episode reward: 12.0\n",
      "num steps taken: 690\n",
      "q.mlp[0].weight[0,0].item()=0.3088274896144867\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3088274896144867\n",
      "episode reward: 11.0\n",
      "num steps taken: 701\n",
      "q.mlp[0].weight[0,0].item()=0.31002628803253174\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.31002628803253174\n",
      "q.mlp[0].weight[0,0].item()=0.31122177839279175\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.31122177839279175\n",
      "episode reward: 10.0\n",
      "num steps taken: 711\n",
      "q.mlp[0].weight[0,0].item()=0.3123076856136322\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3123076856136322\n",
      "episode reward: 10.0\n",
      "num steps taken: 721\n",
      "q.mlp[0].weight[0,0].item()=0.3135603070259094\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3135603070259094\n",
      "episode reward: 9.0\n",
      "num steps taken: 730\n",
      "q.mlp[0].weight[0,0].item()=0.3147791028022766\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3147791028022766\n",
      "episode reward: 10.0\n",
      "num steps taken: 740\n",
      "q.mlp[0].weight[0,0].item()=0.31600359082221985\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.31600359082221985\n",
      "episode reward: 9.0\n",
      "num steps taken: 749\n",
      "q.mlp[0].weight[0,0].item()=0.31725624203681946\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.31725624203681946\n",
      "episode reward: 10.0\n",
      "num steps taken: 759\n",
      "q.mlp[0].weight[0,0].item()=0.31860125064849854\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.31860125064849854\n",
      "episode reward: 9.0\n",
      "num steps taken: 768\n",
      "q.mlp[0].weight[0,0].item()=0.3197539150714874\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3197539150714874\n",
      "episode reward: 8.0\n",
      "num steps taken: 776\n",
      "q.mlp[0].weight[0,0].item()=0.3207521140575409\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3207521140575409\n",
      "episode reward: 9.0\n",
      "num steps taken: 785\n",
      "q.mlp[0].weight[0,0].item()=0.32191094756126404\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.32191094756126404\n",
      "episode reward: 9.0\n",
      "num steps taken: 794\n",
      "q.mlp[0].weight[0,0].item()=0.3230878710746765\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3230878710746765\n",
      "episode reward: 10.0\n",
      "num steps taken: 804\n",
      "q.mlp[0].weight[0,0].item()=0.3241327404975891\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3241327404975891\n",
      "episode reward: 12.0\n",
      "num steps taken: 816\n",
      "q.mlp[0].weight[0,0].item()=0.3248782455921173\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3248782455921173\n",
      "episode reward: 10.0\n",
      "num steps taken: 826\n",
      "q.mlp[0].weight[0,0].item()=0.3256409168243408\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3256409168243408\n",
      "episode reward: 10.0\n",
      "num steps taken: 836\n",
      "q.mlp[0].weight[0,0].item()=0.3265173137187958\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3265173137187958\n",
      "episode reward: 9.0\n",
      "num steps taken: 845\n",
      "q.mlp[0].weight[0,0].item()=0.3274015784263611\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3274015784263611\n",
      "q.mlp[0].weight[0,0].item()=0.3279304802417755\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3279304802417755\n",
      "episode reward: 11.0\n",
      "num steps taken: 856\n",
      "q.mlp[0].weight[0,0].item()=0.3287331759929657\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3287331759929657\n",
      "episode reward: 10.0\n",
      "num steps taken: 866\n",
      "q.mlp[0].weight[0,0].item()=0.32967761158943176\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.32967761158943176\n",
      "episode reward: 11.0\n",
      "num steps taken: 877\n",
      "q.mlp[0].weight[0,0].item()=0.33061644434928894\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33061644434928894\n",
      "episode reward: 9.0\n",
      "num steps taken: 886\n",
      "q.mlp[0].weight[0,0].item()=0.33175691962242126\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33175691962242126\n",
      "episode reward: 10.0\n",
      "num steps taken: 896\n",
      "q.mlp[0].weight[0,0].item()=0.33301714062690735\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33301714062690735\n",
      "episode reward: 8.0\n",
      "num steps taken: 904\n",
      "q.mlp[0].weight[0,0].item()=0.33418819308280945\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33418819308280945\n",
      "episode reward: 9.0\n",
      "num steps taken: 913\n",
      "q.mlp[0].weight[0,0].item()=0.33558693528175354\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33558693528175354\n",
      "episode reward: 9.0\n",
      "num steps taken: 922\n",
      "q.mlp[0].weight[0,0].item()=0.33680394291877747\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33680394291877747\n",
      "episode reward: 11.0\n",
      "num steps taken: 933\n",
      "q.mlp[0].weight[0,0].item()=0.3381110727787018\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3381110727787018\n",
      "episode reward: 9.0\n",
      "num steps taken: 942\n",
      "q.mlp[0].weight[0,0].item()=0.33938828110694885\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.33938828110694885\n",
      "episode reward: 11.0\n",
      "num steps taken: 953\n",
      "q.mlp[0].weight[0,0].item()=0.34061405062675476\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.34061405062675476\n",
      "episode reward: 8.0\n",
      "num steps taken: 961\n",
      "q.mlp[0].weight[0,0].item()=0.34194523096084595\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.34194523096084595\n",
      "episode reward: 9.0\n",
      "num steps taken: 970\n",
      "q.mlp[0].weight[0,0].item()=0.3434419631958008\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3434419631958008\n",
      "q.mlp[0].weight[0,0].item()=0.3447971045970917\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3447971045970917\n",
      "episode reward: 11.0\n",
      "num steps taken: 981\n",
      "q.mlp[0].weight[0,0].item()=0.3463089168071747\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3463089168071747\n",
      "episode reward: 12.0\n",
      "num steps taken: 993\n",
      "q.mlp[0].weight[0,0].item()=0.3475843071937561\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3475843071937561\n",
      "episode reward: 9.0\n",
      "num steps taken: 1002\n",
      "q.mlp[0].weight[0,0].item()=0.3484892249107361\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3484892249107361\n",
      "episode reward: 11.0\n",
      "num steps taken: 1013\n",
      "q.mlp[0].weight[0,0].item()=0.349239706993103\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.349239706993103\n",
      "episode reward: 11.0\n",
      "num steps taken: 1024\n",
      "q.mlp[0].weight[0,0].item()=0.3500906229019165\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3500906229019165\n",
      "episode reward: 9.0\n",
      "num steps taken: 1033\n",
      "q.mlp[0].weight[0,0].item()=0.3509865999221802\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3509865999221802\n",
      "episode reward: 9.0\n",
      "num steps taken: 1042\n",
      "q.mlp[0].weight[0,0].item()=0.3519705533981323\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3519705533981323\n",
      "episode reward: 10.0\n",
      "num steps taken: 1052\n",
      "q.mlp[0].weight[0,0].item()=0.35294193029403687\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.35294193029403687\n",
      "episode reward: 8.0\n",
      "num steps taken: 1060\n",
      "q.mlp[0].weight[0,0].item()=0.3537234961986542\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3537234961986542\n",
      "episode reward: 10.0\n",
      "num steps taken: 1070\n",
      "q.mlp[0].weight[0,0].item()=0.35467827320098877\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.35467827320098877\n",
      "episode reward: 8.0\n",
      "num steps taken: 1078\n",
      "q.mlp[0].weight[0,0].item()=0.35556018352508545\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.35556018352508545\n",
      "episode reward: 9.0\n",
      "num steps taken: 1087\n",
      "q.mlp[0].weight[0,0].item()=0.3563289940357208\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3563289940357208\n",
      "episode reward: 9.0\n",
      "num steps taken: 1096\n",
      "q.mlp[0].weight[0,0].item()=0.3574405014514923\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3574405014514923\n",
      "q.mlp[0].weight[0,0].item()=0.3586328327655792\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3586328327655792\n",
      "episode reward: 12.0\n",
      "num steps taken: 1108\n",
      "q.mlp[0].weight[0,0].item()=0.3595820665359497\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3595820665359497\n",
      "episode reward: 10.0\n",
      "num steps taken: 1118\n",
      "q.mlp[0].weight[0,0].item()=0.36045563220977783\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.36045563220977783\n",
      "episode reward: 11.0\n",
      "num steps taken: 1129\n",
      "q.mlp[0].weight[0,0].item()=0.36157944798469543\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.36157944798469543\n",
      "episode reward: 10.0\n",
      "num steps taken: 1139\n",
      "q.mlp[0].weight[0,0].item()=0.36254510283470154\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.36254510283470154\n",
      "episode reward: 10.0\n",
      "num steps taken: 1149\n",
      "q.mlp[0].weight[0,0].item()=0.36360642313957214\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.36360642313957214\n",
      "episode reward: 10.0\n",
      "num steps taken: 1159\n",
      "q.mlp[0].weight[0,0].item()=0.3646310567855835\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3646310567855835\n",
      "episode reward: 9.0\n",
      "num steps taken: 1168\n",
      "q.mlp[0].weight[0,0].item()=0.3658035099506378\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3658035099506378\n",
      "episode reward: 10.0\n",
      "num steps taken: 1178\n",
      "q.mlp[0].weight[0,0].item()=0.3664337694644928\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3664337694644928\n",
      "q.mlp[0].weight[0,0].item()=0.3670426607131958\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3670426607131958\n",
      "episode reward: 12.0\n",
      "num steps taken: 1190\n",
      "q.mlp[0].weight[0,0].item()=0.3678179383277893\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3678179383277893\n",
      "episode reward: 10.0\n",
      "num steps taken: 1200\n",
      "q.mlp[0].weight[0,0].item()=0.3689088523387909\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3689088523387909\n",
      "episode reward: 9.0\n",
      "num steps taken: 1209\n",
      "q.mlp[0].weight[0,0].item()=0.3702465295791626\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3702465295791626\n",
      "episode reward: 9.0\n",
      "num steps taken: 1218\n",
      "q.mlp[0].weight[0,0].item()=0.37128469347953796\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37128469347953796\n",
      "episode reward: 9.0\n",
      "num steps taken: 1227\n",
      "q.mlp[0].weight[0,0].item()=0.37223508954048157\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37223508954048157\n",
      "episode reward: 9.0\n",
      "num steps taken: 1236\n",
      "q.mlp[0].weight[0,0].item()=0.37324440479278564\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37324440479278564\n",
      "episode reward: 8.0\n",
      "num steps taken: 1244\n",
      "q.mlp[0].weight[0,0].item()=0.37432584166526794\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37432584166526794\n",
      "episode reward: 10.0\n",
      "num steps taken: 1254\n",
      "q.mlp[0].weight[0,0].item()=0.37551406025886536\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37551406025886536\n",
      "episode reward: 8.0\n",
      "num steps taken: 1262\n",
      "q.mlp[0].weight[0,0].item()=0.37673014402389526\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37673014402389526\n",
      "episode reward: 10.0\n",
      "num steps taken: 1272\n",
      "q.mlp[0].weight[0,0].item()=0.37768152356147766\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.37768152356147766\n",
      "episode reward: 9.0\n",
      "num steps taken: 1281\n",
      "q.mlp[0].weight[0,0].item()=0.3785063922405243\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3785063922405243\n",
      "episode reward: 10.0\n",
      "num steps taken: 1291\n",
      "q.mlp[0].weight[0,0].item()=0.3792264759540558\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3792264759540558\n",
      "episode reward: 9.0\n",
      "num steps taken: 1300\n",
      "q.mlp[0].weight[0,0].item()=0.38005149364471436\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38005149364471436\n",
      "episode reward: 9.0\n",
      "num steps taken: 1309\n",
      "q.mlp[0].weight[0,0].item()=0.3809105455875397\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3809105455875397\n",
      "episode reward: 8.0\n",
      "num steps taken: 1317\n",
      "q.mlp[0].weight[0,0].item()=0.38153597712516785\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38153597712516785\n",
      "episode reward: 9.0\n",
      "num steps taken: 1326\n",
      "q.mlp[0].weight[0,0].item()=0.3821296691894531\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3821296691894531\n",
      "episode reward: 10.0\n",
      "num steps taken: 1336\n",
      "q.mlp[0].weight[0,0].item()=0.3828747570514679\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3828747570514679\n",
      "episode reward: 13.0\n",
      "num steps taken: 1349\n",
      "q.mlp[0].weight[0,0].item()=0.3835570216178894\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3835570216178894\n",
      "episode reward: 8.0\n",
      "num steps taken: 1357\n",
      "q.mlp[0].weight[0,0].item()=0.38410118222236633\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38410118222236633\n",
      "q.mlp[0].weight[0,0].item()=0.3845813274383545\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3845813274383545\n",
      "episode reward: 11.0\n",
      "num steps taken: 1368\n",
      "q.mlp[0].weight[0,0].item()=0.3852027356624603\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3852027356624603\n",
      "episode reward: 10.0\n",
      "num steps taken: 1378\n",
      "q.mlp[0].weight[0,0].item()=0.38578513264656067\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38578513264656067\n",
      "episode reward: 10.0\n",
      "num steps taken: 1388\n",
      "q.mlp[0].weight[0,0].item()=0.38653942942619324\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38653942942619324\n",
      "episode reward: 9.0\n",
      "num steps taken: 1397\n",
      "q.mlp[0].weight[0,0].item()=0.3870253562927246\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3870253562927246\n",
      "episode reward: 10.0\n",
      "num steps taken: 1407\n",
      "q.mlp[0].weight[0,0].item()=0.3871014416217804\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3871014416217804\n",
      "episode reward: 10.0\n",
      "num steps taken: 1417\n",
      "q.mlp[0].weight[0,0].item()=0.38688722252845764\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38688722252845764\n",
      "episode reward: 10.0\n",
      "num steps taken: 1427\n",
      "q.mlp[0].weight[0,0].item()=0.38663697242736816\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38663697242736816\n",
      "episode reward: 9.0\n",
      "num steps taken: 1436\n",
      "q.mlp[0].weight[0,0].item()=0.3864392638206482\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3864392638206482\n",
      "episode reward: 10.0\n",
      "num steps taken: 1446\n",
      "q.mlp[0].weight[0,0].item()=0.38620778918266296\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38620778918266296\n",
      "episode reward: 9.0\n",
      "num steps taken: 1455\n",
      "q.mlp[0].weight[0,0].item()=0.38600385189056396\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38600385189056396\n",
      "episode reward: 10.0\n",
      "num steps taken: 1465\n",
      "q.mlp[0].weight[0,0].item()=0.3857000172138214\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3857000172138214\n",
      "episode reward: 9.0\n",
      "num steps taken: 1474\n",
      "q.mlp[0].weight[0,0].item()=0.3857053220272064\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3857053220272064\n",
      "episode reward: 10.0\n",
      "num steps taken: 1484\n",
      "q.mlp[0].weight[0,0].item()=0.3856296241283417\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3856296241283417\n",
      "episode reward: 9.0\n",
      "num steps taken: 1493\n",
      "q.mlp[0].weight[0,0].item()=0.3856186866760254\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3856186866760254\n",
      "q.mlp[0].weight[0,0].item()=0.38559725880622864\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38559725880622864\n",
      "episode reward: 10.0\n",
      "num steps taken: 1503\n",
      "q.mlp[0].weight[0,0].item()=0.38572958111763\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38572958111763\n",
      "episode reward: 10.0\n",
      "num steps taken: 1513\n",
      "q.mlp[0].weight[0,0].item()=0.3859139084815979\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3859139084815979\n",
      "episode reward: 9.0\n",
      "num steps taken: 1522\n",
      "q.mlp[0].weight[0,0].item()=0.38614240288734436\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38614240288734436\n",
      "episode reward: 9.0\n",
      "num steps taken: 1531\n",
      "q.mlp[0].weight[0,0].item()=0.3863614797592163\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3863614797592163\n",
      "episode reward: 9.0\n",
      "num steps taken: 1540\n",
      "q.mlp[0].weight[0,0].item()=0.3863919675350189\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3863919675350189\n",
      "episode reward: 10.0\n",
      "num steps taken: 1550\n",
      "q.mlp[0].weight[0,0].item()=0.38625773787498474\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38625773787498474\n",
      "episode reward: 10.0\n",
      "num steps taken: 1560\n",
      "q.mlp[0].weight[0,0].item()=0.3861633539199829\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3861633539199829\n",
      "episode reward: 10.0\n",
      "num steps taken: 1570\n",
      "q.mlp[0].weight[0,0].item()=0.38611167669296265\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38611167669296265\n",
      "episode reward: 13.0\n",
      "num steps taken: 1583\n",
      "q.mlp[0].weight[0,0].item()=0.38593757152557373\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38593757152557373\n",
      "episode reward: 9.0\n",
      "num steps taken: 1592\n",
      "q.mlp[0].weight[0,0].item()=0.3858679533004761\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3858679533004761\n",
      "q.mlp[0].weight[0,0].item()=0.38577014207839966\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38577014207839966\n",
      "episode reward: 10.0\n",
      "num steps taken: 1602\n",
      "q.mlp[0].weight[0,0].item()=0.38583624362945557\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38583624362945557\n",
      "episode reward: 9.0\n",
      "num steps taken: 1611\n",
      "q.mlp[0].weight[0,0].item()=0.3858952820301056\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3858952820301056\n",
      "episode reward: 9.0\n",
      "num steps taken: 1620\n",
      "episode reward: 8.0\n",
      "num steps taken: 1628\n",
      "q.mlp[0].weight[0,0].item()=0.3861403465270996\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3861403465270996\n",
      "episode reward: 9.0\n",
      "num steps taken: 1637\n",
      "q.mlp[0].weight[0,0].item()=0.38646095991134644\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38646095991134644\n",
      "q.mlp[0].weight[0,0].item()=0.3870680630207062\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3870680630207062\n",
      "episode reward: 10.0\n",
      "num steps taken: 1647\n",
      "q.mlp[0].weight[0,0].item()=0.3876488506793976\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3876488506793976\n",
      "episode reward: 10.0\n",
      "num steps taken: 1657\n",
      "q.mlp[0].weight[0,0].item()=0.3879050016403198\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3879050016403198\n",
      "episode reward: 11.0\n",
      "num steps taken: 1668\n",
      "q.mlp[0].weight[0,0].item()=0.38804560899734497\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38804560899734497\n",
      "episode reward: 9.0\n",
      "num steps taken: 1677\n",
      "q.mlp[0].weight[0,0].item()=0.3878786265850067\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3878786265850067\n",
      "episode reward: 10.0\n",
      "num steps taken: 1687\n",
      "q.mlp[0].weight[0,0].item()=0.38752251863479614\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38752251863479614\n",
      "episode reward: 10.0\n",
      "num steps taken: 1697\n",
      "q.mlp[0].weight[0,0].item()=0.3872740864753723\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3872740864753723\n",
      "episode reward: 9.0\n",
      "num steps taken: 1706\n",
      "q.mlp[0].weight[0,0].item()=0.3870016932487488\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3870016932487488\n",
      "episode reward: 10.0\n",
      "num steps taken: 1716\n",
      "q.mlp[0].weight[0,0].item()=0.3868376314640045\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3868376314640045\n",
      "episode reward: 9.0\n",
      "num steps taken: 1725\n",
      "q.mlp[0].weight[0,0].item()=0.387154757976532\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.387154757976532\n",
      "episode reward: 8.0\n",
      "num steps taken: 1733\n",
      "q.mlp[0].weight[0,0].item()=0.38753437995910645\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38753437995910645\n",
      "episode reward: 8.0\n",
      "num steps taken: 1741\n",
      "q.mlp[0].weight[0,0].item()=0.38780778646469116\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.38780778646469116\n",
      "episode reward: 9.0\n",
      "num steps taken: 1750\n",
      "q.mlp[0].weight[0,0].item()=0.3881263732910156\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3881263732910156\n",
      "episode reward: 9.0\n",
      "num steps taken: 1759\n",
      "q.mlp[0].weight[0,0].item()=0.3887336850166321\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3887336850166321\n",
      "episode reward: 10.0\n",
      "num steps taken: 1769\n",
      "q.mlp[0].weight[0,0].item()=0.3892948627471924\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3892948627471924\n",
      "episode reward: 10.0\n",
      "num steps taken: 1779\n",
      "q.mlp[0].weight[0,0].item()=0.3899843692779541\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3899843692779541\n",
      "episode reward: 10.0\n",
      "num steps taken: 1789\n",
      "q.mlp[0].weight[0,0].item()=0.3905194401741028\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3905194401741028\n",
      "episode reward: 9.0\n",
      "num steps taken: 1798\n",
      "q.mlp[0].weight[0,0].item()=0.3913065791130066\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3913065791130066\n",
      "episode reward: 9.0\n",
      "num steps taken: 1807\n",
      "q.mlp[0].weight[0,0].item()=0.39192673563957214\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.39192673563957214\n",
      "episode reward: 8.0\n",
      "num steps taken: 1815\n",
      "q.mlp[0].weight[0,0].item()=0.3925638198852539\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3925638198852539\n",
      "episode reward: 9.0\n",
      "num steps taken: 1824\n",
      "q.mlp[0].weight[0,0].item()=0.3933136463165283\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3933136463165283\n",
      "episode reward: 8.0\n",
      "num steps taken: 1832\n",
      "q.mlp[0].weight[0,0].item()=0.3940066695213318\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3940066695213318\n",
      "episode reward: 11.0\n",
      "num steps taken: 1843\n",
      "q.mlp[0].weight[0,0].item()=0.3946801722049713\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3946801722049713\n",
      "episode reward: 8.0\n",
      "num steps taken: 1851\n",
      "q.mlp[0].weight[0,0].item()=0.3953087627887726\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3953087627887726\n",
      "q.mlp[0].weight[0,0].item()=0.3958534896373749\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3958534896373749\n",
      "episode reward: 18.0\n",
      "num steps taken: 1869\n",
      "q.mlp[0].weight[0,0].item()=0.39633727073669434\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.39633727073669434\n",
      "episode reward: 9.0\n",
      "num steps taken: 1878\n",
      "q.mlp[0].weight[0,0].item()=0.39703333377838135\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.39703333377838135\n",
      "episode reward: 9.0\n",
      "num steps taken: 1887\n",
      "q.mlp[0].weight[0,0].item()=0.3977619409561157\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3977619409561157\n",
      "episode reward: 11.0\n",
      "num steps taken: 1898\n",
      "q.mlp[0].weight[0,0].item()=0.3984392583370209\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3984392583370209\n",
      "q.mlp[0].weight[0,0].item()=0.3991551995277405\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3991551995277405\n",
      "episode reward: 12.0\n",
      "num steps taken: 1910\n",
      "q.mlp[0].weight[0,0].item()=0.39961671829223633\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.39961671829223633\n",
      "episode reward: 10.0\n",
      "num steps taken: 1920\n",
      "q.mlp[0].weight[0,0].item()=0.3999991714954376\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.3999991714954376\n",
      "episode reward: 10.0\n",
      "num steps taken: 1930\n",
      "q.mlp[0].weight[0,0].item()=0.4001081585884094\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4001081585884094\n",
      "episode reward: 10.0\n",
      "num steps taken: 1940\n",
      "q.mlp[0].weight[0,0].item()=0.40021178126335144\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40021178126335144\n",
      "episode reward: 10.0\n",
      "num steps taken: 1950\n",
      "q.mlp[0].weight[0,0].item()=0.40054452419281006\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40054452419281006\n",
      "episode reward: 9.0\n",
      "num steps taken: 1959\n",
      "q.mlp[0].weight[0,0].item()=0.40111464262008667\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40111464262008667\n",
      "episode reward: 10.0\n",
      "num steps taken: 1969\n",
      "q.mlp[0].weight[0,0].item()=0.401790976524353\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.401790976524353\n",
      "episode reward: 8.0\n",
      "num steps taken: 1977\n",
      "q.mlp[0].weight[0,0].item()=0.4026099145412445\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4026099145412445\n",
      "episode reward: 9.0\n",
      "num steps taken: 1986\n",
      "q.mlp[0].weight[0,0].item()=0.4035782516002655\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4035782516002655\n",
      "episode reward: 9.0\n",
      "num steps taken: 1995\n",
      "q.mlp[0].weight[0,0].item()=0.40477487444877625\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40477487444877625\n",
      "episode reward: 9.0\n",
      "num steps taken: 2004\n",
      "q.mlp[0].weight[0,0].item()=0.4057416617870331\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4057416617870331\n",
      "episode reward: 10.0\n",
      "num steps taken: 2014\n",
      "q.mlp[0].weight[0,0].item()=0.4067915081977844\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4067915081977844\n",
      "episode reward: 9.0\n",
      "num steps taken: 2023\n",
      "q.mlp[0].weight[0,0].item()=0.4077492952346802\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4077492952346802\n",
      "episode reward: 9.0\n",
      "num steps taken: 2032\n",
      "q.mlp[0].weight[0,0].item()=0.4085359275341034\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4085359275341034\n",
      "episode reward: 9.0\n",
      "num steps taken: 2041\n",
      "q.mlp[0].weight[0,0].item()=0.4092497229576111\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4092497229576111\n",
      "q.mlp[0].weight[0,0].item()=0.40951302647590637\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40951302647590637\n",
      "episode reward: 11.0\n",
      "num steps taken: 2052\n",
      "q.mlp[0].weight[0,0].item()=0.40980663895606995\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.40980663895606995\n",
      "episode reward: 9.0\n",
      "num steps taken: 2061\n",
      "q.mlp[0].weight[0,0].item()=0.41007351875305176\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41007351875305176\n",
      "episode reward: 12.0\n",
      "num steps taken: 2073\n",
      "q.mlp[0].weight[0,0].item()=0.4102904498577118\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4102904498577118\n",
      "episode reward: 8.0\n",
      "num steps taken: 2081\n",
      "q.mlp[0].weight[0,0].item()=0.41056114435195923\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41056114435195923\n",
      "episode reward: 9.0\n",
      "num steps taken: 2090\n",
      "q.mlp[0].weight[0,0].item()=0.41086381673812866\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41086381673812866\n",
      "episode reward: 10.0\n",
      "num steps taken: 2100\n",
      "q.mlp[0].weight[0,0].item()=0.41120457649230957\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41120457649230957\n",
      "episode reward: 10.0\n",
      "num steps taken: 2110\n",
      "q.mlp[0].weight[0,0].item()=0.4116167426109314\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4116167426109314\n",
      "episode reward: 10.0\n",
      "num steps taken: 2120\n",
      "q.mlp[0].weight[0,0].item()=0.4120211601257324\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4120211601257324\n",
      "episode reward: 9.0\n",
      "num steps taken: 2129\n",
      "q.mlp[0].weight[0,0].item()=0.4126710295677185\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4126710295677185\n",
      "episode reward: 8.0\n",
      "num steps taken: 2137\n",
      "q.mlp[0].weight[0,0].item()=0.41331958770751953\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41331958770751953\n",
      "episode reward: 9.0\n",
      "num steps taken: 2146\n",
      "q.mlp[0].weight[0,0].item()=0.41392943263053894\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41392943263053894\n",
      "episode reward: 12.0\n",
      "num steps taken: 2158\n",
      "q.mlp[0].weight[0,0].item()=0.41453200578689575\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41453200578689575\n",
      "episode reward: 10.0\n",
      "num steps taken: 2168\n",
      "q.mlp[0].weight[0,0].item()=0.415103018283844\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.415103018283844\n",
      "q.mlp[0].weight[0,0].item()=0.41556212306022644\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41556212306022644\n",
      "episode reward: 11.0\n",
      "num steps taken: 2179\n",
      "q.mlp[0].weight[0,0].item()=0.4161131680011749\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4161131680011749\n",
      "episode reward: 9.0\n",
      "num steps taken: 2188\n",
      "q.mlp[0].weight[0,0].item()=0.41651013493537903\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41651013493537903\n",
      "episode reward: 10.0\n",
      "num steps taken: 2198\n",
      "q.mlp[0].weight[0,0].item()=0.4171448349952698\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4171448349952698\n",
      "episode reward: 9.0\n",
      "num steps taken: 2207\n",
      "q.mlp[0].weight[0,0].item()=0.41774705052375793\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41774705052375793\n",
      "episode reward: 10.0\n",
      "num steps taken: 2217\n",
      "q.mlp[0].weight[0,0].item()=0.41833674907684326\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41833674907684326\n",
      "episode reward: 9.0\n",
      "num steps taken: 2226\n",
      "q.mlp[0].weight[0,0].item()=0.41865628957748413\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41865628957748413\n",
      "episode reward: 11.0\n",
      "num steps taken: 2237\n",
      "q.mlp[0].weight[0,0].item()=0.4189247190952301\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4189247190952301\n",
      "episode reward: 11.0\n",
      "num steps taken: 2248\n",
      "q.mlp[0].weight[0,0].item()=0.41929861903190613\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41929861903190613\n",
      "q.mlp[0].weight[0,0].item()=0.4196377992630005\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4196377992630005\n",
      "episode reward: 12.0\n",
      "num steps taken: 2260\n",
      "q.mlp[0].weight[0,0].item()=0.4200231730937958\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4200231730937958\n",
      "episode reward: 8.0\n",
      "num steps taken: 2268\n",
      "q.mlp[0].weight[0,0].item()=0.4204823970794678\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4204823970794678\n",
      "episode reward: 9.0\n",
      "num steps taken: 2277\n",
      "q.mlp[0].weight[0,0].item()=0.42094945907592773\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42094945907592773\n",
      "episode reward: 9.0\n",
      "num steps taken: 2286\n",
      "q.mlp[0].weight[0,0].item()=0.42121410369873047\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42121410369873047\n",
      "episode reward: 10.0\n",
      "num steps taken: 2296\n",
      "q.mlp[0].weight[0,0].item()=0.4213118851184845\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4213118851184845\n",
      "episode reward: 9.0\n",
      "num steps taken: 2305\n",
      "q.mlp[0].weight[0,0].item()=0.4215027689933777\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4215027689933777\n",
      "episode reward: 11.0\n",
      "num steps taken: 2316\n",
      "q.mlp[0].weight[0,0].item()=0.42176172137260437\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42176172137260437\n",
      "episode reward: 10.0\n",
      "num steps taken: 2326\n",
      "q.mlp[0].weight[0,0].item()=0.42209526896476746\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42209526896476746\n",
      "episode reward: 11.0\n",
      "num steps taken: 2337\n",
      "q.mlp[0].weight[0,0].item()=0.42237740755081177\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42237740755081177\n",
      "episode reward: 9.0\n",
      "num steps taken: 2346\n",
      "q.mlp[0].weight[0,0].item()=0.42277541756629944\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42277541756629944\n",
      "episode reward: 9.0\n",
      "num steps taken: 2355\n",
      "q.mlp[0].weight[0,0].item()=0.4233417809009552\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4233417809009552\n",
      "episode reward: 11.0\n",
      "num steps taken: 2366\n",
      "q.mlp[0].weight[0,0].item()=0.4236401319503784\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4236401319503784\n",
      "q.mlp[0].weight[0,0].item()=0.4240976870059967\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4240976870059967\n",
      "episode reward: 12.0\n",
      "num steps taken: 2378\n",
      "q.mlp[0].weight[0,0].item()=0.42443907260894775\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42443907260894775\n",
      "episode reward: 13.0\n",
      "num steps taken: 2391\n",
      "q.mlp[0].weight[0,0].item()=0.42469149827957153\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42469149827957153\n",
      "episode reward: 10.0\n",
      "num steps taken: 2401\n",
      "q.mlp[0].weight[0,0].item()=0.42492586374282837\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42492586374282837\n",
      "episode reward: 10.0\n",
      "num steps taken: 2411\n",
      "q.mlp[0].weight[0,0].item()=0.42512398958206177\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42512398958206177\n",
      "q.mlp[0].weight[0,0].item()=0.4254021942615509\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4254021942615509\n",
      "episode reward: 10.0\n",
      "num steps taken: 2421\n",
      "q.mlp[0].weight[0,0].item()=0.4255882501602173\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4255882501602173\n",
      "episode reward: 10.0\n",
      "num steps taken: 2431\n",
      "q.mlp[0].weight[0,0].item()=0.425881952047348\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.425881952047348\n",
      "q.mlp[0].weight[0,0].item()=0.4261184632778168\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4261184632778168\n",
      "episode reward: 21.0\n",
      "num steps taken: 2452\n",
      "q.mlp[0].weight[0,0].item()=0.4262355864048004\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4262355864048004\n",
      "episode reward: 9.0\n",
      "num steps taken: 2461\n",
      "q.mlp[0].weight[0,0].item()=0.4264271855354309\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4264271855354309\n",
      "episode reward: 9.0\n",
      "num steps taken: 2470\n",
      "q.mlp[0].weight[0,0].item()=0.4265773892402649\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4265773892402649\n",
      "episode reward: 9.0\n",
      "num steps taken: 2479\n",
      "q.mlp[0].weight[0,0].item()=0.42669054865837097\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42669054865837097\n",
      "episode reward: 10.0\n",
      "num steps taken: 2489\n",
      "q.mlp[0].weight[0,0].item()=0.42690354585647583\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42690354585647583\n",
      "episode reward: 9.0\n",
      "num steps taken: 2498\n",
      "q.mlp[0].weight[0,0].item()=0.42730358242988586\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42730358242988586\n",
      "episode reward: 10.0\n",
      "num steps taken: 2508\n",
      "q.mlp[0].weight[0,0].item()=0.4276963174343109\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4276963174343109\n",
      "episode reward: 10.0\n",
      "num steps taken: 2518\n",
      "q.mlp[0].weight[0,0].item()=0.42783430218696594\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42783430218696594\n",
      "q.mlp[0].weight[0,0].item()=0.4279542565345764\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4279542565345764\n",
      "episode reward: 11.0\n",
      "num steps taken: 2529\n",
      "q.mlp[0].weight[0,0].item()=0.42801937460899353\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42801937460899353\n",
      "episode reward: 10.0\n",
      "num steps taken: 2539\n",
      "q.mlp[0].weight[0,0].item()=0.4280549883842468\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4280549883842468\n",
      "episode reward: 9.0\n",
      "num steps taken: 2548\n",
      "q.mlp[0].weight[0,0].item()=0.42823073267936707\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42823073267936707\n",
      "episode reward: 10.0\n",
      "num steps taken: 2558\n",
      "q.mlp[0].weight[0,0].item()=0.42849090695381165\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42849090695381165\n",
      "episode reward: 9.0\n",
      "num steps taken: 2567\n",
      "q.mlp[0].weight[0,0].item()=0.42876696586608887\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42876696586608887\n",
      "episode reward: 9.0\n",
      "num steps taken: 2576\n",
      "q.mlp[0].weight[0,0].item()=0.42889270186424255\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42889270186424255\n",
      "episode reward: 11.0\n",
      "num steps taken: 2587\n",
      "q.mlp[0].weight[0,0].item()=0.4289270043373108\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289270043373108\n",
      "episode reward: 9.0\n",
      "num steps taken: 2596\n",
      "q.mlp[0].weight[0,0].item()=0.42894431948661804\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42894431948661804\n",
      "episode reward: 10.0\n",
      "num steps taken: 2606\n",
      "q.mlp[0].weight[0,0].item()=0.4288483262062073\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4288483262062073\n",
      "episode reward: 10.0\n",
      "num steps taken: 2616\n",
      "q.mlp[0].weight[0,0].item()=0.42865192890167236\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42865192890167236\n",
      "episode reward: 9.0\n",
      "num steps taken: 2625\n",
      "q.mlp[0].weight[0,0].item()=0.428392618894577\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.428392618894577\n",
      "episode reward: 9.0\n",
      "num steps taken: 2634\n",
      "q.mlp[0].weight[0,0].item()=0.4284440577030182\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4284440577030182\n",
      "episode reward: 9.0\n",
      "num steps taken: 2643\n",
      "q.mlp[0].weight[0,0].item()=0.42848387360572815\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42848387360572815\n",
      "episode reward: 9.0\n",
      "num steps taken: 2652\n",
      "q.mlp[0].weight[0,0].item()=0.42861029505729675\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42861029505729675\n",
      "episode reward: 9.0\n",
      "num steps taken: 2661\n",
      "q.mlp[0].weight[0,0].item()=0.4286133944988251\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4286133944988251\n",
      "episode reward: 10.0\n",
      "num steps taken: 2671\n",
      "q.mlp[0].weight[0,0].item()=0.42875397205352783\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42875397205352783\n",
      "q.mlp[0].weight[0,0].item()=0.42887645959854126\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42887645959854126\n",
      "episode reward: 11.0\n",
      "num steps taken: 2682\n",
      "episode reward: 8.0\n",
      "num steps taken: 2690\n",
      "q.mlp[0].weight[0,0].item()=0.42890313267707825\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42890313267707825\n",
      "q.mlp[0].weight[0,0].item()=0.4288451075553894\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4288451075553894\n",
      "episode reward: 10.0\n",
      "num steps taken: 2700\n",
      "q.mlp[0].weight[0,0].item()=0.4286003112792969\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4286003112792969\n",
      "episode reward: 10.0\n",
      "num steps taken: 2710\n",
      "q.mlp[0].weight[0,0].item()=0.4286298155784607\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4286298155784607\n",
      "episode reward: 10.0\n",
      "num steps taken: 2720\n",
      "q.mlp[0].weight[0,0].item()=0.4285880923271179\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4285880923271179\n",
      "episode reward: 10.0\n",
      "num steps taken: 2730\n",
      "q.mlp[0].weight[0,0].item()=0.42865103483200073\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42865103483200073\n",
      "episode reward: 10.0\n",
      "num steps taken: 2740\n",
      "q.mlp[0].weight[0,0].item()=0.428795725107193\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.428795725107193\n",
      "episode reward: 9.0\n",
      "num steps taken: 2749\n",
      "q.mlp[0].weight[0,0].item()=0.4289029836654663\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289029836654663\n",
      "episode reward: 10.0\n",
      "num steps taken: 2759\n",
      "q.mlp[0].weight[0,0].item()=0.429072767496109\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.429072767496109\n",
      "episode reward: 9.0\n",
      "num steps taken: 2768\n",
      "q.mlp[0].weight[0,0].item()=0.42920035123825073\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42920035123825073\n",
      "episode reward: 8.0\n",
      "num steps taken: 2776\n",
      "q.mlp[0].weight[0,0].item()=0.42920956015586853\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42920956015586853\n",
      "episode reward: 8.0\n",
      "num steps taken: 2784\n",
      "q.mlp[0].weight[0,0].item()=0.4291580021381378\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4291580021381378\n",
      "episode reward: 10.0\n",
      "num steps taken: 2794\n",
      "q.mlp[0].weight[0,0].item()=0.4289262592792511\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289262592792511\n",
      "q.mlp[0].weight[0,0].item()=0.42881983518600464\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42881983518600464\n",
      "episode reward: 14.0\n",
      "num steps taken: 2808\n",
      "q.mlp[0].weight[0,0].item()=0.42851442098617554\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42851442098617554\n",
      "episode reward: 10.0\n",
      "num steps taken: 2818\n",
      "q.mlp[0].weight[0,0].item()=0.42833617329597473\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42833617329597473\n",
      "episode reward: 11.0\n",
      "num steps taken: 2829\n",
      "q.mlp[0].weight[0,0].item()=0.4282972812652588\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4282972812652588\n",
      "episode reward: 10.0\n",
      "num steps taken: 2839\n",
      "q.mlp[0].weight[0,0].item()=0.42813971638679504\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42813971638679504\n",
      "episode reward: 8.0\n",
      "num steps taken: 2847\n",
      "q.mlp[0].weight[0,0].item()=0.428044855594635\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.428044855594635\n",
      "episode reward: 10.0\n",
      "num steps taken: 2857\n",
      "q.mlp[0].weight[0,0].item()=0.42813625931739807\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42813625931739807\n",
      "episode reward: 10.0\n",
      "num steps taken: 2867\n",
      "q.mlp[0].weight[0,0].item()=0.4281734526157379\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4281734526157379\n",
      "episode reward: 10.0\n",
      "num steps taken: 2877\n",
      "q.mlp[0].weight[0,0].item()=0.42834198474884033\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42834198474884033\n",
      "episode reward: 9.0\n",
      "num steps taken: 2886\n",
      "q.mlp[0].weight[0,0].item()=0.4284343123435974\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4284343123435974\n",
      "episode reward: 10.0\n",
      "num steps taken: 2896\n",
      "q.mlp[0].weight[0,0].item()=0.42851150035858154\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42851150035858154\n",
      "q.mlp[0].weight[0,0].item()=0.4286688268184662\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4286688268184662\n",
      "episode reward: 11.0\n",
      "num steps taken: 2907\n",
      "q.mlp[0].weight[0,0].item()=0.42874157428741455\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42874157428741455\n",
      "episode reward: 10.0\n",
      "num steps taken: 2917\n",
      "q.mlp[0].weight[0,0].item()=0.42885124683380127\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42885124683380127\n",
      "episode reward: 9.0\n",
      "num steps taken: 2926\n",
      "q.mlp[0].weight[0,0].item()=0.42886096239089966\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42886096239089966\n",
      "episode reward: 8.0\n",
      "num steps taken: 2934\n",
      "q.mlp[0].weight[0,0].item()=0.4289191961288452\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289191961288452\n",
      "episode reward: 10.0\n",
      "num steps taken: 2944\n",
      "q.mlp[0].weight[0,0].item()=0.42893528938293457\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42893528938293457\n",
      "episode reward: 9.0\n",
      "num steps taken: 2953\n",
      "q.mlp[0].weight[0,0].item()=0.42896658182144165\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42896658182144165\n",
      "episode reward: 12.0\n",
      "num steps taken: 2965\n",
      "q.mlp[0].weight[0,0].item()=0.4289974570274353\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289974570274353\n",
      "episode reward: 9.0\n",
      "num steps taken: 2974\n",
      "q.mlp[0].weight[0,0].item()=0.4289343059062958\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289343059062958\n",
      "episode reward: 9.0\n",
      "num steps taken: 2983\n",
      "q.mlp[0].weight[0,0].item()=0.4289345443248749\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4289345443248749\n",
      "episode reward: 10.0\n",
      "num steps taken: 2993\n",
      "q.mlp[0].weight[0,0].item()=0.42866915464401245\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42866915464401245\n",
      "episode reward: 8.0\n",
      "num steps taken: 3001\n",
      "q.mlp[0].weight[0,0].item()=0.42844754457473755\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42844754457473755\n",
      "episode reward: 11.0\n",
      "num steps taken: 3012\n",
      "q.mlp[0].weight[0,0].item()=0.42823344469070435\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42823344469070435\n",
      "episode reward: 8.0\n",
      "num steps taken: 3020\n",
      "q.mlp[0].weight[0,0].item()=0.4281425178050995\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4281425178050995\n",
      "episode reward: 8.0\n",
      "num steps taken: 3028\n",
      "q.mlp[0].weight[0,0].item()=0.4278656840324402\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4278656840324402\n",
      "episode reward: 10.0\n",
      "num steps taken: 3038\n",
      "q.mlp[0].weight[0,0].item()=0.42745423316955566\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42745423316955566\n",
      "episode reward: 9.0\n",
      "num steps taken: 3047\n",
      "q.mlp[0].weight[0,0].item()=0.4269183874130249\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4269183874130249\n",
      "episode reward: 10.0\n",
      "num steps taken: 3057\n",
      "q.mlp[0].weight[0,0].item()=0.4264427423477173\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4264427423477173\n",
      "q.mlp[0].weight[0,0].item()=0.4261769950389862\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4261769950389862\n",
      "episode reward: 17.0\n",
      "num steps taken: 3074\n",
      "q.mlp[0].weight[0,0].item()=0.42590194940567017\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42590194940567017\n",
      "episode reward: 10.0\n",
      "num steps taken: 3084\n",
      "q.mlp[0].weight[0,0].item()=0.4256775677204132\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4256775677204132\n",
      "episode reward: 10.0\n",
      "num steps taken: 3094\n",
      "q.mlp[0].weight[0,0].item()=0.42557471990585327\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42557471990585327\n",
      "episode reward: 10.0\n",
      "num steps taken: 3104\n",
      "q.mlp[0].weight[0,0].item()=0.425516813993454\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.425516813993454\n",
      "q.mlp[0].weight[0,0].item()=0.4253009855747223\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4253009855747223\n",
      "episode reward: 11.0\n",
      "num steps taken: 3115\n",
      "q.mlp[0].weight[0,0].item()=0.4251069724559784\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4251069724559784\n",
      "episode reward: 8.0\n",
      "num steps taken: 3123\n",
      "q.mlp[0].weight[0,0].item()=0.4248490333557129\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4248490333557129\n",
      "episode reward: 9.0\n",
      "num steps taken: 3132\n",
      "q.mlp[0].weight[0,0].item()=0.4247465133666992\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4247465133666992\n",
      "episode reward: 9.0\n",
      "num steps taken: 3141\n",
      "q.mlp[0].weight[0,0].item()=0.4246656000614166\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4246656000614166\n",
      "episode reward: 9.0\n",
      "num steps taken: 3150\n",
      "q.mlp[0].weight[0,0].item()=0.4246293306350708\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4246293306350708\n",
      "episode reward: 9.0\n",
      "num steps taken: 3159\n",
      "q.mlp[0].weight[0,0].item()=0.42461270093917847\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42461270093917847\n",
      "episode reward: 10.0\n",
      "num steps taken: 3169\n",
      "q.mlp[0].weight[0,0].item()=0.4245702922344208\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4245702922344208\n",
      "episode reward: 10.0\n",
      "num steps taken: 3179\n",
      "q.mlp[0].weight[0,0].item()=0.4246503412723541\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4246503412723541\n",
      "episode reward: 10.0\n",
      "num steps taken: 3189\n",
      "q.mlp[0].weight[0,0].item()=0.42474132776260376\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42474132776260376\n",
      "episode reward: 10.0\n",
      "num steps taken: 3199\n",
      "q.mlp[0].weight[0,0].item()=0.42475515604019165\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42475515604019165\n",
      "episode reward: 8.0\n",
      "num steps taken: 3207\n",
      "q.mlp[0].weight[0,0].item()=0.42473697662353516\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42473697662353516\n",
      "episode reward: 11.0\n",
      "num steps taken: 3218\n",
      "q.mlp[0].weight[0,0].item()=0.42472654581069946\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42472654581069946\n",
      "episode reward: 10.0\n",
      "num steps taken: 3228\n",
      "q.mlp[0].weight[0,0].item()=0.4246979057788849\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4246979057788849\n",
      "episode reward: 9.0\n",
      "num steps taken: 3237\n",
      "q.mlp[0].weight[0,0].item()=0.42460158467292786\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42460158467292786\n",
      "episode reward: 10.0\n",
      "num steps taken: 3247\n",
      "q.mlp[0].weight[0,0].item()=0.42438575625419617\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42438575625419617\n",
      "episode reward: 8.0\n",
      "num steps taken: 3255\n",
      "q.mlp[0].weight[0,0].item()=0.42415162920951843\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42415162920951843\n",
      "episode reward: 10.0\n",
      "num steps taken: 3265\n",
      "q.mlp[0].weight[0,0].item()=0.42382919788360596\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42382919788360596\n",
      "q.mlp[0].weight[0,0].item()=0.4235170781612396\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4235170781612396\n",
      "episode reward: 14.0\n",
      "num steps taken: 3279\n",
      "q.mlp[0].weight[0,0].item()=0.42311662435531616\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42311662435531616\n",
      "episode reward: 12.0\n",
      "num steps taken: 3291\n",
      "q.mlp[0].weight[0,0].item()=0.4225817918777466\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4225817918777466\n",
      "episode reward: 8.0\n",
      "num steps taken: 3299\n",
      "q.mlp[0].weight[0,0].item()=0.4220063388347626\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4220063388347626\n",
      "episode reward: 10.0\n",
      "num steps taken: 3309\n",
      "q.mlp[0].weight[0,0].item()=0.42140623927116394\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42140623927116394\n",
      "q.mlp[0].weight[0,0].item()=0.42093122005462646\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42093122005462646\n",
      "episode reward: 12.0\n",
      "num steps taken: 3321\n",
      "q.mlp[0].weight[0,0].item()=0.42059919238090515\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42059919238090515\n",
      "episode reward: 9.0\n",
      "num steps taken: 3330\n",
      "episode reward: 8.0\n",
      "num steps taken: 3338\n",
      "q.mlp[0].weight[0,0].item()=0.42040058970451355\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42040058970451355\n",
      "episode reward: 9.0\n",
      "num steps taken: 3347\n",
      "q.mlp[0].weight[0,0].item()=0.4201574921607971\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4201574921607971\n",
      "episode reward: 8.0\n",
      "num steps taken: 3355\n",
      "q.mlp[0].weight[0,0].item()=0.41991108655929565\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41991108655929565\n",
      "episode reward: 10.0\n",
      "num steps taken: 3365\n",
      "q.mlp[0].weight[0,0].item()=0.4197968542575836\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4197968542575836\n",
      "q.mlp[0].weight[0,0].item()=0.41961681842803955\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41961681842803955\n",
      "episode reward: 10.0\n",
      "num steps taken: 3375\n",
      "q.mlp[0].weight[0,0].item()=0.4194799065589905\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4194799065589905\n",
      "episode reward: 9.0\n",
      "num steps taken: 3384\n",
      "q.mlp[0].weight[0,0].item()=0.4193577766418457\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4193577766418457\n",
      "episode reward: 9.0\n",
      "num steps taken: 3393\n",
      "q.mlp[0].weight[0,0].item()=0.41914957761764526\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41914957761764526\n",
      "episode reward: 12.0\n",
      "num steps taken: 3405\n",
      "q.mlp[0].weight[0,0].item()=0.4189477562904358\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4189477562904358\n",
      "episode reward: 9.0\n",
      "num steps taken: 3414\n",
      "q.mlp[0].weight[0,0].item()=0.41888105869293213\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41888105869293213\n",
      "episode reward: 12.0\n",
      "num steps taken: 3426\n",
      "q.mlp[0].weight[0,0].item()=0.41888728737831116\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41888728737831116\n",
      "episode reward: 8.0\n",
      "num steps taken: 3434\n",
      "q.mlp[0].weight[0,0].item()=0.4188788831233978\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4188788831233978\n",
      "episode reward: 10.0\n",
      "num steps taken: 3444\n",
      "q.mlp[0].weight[0,0].item()=0.418737530708313\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.418737530708313\n",
      "episode reward: 10.0\n",
      "num steps taken: 3454\n",
      "q.mlp[0].weight[0,0].item()=0.4186016321182251\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4186016321182251\n",
      "episode reward: 9.0\n",
      "num steps taken: 3463\n",
      "q.mlp[0].weight[0,0].item()=0.4183192253112793\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4183192253112793\n",
      "episode reward: 9.0\n",
      "num steps taken: 3472\n",
      "q.mlp[0].weight[0,0].item()=0.4179772734642029\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4179772734642029\n",
      "episode reward: 8.0\n",
      "num steps taken: 3480\n",
      "q.mlp[0].weight[0,0].item()=0.4178816080093384\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4178816080093384\n",
      "episode reward: 9.0\n",
      "num steps taken: 3489\n",
      "q.mlp[0].weight[0,0].item()=0.41782447695732117\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41782447695732117\n",
      "episode reward: 10.0\n",
      "num steps taken: 3499\n",
      "q.mlp[0].weight[0,0].item()=0.4175935983657837\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4175935983657837\n",
      "episode reward: 10.0\n",
      "num steps taken: 3509\n",
      "q.mlp[0].weight[0,0].item()=0.41746750473976135\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41746750473976135\n",
      "episode reward: 9.0\n",
      "num steps taken: 3518\n",
      "q.mlp[0].weight[0,0].item()=0.4171689748764038\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4171689748764038\n",
      "q.mlp[0].weight[0,0].item()=0.4170091450214386\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4170091450214386\n",
      "episode reward: 10.0\n",
      "num steps taken: 3528\n",
      "q.mlp[0].weight[0,0].item()=0.4167422652244568\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4167422652244568\n",
      "episode reward: 10.0\n",
      "num steps taken: 3538\n",
      "q.mlp[0].weight[0,0].item()=0.4163476824760437\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4163476824760437\n",
      "episode reward: 10.0\n",
      "num steps taken: 3548\n",
      "q.mlp[0].weight[0,0].item()=0.4161685109138489\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4161685109138489\n",
      "episode reward: 9.0\n",
      "num steps taken: 3557\n",
      "q.mlp[0].weight[0,0].item()=0.416139155626297\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.416139155626297\n",
      "episode reward: 10.0\n",
      "num steps taken: 3567\n",
      "q.mlp[0].weight[0,0].item()=0.41618093848228455\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41618093848228455\n",
      "episode reward: 9.0\n",
      "num steps taken: 3576\n",
      "q.mlp[0].weight[0,0].item()=0.4163937270641327\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4163937270641327\n",
      "episode reward: 12.0\n",
      "num steps taken: 3588\n",
      "q.mlp[0].weight[0,0].item()=0.41664546728134155\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41664546728134155\n",
      "episode reward: 10.0\n",
      "num steps taken: 3598\n",
      "q.mlp[0].weight[0,0].item()=0.4167773425579071\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4167773425579071\n",
      "episode reward: 10.0\n",
      "num steps taken: 3608\n",
      "q.mlp[0].weight[0,0].item()=0.4168889820575714\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4168889820575714\n",
      "q.mlp[0].weight[0,0].item()=0.4168618619441986\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4168618619441986\n",
      "episode reward: 10.0\n",
      "num steps taken: 3618\n",
      "q.mlp[0].weight[0,0].item()=0.41697320342063904\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41697320342063904\n",
      "episode reward: 9.0\n",
      "num steps taken: 3627\n",
      "q.mlp[0].weight[0,0].item()=0.4169992208480835\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4169992208480835\n",
      "episode reward: 9.0\n",
      "num steps taken: 3636\n",
      "q.mlp[0].weight[0,0].item()=0.41695675253868103\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41695675253868103\n",
      "episode reward: 12.0\n",
      "num steps taken: 3648\n",
      "q.mlp[0].weight[0,0].item()=0.4168824553489685\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4168824553489685\n",
      "episode reward: 10.0\n",
      "num steps taken: 3658\n",
      "q.mlp[0].weight[0,0].item()=0.4166580140590668\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4166580140590668\n",
      "episode reward: 10.0\n",
      "num steps taken: 3668\n",
      "q.mlp[0].weight[0,0].item()=0.4160952866077423\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4160952866077423\n",
      "episode reward: 8.0\n",
      "num steps taken: 3676\n",
      "q.mlp[0].weight[0,0].item()=0.41577693819999695\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41577693819999695\n",
      "episode reward: 10.0\n",
      "num steps taken: 3686\n",
      "q.mlp[0].weight[0,0].item()=0.4155643582344055\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4155643582344055\n",
      "episode reward: 10.0\n",
      "num steps taken: 3696\n",
      "q.mlp[0].weight[0,0].item()=0.4151005744934082\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4151005744934082\n",
      "episode reward: 10.0\n",
      "num steps taken: 3706\n",
      "q.mlp[0].weight[0,0].item()=0.41481146216392517\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41481146216392517\n",
      "episode reward: 9.0\n",
      "num steps taken: 3715\n",
      "q.mlp[0].weight[0,0].item()=0.4144989848136902\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4144989848136902\n",
      "episode reward: 10.0\n",
      "num steps taken: 3725\n",
      "q.mlp[0].weight[0,0].item()=0.4142414331436157\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4142414331436157\n",
      "episode reward: 9.0\n",
      "num steps taken: 3734\n",
      "q.mlp[0].weight[0,0].item()=0.41414061188697815\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41414061188697815\n",
      "q.mlp[0].weight[0,0].item()=0.4138663411140442\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4138663411140442\n",
      "episode reward: 10.0\n",
      "num steps taken: 3744\n",
      "q.mlp[0].weight[0,0].item()=0.41373410820961\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41373410820961\n",
      "episode reward: 10.0\n",
      "num steps taken: 3754\n",
      "q.mlp[0].weight[0,0].item()=0.41352006793022156\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41352006793022156\n",
      "episode reward: 10.0\n",
      "num steps taken: 3764\n",
      "q.mlp[0].weight[0,0].item()=0.41337987780570984\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41337987780570984\n",
      "episode reward: 9.0\n",
      "num steps taken: 3773\n",
      "q.mlp[0].weight[0,0].item()=0.41344305872917175\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41344305872917175\n",
      "episode reward: 9.0\n",
      "num steps taken: 3782\n",
      "q.mlp[0].weight[0,0].item()=0.41351208090782166\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41351208090782166\n",
      "episode reward: 11.0\n",
      "num steps taken: 3793\n",
      "q.mlp[0].weight[0,0].item()=0.4135683476924896\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4135683476924896\n",
      "episode reward: 9.0\n",
      "num steps taken: 3802\n",
      "q.mlp[0].weight[0,0].item()=0.41353389620780945\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41353389620780945\n",
      "episode reward: 10.0\n",
      "num steps taken: 3812\n",
      "q.mlp[0].weight[0,0].item()=0.4134564697742462\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4134564697742462\n",
      "episode reward: 9.0\n",
      "num steps taken: 3821\n",
      "q.mlp[0].weight[0,0].item()=0.41322559118270874\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41322559118270874\n",
      "episode reward: 11.0\n",
      "num steps taken: 3832\n",
      "q.mlp[0].weight[0,0].item()=0.41280630230903625\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41280630230903625\n",
      "q.mlp[0].weight[0,0].item()=0.4125254452228546\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4125254452228546\n",
      "episode reward: 15.0\n",
      "num steps taken: 3847\n",
      "q.mlp[0].weight[0,0].item()=0.4123251140117645\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4123251140117645\n",
      "episode reward: 11.0\n",
      "num steps taken: 3858\n",
      "q.mlp[0].weight[0,0].item()=0.4121955335140228\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4121955335140228\n",
      "episode reward: 10.0\n",
      "num steps taken: 3868\n",
      "q.mlp[0].weight[0,0].item()=0.412058025598526\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.412058025598526\n",
      "episode reward: 10.0\n",
      "num steps taken: 3878\n",
      "q.mlp[0].weight[0,0].item()=0.41191259026527405\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41191259026527405\n",
      "episode reward: 9.0\n",
      "num steps taken: 3887\n",
      "q.mlp[0].weight[0,0].item()=0.4118209481239319\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4118209481239319\n",
      "episode reward: 9.0\n",
      "num steps taken: 3896\n",
      "q.mlp[0].weight[0,0].item()=0.41173529624938965\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41173529624938965\n",
      "q.mlp[0].weight[0,0].item()=0.4116296172142029\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4116296172142029\n",
      "episode reward: 10.0\n",
      "num steps taken: 3906\n",
      "q.mlp[0].weight[0,0].item()=0.41168925166130066\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41168925166130066\n",
      "episode reward: 9.0\n",
      "num steps taken: 3915\n",
      "q.mlp[0].weight[0,0].item()=0.4116779565811157\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4116779565811157\n",
      "episode reward: 10.0\n",
      "num steps taken: 3925\n",
      "q.mlp[0].weight[0,0].item()=0.41147756576538086\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41147756576538086\n",
      "episode reward: 10.0\n",
      "num steps taken: 3935\n",
      "q.mlp[0].weight[0,0].item()=0.4112702012062073\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4112702012062073\n",
      "episode reward: 10.0\n",
      "num steps taken: 3945\n",
      "q.mlp[0].weight[0,0].item()=0.41128551959991455\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41128551959991455\n",
      "episode reward: 10.0\n",
      "num steps taken: 3955\n",
      "q.mlp[0].weight[0,0].item()=0.41136565804481506\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41136565804481506\n",
      "episode reward: 10.0\n",
      "num steps taken: 3965\n",
      "q.mlp[0].weight[0,0].item()=0.41148167848587036\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41148167848587036\n",
      "episode reward: 9.0\n",
      "num steps taken: 3974\n",
      "q.mlp[0].weight[0,0].item()=0.4117013216018677\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4117013216018677\n",
      "episode reward: 12.0\n",
      "num steps taken: 3986\n",
      "q.mlp[0].weight[0,0].item()=0.4118945002555847\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4118945002555847\n",
      "q.mlp[0].weight[0,0].item()=0.4121423363685608\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4121423363685608\n",
      "episode reward: 11.0\n",
      "num steps taken: 3997\n",
      "q.mlp[0].weight[0,0].item()=0.412406861782074\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.412406861782074\n",
      "episode reward: 9.0\n",
      "num steps taken: 4006\n",
      "q.mlp[0].weight[0,0].item()=0.41268861293792725\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41268861293792725\n",
      "episode reward: 9.0\n",
      "num steps taken: 4015\n",
      "q.mlp[0].weight[0,0].item()=0.4129945933818817\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4129945933818817\n",
      "episode reward: 10.0\n",
      "num steps taken: 4025\n",
      "q.mlp[0].weight[0,0].item()=0.41322481632232666\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41322481632232666\n",
      "episode reward: 10.0\n",
      "num steps taken: 4035\n",
      "q.mlp[0].weight[0,0].item()=0.4134346544742584\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4134346544742584\n",
      "episode reward: 9.0\n",
      "num steps taken: 4044\n",
      "q.mlp[0].weight[0,0].item()=0.413507342338562\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.413507342338562\n",
      "episode reward: 9.0\n",
      "num steps taken: 4053\n",
      "q.mlp[0].weight[0,0].item()=0.4136073589324951\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136073589324951\n",
      "episode reward: 9.0\n",
      "num steps taken: 4062\n",
      "q.mlp[0].weight[0,0].item()=0.4137575328350067\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4137575328350067\n",
      "episode reward: 9.0\n",
      "num steps taken: 4071\n",
      "q.mlp[0].weight[0,0].item()=0.41386985778808594\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41386985778808594\n",
      "episode reward: 9.0\n",
      "num steps taken: 4080\n",
      "q.mlp[0].weight[0,0].item()=0.41393953561782837\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41393953561782837\n",
      "episode reward: 10.0\n",
      "num steps taken: 4090\n",
      "q.mlp[0].weight[0,0].item()=0.41414037346839905\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41414037346839905\n",
      "episode reward: 11.0\n",
      "num steps taken: 4101\n",
      "q.mlp[0].weight[0,0].item()=0.4142182469367981\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4142182469367981\n",
      "episode reward: 9.0\n",
      "num steps taken: 4110\n",
      "q.mlp[0].weight[0,0].item()=0.41428399085998535\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41428399085998535\n",
      "episode reward: 10.0\n",
      "num steps taken: 4120\n",
      "q.mlp[0].weight[0,0].item()=0.4144415557384491\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4144415557384491\n",
      "episode reward: 9.0\n",
      "num steps taken: 4129\n",
      "q.mlp[0].weight[0,0].item()=0.4144729971885681\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4144729971885681\n",
      "episode reward: 10.0\n",
      "num steps taken: 4139\n",
      "q.mlp[0].weight[0,0].item()=0.41437476873397827\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41437476873397827\n",
      "q.mlp[0].weight[0,0].item()=0.4143936038017273\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4143936038017273\n",
      "episode reward: 12.0\n",
      "num steps taken: 4151\n",
      "q.mlp[0].weight[0,0].item()=0.41443300247192383\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41443300247192383\n",
      "episode reward: 9.0\n",
      "num steps taken: 4160\n",
      "q.mlp[0].weight[0,0].item()=0.41460326313972473\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41460326313972473\n",
      "episode reward: 8.0\n",
      "num steps taken: 4168\n",
      "q.mlp[0].weight[0,0].item()=0.4148111045360565\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4148111045360565\n",
      "episode reward: 8.0\n",
      "num steps taken: 4176\n",
      "q.mlp[0].weight[0,0].item()=0.4148608148097992\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4148608148097992\n",
      "episode reward: 15.0\n",
      "num steps taken: 4191\n",
      "q.mlp[0].weight[0,0].item()=0.4150450527667999\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4150450527667999\n",
      "episode reward: 9.0\n",
      "num steps taken: 4200\n",
      "q.mlp[0].weight[0,0].item()=0.4152003824710846\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4152003824710846\n",
      "episode reward: 11.0\n",
      "num steps taken: 4211\n",
      "q.mlp[0].weight[0,0].item()=0.41523805260658264\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41523805260658264\n",
      "episode reward: 9.0\n",
      "num steps taken: 4220\n",
      "q.mlp[0].weight[0,0].item()=0.4151242673397064\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4151242673397064\n",
      "q.mlp[0].weight[0,0].item()=0.4148727357387543\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4148727357387543\n",
      "episode reward: 10.0\n",
      "num steps taken: 4230\n",
      "episode reward: 8.0\n",
      "num steps taken: 4238\n",
      "q.mlp[0].weight[0,0].item()=0.4145891070365906\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4145891070365906\n",
      "q.mlp[0].weight[0,0].item()=0.41447749733924866\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41447749733924866\n",
      "episode reward: 11.0\n",
      "num steps taken: 4249\n",
      "q.mlp[0].weight[0,0].item()=0.41437387466430664\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41437387466430664\n",
      "episode reward: 9.0\n",
      "num steps taken: 4258\n",
      "q.mlp[0].weight[0,0].item()=0.41431769728660583\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41431769728660583\n",
      "episode reward: 8.0\n",
      "num steps taken: 4266\n",
      "q.mlp[0].weight[0,0].item()=0.41420117020606995\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41420117020606995\n",
      "episode reward: 10.0\n",
      "num steps taken: 4276\n",
      "q.mlp[0].weight[0,0].item()=0.4141186475753784\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4141186475753784\n",
      "episode reward: 9.0\n",
      "num steps taken: 4285\n",
      "q.mlp[0].weight[0,0].item()=0.41388997435569763\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41388997435569763\n",
      "episode reward: 9.0\n",
      "num steps taken: 4294\n",
      "q.mlp[0].weight[0,0].item()=0.41371089220046997\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41371089220046997\n",
      "episode reward: 8.0\n",
      "num steps taken: 4302\n",
      "episode reward: 8.0\n",
      "num steps taken: 4310\n",
      "q.mlp[0].weight[0,0].item()=0.4137173891067505\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4137173891067505\n",
      "episode reward: 8.0\n",
      "num steps taken: 4318\n",
      "q.mlp[0].weight[0,0].item()=0.41403552889823914\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41403552889823914\n",
      "episode reward: 9.0\n",
      "num steps taken: 4327\n",
      "q.mlp[0].weight[0,0].item()=0.41415995359420776\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41415995359420776\n",
      "q.mlp[0].weight[0,0].item()=0.41403985023498535\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41403985023498535\n",
      "episode reward: 15.0\n",
      "num steps taken: 4342\n",
      "q.mlp[0].weight[0,0].item()=0.41405031085014343\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41405031085014343\n",
      "episode reward: 10.0\n",
      "num steps taken: 4352\n",
      "q.mlp[0].weight[0,0].item()=0.41408535838127136\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41408535838127136\n",
      "episode reward: 9.0\n",
      "num steps taken: 4361\n",
      "q.mlp[0].weight[0,0].item()=0.4141255021095276\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4141255021095276\n",
      "episode reward: 9.0\n",
      "num steps taken: 4370\n",
      "q.mlp[0].weight[0,0].item()=0.4140715003013611\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4140715003013611\n",
      "episode reward: 9.0\n",
      "num steps taken: 4379\n",
      "q.mlp[0].weight[0,0].item()=0.41412675380706787\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41412675380706787\n",
      "episode reward: 10.0\n",
      "num steps taken: 4389\n",
      "q.mlp[0].weight[0,0].item()=0.4141847491264343\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4141847491264343\n",
      "episode reward: 10.0\n",
      "num steps taken: 4399\n",
      "q.mlp[0].weight[0,0].item()=0.41412732005119324\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41412732005119324\n",
      "episode reward: 8.0\n",
      "num steps taken: 4407\n",
      "q.mlp[0].weight[0,0].item()=0.41400036215782166\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41400036215782166\n",
      "episode reward: 10.0\n",
      "num steps taken: 4417\n",
      "q.mlp[0].weight[0,0].item()=0.4137447476387024\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4137447476387024\n",
      "episode reward: 8.0\n",
      "num steps taken: 4425\n",
      "q.mlp[0].weight[0,0].item()=0.41352227330207825\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41352227330207825\n",
      "episode reward: 8.0\n",
      "num steps taken: 4433\n",
      "q.mlp[0].weight[0,0].item()=0.41325145959854126\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41325145959854126\n",
      "episode reward: 11.0\n",
      "num steps taken: 4444\n",
      "q.mlp[0].weight[0,0].item()=0.41324177384376526\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41324177384376526\n",
      "episode reward: 10.0\n",
      "num steps taken: 4454\n",
      "q.mlp[0].weight[0,0].item()=0.4133455455303192\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4133455455303192\n",
      "episode reward: 9.0\n",
      "num steps taken: 4463\n",
      "q.mlp[0].weight[0,0].item()=0.41332224011421204\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41332224011421204\n",
      "episode reward: 9.0\n",
      "num steps taken: 4472\n",
      "q.mlp[0].weight[0,0].item()=0.41340669989585876\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41340669989585876\n",
      "episode reward: 8.0\n",
      "num steps taken: 4480\n",
      "q.mlp[0].weight[0,0].item()=0.4135584235191345\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4135584235191345\n",
      "episode reward: 10.0\n",
      "num steps taken: 4490\n",
      "q.mlp[0].weight[0,0].item()=0.4135932922363281\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4135932922363281\n",
      "q.mlp[0].weight[0,0].item()=0.4136328101158142\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136328101158142\n",
      "episode reward: 10.0\n",
      "num steps taken: 4500\n",
      "q.mlp[0].weight[0,0].item()=0.41365769505500793\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41365769505500793\n",
      "episode reward: 10.0\n",
      "num steps taken: 4510\n",
      "q.mlp[0].weight[0,0].item()=0.4136675000190735\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136675000190735\n",
      "episode reward: 12.0\n",
      "num steps taken: 4522\n",
      "q.mlp[0].weight[0,0].item()=0.4136871099472046\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136871099472046\n",
      "episode reward: 11.0\n",
      "num steps taken: 4533\n",
      "q.mlp[0].weight[0,0].item()=0.413709819316864\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.413709819316864\n",
      "episode reward: 9.0\n",
      "num steps taken: 4542\n",
      "q.mlp[0].weight[0,0].item()=0.4136081337928772\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136081337928772\n",
      "episode reward: 8.0\n",
      "num steps taken: 4550\n",
      "q.mlp[0].weight[0,0].item()=0.413634717464447\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.413634717464447\n",
      "episode reward: 8.0\n",
      "num steps taken: 4558\n",
      "q.mlp[0].weight[0,0].item()=0.4136713445186615\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4136713445186615\n",
      "episode reward: 11.0\n",
      "num steps taken: 4569\n",
      "q.mlp[0].weight[0,0].item()=0.41384178400039673\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41384178400039673\n",
      "episode reward: 8.0\n",
      "num steps taken: 4577\n",
      "q.mlp[0].weight[0,0].item()=0.4140896797180176\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4140896797180176\n",
      "episode reward: 10.0\n",
      "num steps taken: 4587\n",
      "q.mlp[0].weight[0,0].item()=0.41437721252441406\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41437721252441406\n",
      "episode reward: 11.0\n",
      "num steps taken: 4598\n",
      "q.mlp[0].weight[0,0].item()=0.414589524269104\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.414589524269104\n",
      "q.mlp[0].weight[0,0].item()=0.4147929251194\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4147929251194\n",
      "episode reward: 13.0\n",
      "num steps taken: 4611\n",
      "q.mlp[0].weight[0,0].item()=0.41480395197868347\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41480395197868347\n",
      "episode reward: 10.0\n",
      "num steps taken: 4621\n",
      "q.mlp[0].weight[0,0].item()=0.41469138860702515\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41469138860702515\n",
      "episode reward: 10.0\n",
      "num steps taken: 4631\n",
      "q.mlp[0].weight[0,0].item()=0.4144556522369385\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4144556522369385\n",
      "episode reward: 10.0\n",
      "num steps taken: 4641\n",
      "q.mlp[0].weight[0,0].item()=0.4142943024635315\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4142943024635315\n",
      "episode reward: 10.0\n",
      "num steps taken: 4651\n",
      "q.mlp[0].weight[0,0].item()=0.41416120529174805\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41416120529174805\n",
      "q.mlp[0].weight[0,0].item()=0.4140399098396301\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4140399098396301\n",
      "episode reward: 14.0\n",
      "num steps taken: 4665\n",
      "q.mlp[0].weight[0,0].item()=0.4139883816242218\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4139883816242218\n",
      "episode reward: 12.0\n",
      "num steps taken: 4677\n",
      "q.mlp[0].weight[0,0].item()=0.4137408137321472\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4137408137321472\n",
      "episode reward: 10.0\n",
      "num steps taken: 4687\n",
      "q.mlp[0].weight[0,0].item()=0.4135807156562805\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4135807156562805\n",
      "episode reward: 9.0\n",
      "num steps taken: 4696\n",
      "q.mlp[0].weight[0,0].item()=0.41336119174957275\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41336119174957275\n",
      "episode reward: 9.0\n",
      "num steps taken: 4705\n",
      "q.mlp[0].weight[0,0].item()=0.4133937656879425\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4133937656879425\n",
      "q.mlp[0].weight[0,0].item()=0.41355323791503906\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41355323791503906\n",
      "episode reward: 11.0\n",
      "num steps taken: 4716\n",
      "episode reward: 8.0\n",
      "num steps taken: 4724\n",
      "q.mlp[0].weight[0,0].item()=0.41365739703178406\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41365739703178406\n",
      "q.mlp[0].weight[0,0].item()=0.41392531991004944\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41392531991004944\n",
      "episode reward: 11.0\n",
      "num steps taken: 4735\n",
      "q.mlp[0].weight[0,0].item()=0.41409802436828613\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41409802436828613\n",
      "episode reward: 11.0\n",
      "num steps taken: 4746\n",
      "q.mlp[0].weight[0,0].item()=0.4140971004962921\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4140971004962921\n",
      "episode reward: 10.0\n",
      "num steps taken: 4756\n",
      "q.mlp[0].weight[0,0].item()=0.41418540477752686\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41418540477752686\n",
      "q.mlp[0].weight[0,0].item()=0.41417691111564636\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41417691111564636\n",
      "q.mlp[0].weight[0,0].item()=0.4142247438430786\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4142247438430786\n",
      "episode reward: 24.0\n",
      "num steps taken: 4780\n",
      "q.mlp[0].weight[0,0].item()=0.41418588161468506\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41418588161468506\n",
      "episode reward: 11.0\n",
      "num steps taken: 4791\n",
      "q.mlp[0].weight[0,0].item()=0.4140288829803467\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4140288829803467\n",
      "episode reward: 11.0\n",
      "num steps taken: 4802\n",
      "q.mlp[0].weight[0,0].item()=0.413919597864151\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.413919597864151\n",
      "episode reward: 10.0\n",
      "num steps taken: 4812\n",
      "q.mlp[0].weight[0,0].item()=0.41378796100616455\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41378796100616455\n",
      "episode reward: 10.0\n",
      "num steps taken: 4822\n",
      "q.mlp[0].weight[0,0].item()=0.4135721027851105\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4135721027851105\n",
      "episode reward: 8.0\n",
      "num steps taken: 4830\n",
      "q.mlp[0].weight[0,0].item()=0.4133528470993042\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4133528470993042\n",
      "episode reward: 8.0\n",
      "num steps taken: 4838\n",
      "q.mlp[0].weight[0,0].item()=0.41313183307647705\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41313183307647705\n",
      "episode reward: 10.0\n",
      "num steps taken: 4848\n",
      "q.mlp[0].weight[0,0].item()=0.41282594203948975\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41282594203948975\n",
      "episode reward: 11.0\n",
      "num steps taken: 4859\n",
      "q.mlp[0].weight[0,0].item()=0.4125325083732605\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4125325083732605\n",
      "q.mlp[0].weight[0,0].item()=0.4122627377510071\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4122627377510071\n",
      "episode reward: 10.0\n",
      "num steps taken: 4869\n",
      "q.mlp[0].weight[0,0].item()=0.4121270477771759\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4121270477771759\n",
      "episode reward: 9.0\n",
      "num steps taken: 4878\n",
      "q.mlp[0].weight[0,0].item()=0.41200026869773865\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41200026869773865\n",
      "episode reward: 10.0\n",
      "num steps taken: 4888\n",
      "q.mlp[0].weight[0,0].item()=0.412253201007843\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.412253201007843\n",
      "episode reward: 10.0\n",
      "num steps taken: 4898\n",
      "q.mlp[0].weight[0,0].item()=0.4123821556568146\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4123821556568146\n",
      "episode reward: 8.0\n",
      "num steps taken: 4906\n",
      "q.mlp[0].weight[0,0].item()=0.4123595654964447\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4123595654964447\n",
      "episode reward: 8.0\n",
      "num steps taken: 4914\n",
      "q.mlp[0].weight[0,0].item()=0.41227686405181885\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41227686405181885\n",
      "episode reward: 10.0\n",
      "num steps taken: 4924\n",
      "q.mlp[0].weight[0,0].item()=0.41224023699760437\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41224023699760437\n",
      "episode reward: 12.0\n",
      "num steps taken: 4936\n",
      "q.mlp[0].weight[0,0].item()=0.4120362401008606\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4120362401008606\n",
      "episode reward: 10.0\n",
      "num steps taken: 4946\n",
      "q.mlp[0].weight[0,0].item()=0.41202452778816223\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41202452778816223\n",
      "episode reward: 9.0\n",
      "num steps taken: 4955\n",
      "q.mlp[0].weight[0,0].item()=0.4120465815067291\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4120465815067291\n",
      "episode reward: 9.0\n",
      "num steps taken: 4964\n",
      "q.mlp[0].weight[0,0].item()=0.41210612654685974\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41210612654685974\n",
      "episode reward: 8.0\n",
      "num steps taken: 4972\n",
      "q.mlp[0].weight[0,0].item()=0.4122089445590973\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4122089445590973\n",
      "episode reward: 9.0\n",
      "num steps taken: 4981\n",
      "q.mlp[0].weight[0,0].item()=0.41225215792655945\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41225215792655945\n",
      "episode reward: 11.0\n",
      "num steps taken: 4992\n",
      "q.mlp[0].weight[0,0].item()=0.4123659133911133\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4123659133911133\n",
      "episode reward: 8.0\n",
      "num steps taken: 5000\n",
      "q.mlp[0].weight[0,0].item()=0.41241228580474854\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41241228580474854\n",
      "episode reward: 10.0\n",
      "num steps taken: 5010\n",
      "q.mlp[0].weight[0,0].item()=0.41249263286590576\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41249263286590576\n",
      "episode reward: 10.0\n",
      "num steps taken: 5020\n",
      "q.mlp[0].weight[0,0].item()=0.4125206172466278\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4125206172466278\n",
      "q.mlp[0].weight[0,0].item()=0.41226452589035034\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41226452589035034\n",
      "episode reward: 11.0\n",
      "num steps taken: 5031\n",
      "q.mlp[0].weight[0,0].item()=0.41204074025154114\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41204074025154114\n",
      "episode reward: 9.0\n",
      "num steps taken: 5040\n",
      "q.mlp[0].weight[0,0].item()=0.4120239317417145\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4120239317417145\n",
      "episode reward: 10.0\n",
      "num steps taken: 5050\n",
      "q.mlp[0].weight[0,0].item()=0.41204696893692017\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41204696893692017\n",
      "episode reward: 10.0\n",
      "num steps taken: 5060\n",
      "q.mlp[0].weight[0,0].item()=0.4120745062828064\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4120745062828064\n",
      "episode reward: 9.0\n",
      "num steps taken: 5069\n",
      "q.mlp[0].weight[0,0].item()=0.4118695855140686\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4118695855140686\n",
      "episode reward: 10.0\n",
      "num steps taken: 5079\n",
      "q.mlp[0].weight[0,0].item()=0.4117138981819153\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4117138981819153\n",
      "episode reward: 8.0\n",
      "num steps taken: 5087\n",
      "q.mlp[0].weight[0,0].item()=0.4116434156894684\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4116434156894684\n",
      "episode reward: 13.0\n",
      "num steps taken: 5100\n",
      "q.mlp[0].weight[0,0].item()=0.4117557406425476\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4117557406425476\n",
      "episode reward: 10.0\n",
      "num steps taken: 5110\n",
      "q.mlp[0].weight[0,0].item()=0.41195106506347656\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41195106506347656\n",
      "episode reward: 9.0\n",
      "num steps taken: 5119\n",
      "q.mlp[0].weight[0,0].item()=0.4121977686882019\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4121977686882019\n",
      "q.mlp[0].weight[0,0].item()=0.4123522639274597\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4123522639274597\n",
      "episode reward: 11.0\n",
      "num steps taken: 5130\n",
      "q.mlp[0].weight[0,0].item()=0.4127330183982849\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4127330183982849\n",
      "episode reward: 10.0\n",
      "num steps taken: 5140\n",
      "q.mlp[0].weight[0,0].item()=0.41309791803359985\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41309791803359985\n",
      "episode reward: 10.0\n",
      "num steps taken: 5150\n",
      "q.mlp[0].weight[0,0].item()=0.41355711221694946\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41355711221694946\n",
      "episode reward: 9.0\n",
      "num steps taken: 5159\n",
      "q.mlp[0].weight[0,0].item()=0.4141070544719696\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4141070544719696\n",
      "episode reward: 10.0\n",
      "num steps taken: 5169\n",
      "q.mlp[0].weight[0,0].item()=0.4145704209804535\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4145704209804535\n",
      "episode reward: 11.0\n",
      "num steps taken: 5180\n",
      "q.mlp[0].weight[0,0].item()=0.4148910939693451\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4148910939693451\n",
      "episode reward: 9.0\n",
      "num steps taken: 5189\n",
      "q.mlp[0].weight[0,0].item()=0.41506636142730713\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41506636142730713\n",
      "episode reward: 8.0\n",
      "num steps taken: 5197\n",
      "q.mlp[0].weight[0,0].item()=0.4152987003326416\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4152987003326416\n",
      "q.mlp[0].weight[0,0].item()=0.41550400853157043\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41550400853157043\n",
      "q.mlp[0].weight[0,0].item()=0.41588446497917175\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41588446497917175\n",
      "episode reward: 24.0\n",
      "num steps taken: 5221\n",
      "q.mlp[0].weight[0,0].item()=0.41617581248283386\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41617581248283386\n",
      "episode reward: 10.0\n",
      "num steps taken: 5231\n",
      "q.mlp[0].weight[0,0].item()=0.41649889945983887\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41649889945983887\n",
      "episode reward: 10.0\n",
      "num steps taken: 5241\n",
      "q.mlp[0].weight[0,0].item()=0.41697263717651367\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41697263717651367\n",
      "episode reward: 9.0\n",
      "num steps taken: 5250\n",
      "q.mlp[0].weight[0,0].item()=0.41751155257225037\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41751155257225037\n",
      "episode reward: 10.0\n",
      "num steps taken: 5260\n",
      "q.mlp[0].weight[0,0].item()=0.4180591404438019\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4180591404438019\n",
      "episode reward: 8.0\n",
      "num steps taken: 5268\n",
      "q.mlp[0].weight[0,0].item()=0.4184216558933258\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4184216558933258\n",
      "episode reward: 9.0\n",
      "num steps taken: 5277\n",
      "q.mlp[0].weight[0,0].item()=0.41867440938949585\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41867440938949585\n",
      "episode reward: 10.0\n",
      "num steps taken: 5287\n",
      "q.mlp[0].weight[0,0].item()=0.4188818037509918\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4188818037509918\n",
      "episode reward: 9.0\n",
      "num steps taken: 5296\n",
      "q.mlp[0].weight[0,0].item()=0.4190719425678253\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4190719425678253\n",
      "episode reward: 8.0\n",
      "num steps taken: 5304\n",
      "q.mlp[0].weight[0,0].item()=0.41921186447143555\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41921186447143555\n",
      "episode reward: 9.0\n",
      "num steps taken: 5313\n",
      "q.mlp[0].weight[0,0].item()=0.4193372428417206\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4193372428417206\n",
      "episode reward: 12.0\n",
      "num steps taken: 5325\n",
      "q.mlp[0].weight[0,0].item()=0.41958487033843994\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41958487033843994\n",
      "episode reward: 8.0\n",
      "num steps taken: 5333\n",
      "q.mlp[0].weight[0,0].item()=0.4198661148548126\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4198661148548126\n",
      "episode reward: 9.0\n",
      "num steps taken: 5342\n",
      "q.mlp[0].weight[0,0].item()=0.4201620817184448\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4201620817184448\n",
      "episode reward: 10.0\n",
      "num steps taken: 5352\n",
      "q.mlp[0].weight[0,0].item()=0.4204665422439575\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4204665422439575\n",
      "episode reward: 9.0\n",
      "num steps taken: 5361\n",
      "q.mlp[0].weight[0,0].item()=0.42080917954444885\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42080917954444885\n",
      "episode reward: 9.0\n",
      "num steps taken: 5370\n",
      "q.mlp[0].weight[0,0].item()=0.4210602939128876\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4210602939128876\n",
      "q.mlp[0].weight[0,0].item()=0.4213543236255646\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4213543236255646\n",
      "episode reward: 17.0\n",
      "num steps taken: 5387\n",
      "q.mlp[0].weight[0,0].item()=0.42154765129089355\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42154765129089355\n",
      "episode reward: 9.0\n",
      "num steps taken: 5396\n",
      "q.mlp[0].weight[0,0].item()=0.4218621551990509\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4218621551990509\n",
      "episode reward: 10.0\n",
      "num steps taken: 5406\n",
      "q.mlp[0].weight[0,0].item()=0.4219065308570862\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4219065308570862\n",
      "episode reward: 9.0\n",
      "num steps taken: 5415\n",
      "q.mlp[0].weight[0,0].item()=0.4217708706855774\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4217708706855774\n",
      "q.mlp[0].weight[0,0].item()=0.42146405577659607\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42146405577659607\n",
      "episode reward: 13.0\n",
      "num steps taken: 5428\n",
      "q.mlp[0].weight[0,0].item()=0.4210038185119629\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4210038185119629\n",
      "episode reward: 9.0\n",
      "num steps taken: 5437\n",
      "q.mlp[0].weight[0,0].item()=0.42055606842041016\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.42055606842041016\n",
      "episode reward: 10.0\n",
      "num steps taken: 5447\n",
      "q.mlp[0].weight[0,0].item()=0.4201221168041229\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4201221168041229\n",
      "episode reward: 9.0\n",
      "num steps taken: 5456\n",
      "q.mlp[0].weight[0,0].item()=0.4196048974990845\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4196048974990845\n",
      "episode reward: 10.0\n",
      "num steps taken: 5466\n",
      "q.mlp[0].weight[0,0].item()=0.41904571652412415\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41904571652412415\n",
      "episode reward: 10.0\n",
      "num steps taken: 5476\n",
      "q.mlp[0].weight[0,0].item()=0.41842833161354065\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41842833161354065\n",
      "episode reward: 10.0\n",
      "num steps taken: 5486\n",
      "q.mlp[0].weight[0,0].item()=0.41789767146110535\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41789767146110535\n",
      "episode reward: 12.0\n",
      "num steps taken: 5498\n",
      "q.mlp[0].weight[0,0].item()=0.4173067510128021\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4173067510128021\n",
      "episode reward: 9.0\n",
      "num steps taken: 5507\n",
      "q.mlp[0].weight[0,0].item()=0.41666552424430847\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41666552424430847\n",
      "q.mlp[0].weight[0,0].item()=0.4160712957382202\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4160712957382202\n",
      "episode reward: 10.0\n",
      "num steps taken: 5517\n",
      "q.mlp[0].weight[0,0].item()=0.4155306816101074\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4155306816101074\n",
      "episode reward: 11.0\n",
      "num steps taken: 5528\n",
      "q.mlp[0].weight[0,0].item()=0.4149314761161804\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4149314761161804\n",
      "episode reward: 10.0\n",
      "num steps taken: 5538\n",
      "q.mlp[0].weight[0,0].item()=0.41447681188583374\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41447681188583374\n",
      "q.mlp[0].weight[0,0].item()=0.4141836166381836\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4141836166381836\n",
      "q.mlp[0].weight[0,0].item()=0.4138720631599426\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4138720631599426\n",
      "episode reward: 27.0\n",
      "num steps taken: 5565\n",
      "q.mlp[0].weight[0,0].item()=0.4137035608291626\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4137035608291626\n",
      "episode reward: 9.0\n",
      "num steps taken: 5574\n",
      "q.mlp[0].weight[0,0].item()=0.41356194019317627\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41356194019317627\n",
      "episode reward: 10.0\n",
      "num steps taken: 5584\n",
      "q.mlp[0].weight[0,0].item()=0.4133932888507843\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4133932888507843\n",
      "episode reward: 11.0\n",
      "num steps taken: 5595\n",
      "q.mlp[0].weight[0,0].item()=0.41312897205352783\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41312897205352783\n",
      "episode reward: 10.0\n",
      "num steps taken: 5605\n",
      "q.mlp[0].weight[0,0].item()=0.41303324699401855\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41303324699401855\n",
      "episode reward: 9.0\n",
      "num steps taken: 5614\n",
      "q.mlp[0].weight[0,0].item()=0.4128892123699188\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4128892123699188\n",
      "q.mlp[0].weight[0,0].item()=0.41294944286346436\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41294944286346436\n",
      "episode reward: 11.0\n",
      "num steps taken: 5625\n",
      "q.mlp[0].weight[0,0].item()=0.41287118196487427\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41287118196487427\n",
      "episode reward: 11.0\n",
      "num steps taken: 5636\n",
      "q.mlp[0].weight[0,0].item()=0.4126998782157898\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4126998782157898\n",
      "episode reward: 13.0\n",
      "num steps taken: 5649\n",
      "q.mlp[0].weight[0,0].item()=0.4126480221748352\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4126480221748352\n",
      "episode reward: 10.0\n",
      "num steps taken: 5659\n",
      "q.mlp[0].weight[0,0].item()=0.41258370876312256\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41258370876312256\n",
      "q.mlp[0].weight[0,0].item()=0.41254013776779175\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41254013776779175\n",
      "episode reward: 12.0\n",
      "num steps taken: 5671\n",
      "q.mlp[0].weight[0,0].item()=0.4125918447971344\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4125918447971344\n",
      "episode reward: 10.0\n",
      "num steps taken: 5681\n",
      "q.mlp[0].weight[0,0].item()=0.4127404987812042\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4127404987812042\n",
      "episode reward: 10.0\n",
      "num steps taken: 5691\n",
      "q.mlp[0].weight[0,0].item()=0.41307899355888367\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41307899355888367\n",
      "episode reward: 9.0\n",
      "num steps taken: 5700\n",
      "q.mlp[0].weight[0,0].item()=0.41347742080688477\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41347742080688477\n",
      "episode reward: 10.0\n",
      "num steps taken: 5710\n",
      "q.mlp[0].weight[0,0].item()=0.41384264826774597\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41384264826774597\n",
      "episode reward: 8.0\n",
      "num steps taken: 5718\n",
      "q.mlp[0].weight[0,0].item()=0.41425102949142456\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41425102949142456\n",
      "episode reward: 8.0\n",
      "num steps taken: 5726\n",
      "q.mlp[0].weight[0,0].item()=0.4147873520851135\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4147873520851135\n",
      "episode reward: 10.0\n",
      "num steps taken: 5736\n",
      "q.mlp[0].weight[0,0].item()=0.41530606150627136\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.41530606150627136\n",
      "q.mlp[0].weight[0,0].item()=0.4157866835594177\n",
      "frozen_q.mlp[0].weight[0,0].item()=0.4157866835594177\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "num_observations = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "qnetwork = QNetwork(num_observations, 64, num_actions)\n",
    "\n",
    "def train(q: nn.Module, env: gym.Env, episodes: int = 1000, eps: float = 0.05, max_buffer_size = 10000, discount = 0.98,\n",
    "          train_freq = 16, batch_size = 128, lr = 1e-3, steps = 20000, video_name = \"train_0\") -> float:\n",
    "    \n",
    "    # (observation, action, reward, done)\n",
    "    # We can index into next_observation: if current observation is at index i then the next observation is at index i+1\n",
    "    buffer = deque()\n",
    "    \n",
    "    num_steps_taken = 0\n",
    "    check_for_training = 10000\n",
    "\n",
    "    frozen_q = copy.deepcopy(q)\n",
    "    frozen_q.eval()\n",
    "    q.train()\n",
    "    optimizer = torch.optim.Adam(q.parameters(), lr=lr)\n",
    "    \n",
    "    # Ensure that VideoRecorder is initialized outside\n",
    "    video_recorder = VideoRecorder(env,\n",
    "        f\"videos/{video_name}.mp4\"\n",
    "    )    \n",
    "\n",
    "    for episode in range(episodes):\n",
    "        print('test')\n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            num_steps_taken += 1\n",
    "            if record:\n",
    "                video_recorder.capture_frame()\n",
    "            else:  \n",
    "                show_state(env)\n",
    "            # Batch the observation\n",
    "            obs = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            action = make_choice(env, eps, q, obs, 'cuda:0').item()\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # If buffer is full, take oldest time out\n",
    "            if len(buffer) == max_buffer_size:\n",
    "                buffer.popleft()\n",
    "            buffer.append((cur_state, action, reward, done)) \n",
    "                \n",
    "            new_state = cur_state\n",
    "            # print(f\"{reward=}\")\n",
    "            episode_reward += reward\n",
    "\n",
    "                \n",
    "            if num_steps_taken % train_freq == 0 and num_steps_taken > batch_size:\n",
    "                train_freq += 1\n",
    "                # Get sample indices\n",
    "                # Doing a len(buffer) - 1 so there's always a next sample\n",
    "                buffer_size = min(len(buffer), max_buffer_size)\n",
    "                # print(f\"{buffer_size=}\")\n",
    "                # print(f\"{batch_size=}\")\n",
    "                buffer_indices = random.sample(range(buffer_size - 1), batch_size)\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "                for index in buffer_indices:\n",
    "                    \n",
    "                    cur_state, cur_action, cur_reward, cur_done = buffer[index]\n",
    "                    next_state, _, _, next_state_done = buffer[index+1]\n",
    "                    \n",
    "                    cur_state = torch.tensor(cur_state, dtype=torch.float)\n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # If we have no next state, then we can't do a max over the Q function\n",
    "                        # So we resort to taking the reward\n",
    "                        if cur_done:\n",
    "                            target = reward\n",
    "                        else:\n",
    "                            target = reward + discount * max(frozen_q(next_state))\n",
    "                    \n",
    "                    predicted = q(cur_state)[cur_action]\n",
    "                    \n",
    "                    loss += (target - predicted)**2\n",
    "                    \n",
    "                loss /= batch_size\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                                    \n",
    "                # Update frozen Q network\n",
    "                frozen_q.load_state_dict(q.state_dict())\n",
    "                # print(f\"{q.mlp[0].weight[0,0].item()=}\")\n",
    "                # print(f\"{frozen_q.mlp[0].weight[0,0].item()=}\")\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"episode {epi reward: {episode_reward}\")\n",
    "        if num_steps_taken >= steps:\n",
    "            break\n",
    "        if num_steps_taken % check_for_training == 0:\n",
    "            print(\"evaluating agent\")\n",
    "            evaluate(frozen_q, env, eps=0, video_name=f\"training_checkpoint_{num_steps_taken}\")\n",
    "        print(f\"num steps taken: {num_steps_taken}\")\n",
    "    if record:\n",
    "        video_recorder.close()\n",
    "train(qnetwork, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1145, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnetwork.mlp[0].weight[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train the QNetwork from the buffer\n",
    "buffer_indices  = random.random_indices(num_samples, (0, buf_size-1))\n",
    "obs, action, reward, done = buffer[i]\n",
    "next_obs, _, _, _ = buffer[i+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
