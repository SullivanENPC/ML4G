{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from torch import nn \n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import transformers\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psudeocode for single head\n",
    "\n",
    "raw_attention_pattern(activations, query_fn, key_fn):\n",
    "    K = key_fn(activations)\n",
    "    Q = query_fn(activations)\n",
    "    return QK^T / sqrt(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.00398 STD: 0.1104 VALS [0.1204 -0.09962 -0.1777 0.09833 0.1279 -0.03608 0.03733 0.1682 -0.101 -0.08703...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int, \n",
    "    project_query,\n",
    "    project_key\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        token_activations: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "        num_heads: int, \n",
    "        project_query: function( (Tensor[b, n, c: 768]) -> Tensor[b, n, (hn hs) : 768] ),\n",
    "        project_key: function( (Tensor[..., 768]) -> Tensor[..., 768] \n",
    "    returns:\n",
    "        Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "    \"\"\"\n",
    "    Q = rearrange(project_query(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    K = rearrange(project_key(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    ans = einsum(\"bhji,bhki->bhjk\", K, Q)\n",
    "    head_size = Q.shape[3]\n",
    "    return ans / math.sqrt(head_size)\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.00331 STD: 0.1175 VALS [0.08122 -0.097 0.1457 -0.09818 0.1429 -0.09786 -0.03961 0.05037 -0.08267 0.04423...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int, \n",
    "    attention_pattern: t.Tensor,\n",
    "    project_value, \n",
    "    project_output,\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        token_activations: Tensor[batch_size, seq_length, hidden_size (768)], \n",
    "        num_heads: int, \n",
    "        attention_pattern: Tensor[batch_size,num_heads, seq_length, seq_length], \n",
    "        project_value: function( (Tensor[..., 768]) -> Tensor[..., 768] ), \n",
    "        project_output: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    output:\n",
    "        Tensor[batch_size, seq_length, hidden_size] \n",
    "    \"\"\"\n",
    "    attention_pattern = t.softmax(attention_pattern, dim = 2)\n",
    "    V = rearrange(project_value(token_activations), \"b n (hn hs) -> b hn n hs\", hn = num_heads)\n",
    "    A = rearrange(einsum('bhkq,bhki->bhqi',attention_pattern,V), \"b hn n hs -> b n (hn hs)\", hn = num_heads)\n",
    "    return project_output(A)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads = 12, hidden_size = 768):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.O = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_pattern = raw_attention_pattern(input, self.num_heads, self.Q, self.K)\n",
    "        return bert_attention(input, self.num_heads, attention_pattern, self.V, self.O)\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(\n",
    "    token_activations: t.Tensor,\n",
    "    linear_1: nn.Module,\n",
    "    linear_2: nn.Module\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        token_activations: torch.Tensor[batch_size,seq_length,768],\n",
    "        linear_1: nn.Module,\n",
    "        linear_2: nn.Module\n",
    "    output:\n",
    "        torch.Tensor[batch_size, seq_length, 768]\n",
    "    \"\"\"\n",
    "    return linear_2(nn.functional.gelu(linear_1(token_activations)))\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size:int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.linear1 = nn.Linear(self.input_size, self.intermediate_size)\n",
    "        self.linear2 = nn.Linear(self.intermediate_size, self.input_size)\n",
    "    \n",
    "    def forward(self, activations):\n",
    "        return bert_mlp(activations, self.linear1, self.linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super().__init__()\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.weight = nn.Parameter(t.ones([self.normalized_dim]))\n",
    "        self.bias = nn.Parameter(t.zeros([self.normalized_dim]))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mean = input.mean(dim=-1, keepdim=True).detach()\n",
    "        var = input.var(dim=-1, keepdim=True, unbiased=False).detach()\n",
    "        normed = (input-mean) / t.sqrt(var + self.eps)\n",
    "        return normed*self.weight + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size:int, intermediate_size:int, num_heads:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = MultiHeadedSelfAttention(self.num_heads, self.hidden_size)\n",
    "        self.ln1 = LayerNorm(self.hidden_size)\n",
    "        self.mlp = BertMLP(self.hidden_size, self.intermediate_size)\n",
    "        self.ln2 = LayerNorm(self.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.ln1(self.dropout1(self.attention(input)) + input)\n",
    "        return self.ln2(self.dropout2(self.mlp(x)) + x)\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.weight = nn.Parameter(t.randn((self.vocab_size, self.embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "    input_ids, \n",
    "    token_type_ids, \n",
    "    position_embedding: Embedding,\n",
    "    token_embedding: Embedding, \n",
    "    token_type_embedding: Embedding, \n",
    "    layer_norm: LayerNorm, \n",
    "    dropout: nn.Dropout\n",
    "):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        input_ids: [batch, seqlen], \n",
    "        token_type_ids: [batch, seqlen], \n",
    "        position_embedding: Embedding,\n",
    "        token_embedding: Embedding, \n",
    "        token_type_embedding: Embedding, \n",
    "        layer_norm: LayerNorm, \n",
    "        dropout: nn.Dropout)\n",
    "    returns:\n",
    "\n",
    "    \"\"\"\n",
    "    pos_ids = repeat(t.arange(input_ids.shape[1]), 's -> b s', b = input_ids.shape[0]).to(input_ids.device) \n",
    "    x = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(pos_ids)\n",
    "    return dropout(layer_norm(x))\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 2.07e-10 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size:int, hidden_size:int, max_position_embeddings:int, type_vocab_size:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.token_embedding = Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.position_embedding = Embedding(self.max_position_embeddings, self.hidden_size)\n",
    "        self.token_type_embedding = Embedding(self.type_vocab_size, self.hidden_size)\n",
    "        self.layer_norm = LayerNorm(self.hidden_size) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids,\n",
    "            self.position_embedding, self.token_embedding,\n",
    "            self.token_type_embedding, self.layer_norm, self.dropout) \n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "        max_position_embeddings: int, type_vocab_size: int, \n",
    "        dropout: float, intermediate_size: int, num_heads: int, \n",
    "        num_layers: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for i in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.Sequential(OrderedDict([\n",
    "            [\"mlp\", nn.Linear(hidden_size, hidden_size)],\n",
    "            [\"gelu\", nn.GELU()],\n",
    "            [\"layer_norm\" , LayerNorm(hidden_size)],\n",
    "            [\"unembedding\", nn.Linear(hidden_size, vocab_size)],\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.lm_head(self.transformer(self.embedding(input_ids, token_type_ids)))\n",
    "    \n",
    "\n",
    "bert_tests.test_bert(Bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Extra Keys  ----\n",
      "['classification_head', 'bias']\n",
      "['classification_head', 'weight']\n",
      "----  Missing Keys  ----\n"
     ]
    }
   ],
   "source": [
    "def key_map(old_key):\n",
    "    key_segments = old_key.split(\".\")\n",
    "    if key_segments[0] == \"transformer\":\n",
    "        if key_segments[2] == 'layer_norm':\n",
    "            key_segments[2] = 'ln1'\n",
    "        elif key_segments[2] == 'attention':\n",
    "            if key_segments[3] == 'pattern':\n",
    "                del key_segments[3]\n",
    "            key_segments[3] = {\n",
    "                \"project_query\": \"Q\",\n",
    "                \"project_key\": \"K\",\n",
    "                \"project_value\": \"V\",\n",
    "                \"project_out\": \"O\",\n",
    "            }[key_segments[3]]\n",
    "        elif key_segments[2] == 'residual':\n",
    "            if key_segments[3] == 'mlp1':\n",
    "                key_segments[2] = 'mlp'\n",
    "                key_segments[3] = 'linear1'\n",
    "            elif key_segments[3] == 'mlp2':\n",
    "                key_segments[2] = 'mlp'\n",
    "                key_segments[3] = 'linear2'\n",
    "            elif key_segments[3] == 'layer_norm':\n",
    "                del key_segments[2]\n",
    "                key_segments[2] = 'ln2'\n",
    "\n",
    "    return \".\".join(key_segments)\n",
    "\n",
    "def skip_key(old_key: str):\n",
    "    return old_key.startswith(\"classification_head\")\n",
    "\n",
    "remapped_keys = set([key_map(k) for k in pretrained_bert.state_dict().keys()])\n",
    "target_keys = my_bert.state_dict().keys()\n",
    "\n",
    "def split_and_filter(k):\n",
    "    segs = k.split(\".\")\n",
    "    # if segs[0] == \"transformer\" and segs[1] == '0':\n",
    "    print(segs)\n",
    "\n",
    "print(\"----  Extra Keys  ----\")\n",
    "\n",
    "for k in remapped_keys - target_keys:\n",
    "    split_and_filter(k)\n",
    "\n",
    "print(\"----  Missing Keys  ----\")\n",
    "\n",
    "for k in target_keys - remapped_keys:\n",
    "    split_and_filter(k)\n",
    "\n",
    "# for k in my_bert.state_dict().keys():\n",
    "#     key_segments = k.split(\".\")\n",
    "#     if key_segments[0] == \"transformer\":\n",
    "#         if key_segments[1] == '0':\n",
    "#             print(key_segments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(pretrained, ours, strict = True):\n",
    "    ours.load_state_dict({\n",
    "        key_map(k): v \n",
    "        for k, v in pretrained.state_dict().items()\n",
    "        if not skip_key(k)\n",
    "    }, strict = strict)\n",
    "\n",
    "transfer_weights(pretrained_bert, my_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def ascii_art_probs(inp, k = 5):\n",
    "    input_list = tokenizer(inp)['input_ids']\n",
    "    mask_id = input_list.index(103)\n",
    "    input_ids = t.tensor([input_list])\n",
    "    out = my_bert(input_ids)\n",
    "    soft = t.softmax(out[0,mask_id,:], dim = 0)\n",
    "    vals, inds = t.topk(soft, k, dim = 0)\n",
    "    print(inp.replace('[MASK]', '___'))\n",
    "    mask_location = inp.index('[MASK]')\n",
    "    preds = [f\"{vals[i]:.0%}\".rjust(mask_location-1) + ' ' + tokenizer.decode(inds[i]) for i in range(k)]\n",
    "    print('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 + 12 = ___.\n",
      "      10% 0\n",
      "      10% 1\n",
      "       6% 2\n",
      "       4% 3\n",
      "       4% 4\n"
     ]
    }
   ],
   "source": [
    "ascii_art_probs(\"58 + 12 = [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(Bert):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        trans_out = self.transformer(self.embedding(input_ids, token_type_ids))\n",
    "        return self.class_head(trans_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_set, batch_size=8, max_seq_length=512):\n",
    "    def process_element(el):\n",
    "        sentiment, review = el\n",
    "        numerical_sent = 1 if sentiment==\"pos\" else 0\n",
    "        tokens = tokenizer(review)['input_ids']\n",
    "        tokens = tokens[:max_seq_length] + [102] if len(tokens) > max_seq_length-1 else tokens\n",
    "        return numerical_sent, tokenizer(review)['input_ids']\n",
    "\n",
    "    data = [process_element(el) for el in data_set]\n",
    "    random.shuffle(data)\n",
    "    data.sort(key=lambda tup:len(tup[1]), reverse=True)\n",
    "\n",
    "    def pad(tokens, pad_length):\n",
    "        return tokens + [0]*(pad_length - len(tokens)) \n",
    "\n",
    "    batches = []\n",
    "    for b in range((len(data)+batch_size-1) // batch_size):\n",
    "        batch_data = data[b*batch_size:b*batch_size+batch_size]\n",
    "        longest = len(batch_data[0][1])\n",
    "        token_tensor = t.tensor([pad(tokens, longest) for _, tokens in batch_data])\n",
    "        sent_tensor = t.tensor([sent for sent, _ in batch_data])\n",
    "        batches.append((token_tensor, sent_tensor))\n",
    "    \n",
    "    random.shuffle(batches)\n",
    "    return batches\n",
    "\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "batched_train = process_data(data_train)\n",
    "batched_test = process_data(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_bert_classifier = BertClassifier(num_classes = 2, vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12)\n",
    "transfer_weights(pretrained_bert, our_bert_classifier, strict = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
