{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%run w2d1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional, List\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19082, 1184, 112, 188, 1146, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 7592, 2054, 1005, 1055, 2039, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"[CLS] hello what's up [SEP]\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(\"hello what's up\"))\n",
    "uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(uncased_tokenizer([\"hello what's up\"]))\n",
    "coded = uncased_tokenizer([\"hello what's up\"])\n",
    "uncased_tokenizer.batch_decode(coded['input_ids'])\n",
    "tokenizer.batch_decode(coded['input_ids'])\n",
    "uncased_tokenizer.batch_decode(coded['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert, pretrained_bert = load_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1109, 3489, 7871, 1106, 3940, 103, 119, 102]\n",
      "[7]\n",
      "torch.Size([28996])\n",
      "tensor([ 119,  132,  106, 1232,  136,  117,  131,  107,  197,  112,  118,  635,\n",
      "         114, 1116,  100, 1105,  113, 1272, 1183, 1112])\n",
      ". ;!...?, : \" |'- ред )s [UNK] and ( becausey as\n"
     ]
    }
   ],
   "source": [
    "def feed_bert(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    print(input_ids)\n",
    "    mask_idxs = [idx for idx, token in enumerate(input_ids) if token == 103]\n",
    "\n",
    "    print(mask_idxs)\n",
    "\n",
    "    all_logits = model(t.tensor([input_ids], dtype=t.long))[0]\n",
    "\n",
    "    for mask_idx in mask_idxs:\n",
    "        logits = all_logits[mask_idx]\n",
    "        print(logits.shape)\n",
    "        probs = t.softmax(logits, dim=0)\n",
    "\n",
    "        top_logit_idxs = t.argsort(logits, descending=True)[:top_k]\n",
    "        print(top_logit_idxs)\n",
    "        \n",
    "        top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "        print(top_logit_words)\n",
    "\n",
    "\n",
    "feed_bert(my_bert, \"The fish loves to eat [MASK].\", tokenizer, top_k=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
