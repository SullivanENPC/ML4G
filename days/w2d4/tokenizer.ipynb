{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tokenizer as tok_tests\n",
    "from collections import Counter\n",
    "\n",
    "def to_tokens(s):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", s)\n",
    "\n",
    "def corpus_common_tokens(strings_list):\n",
    "    c = Counter()\n",
    "    for s in strings_list:\n",
    "        c.update(to_tokens(s))\n",
    "    return [i[0] for i in c.most_common(30000)]\n",
    "\n",
    "tok_tests.test_tokenizer_from_corpus_fn(corpus_common_tokens)\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, token_list):\n",
    "        self.token_list = token_list\n",
    "        # print(token_list)\n",
    "        self.vocab = {i[\"piece\"]: i[\"id\"] for i in token_list}\n",
    "        self.vocab_by_id = {i[\"id\"]: i[\"piece\"] for i in token_list}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ' '.join(self.vocab_by_id[i] for i in ids)\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "        return [self.vocab.get(i, 3) for i in to_tokens(s)]\n",
    "\n",
    "\n",
    "#tok_tests.test_tokenizer(Tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[882, 242, 80, 132, 343, 145, 113, 888, 2, 168, 76, 811, 113, 113, 106, 118, 12, 252, 12, 146, 811, 74, 908, 526, 58, 162, 39, 237, 53, 59, 1018, 573, 276, 78, 59, 422, 116, 180, 40, 86, 387, 6, 49]\n",
      "hello, my name is Macbeth. This is a sentence. The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, token_list):\n",
    "        super().__init__(token_list)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        stack = []\n",
    "        for curr_token in list(text):\n",
    "            if len(stack) == 0:\n",
    "                stack.append(curr_token)\n",
    "            else:\n",
    "                prev_token = stack[-1]\n",
    "                combined_token = prev_token + curr_token\n",
    "                if combined_token in self.vocab:\n",
    "                    stack.pop()\n",
    "                    stack.append(combined_token)\n",
    "                else:\n",
    "                    stack.append(curr_token)\n",
    "        return [self.vocab[token] for token in stack]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return ''.join(self.vocab_by_id[i] for i in ids)\n",
    "                \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, merges=1000):\n",
    "        tokens = set()\n",
    "        for word in corpus:\n",
    "            tokens.update(word) # adds all chars to `tokens`\n",
    "\n",
    "        token_mapping = {c: idx for idx, c in enumerate(tokens)}\n",
    "        token_mapping_inv = list(token_mapping)\n",
    "        # transform each word to a list of token IDs\n",
    "        corpus = [[token_mapping[c] for c in word] for word in corpus]\n",
    "\n",
    "        # Finds instances of xy in l and replaces with z\n",
    "        def do_replace(x, y, z, l):\n",
    "            stack = []\n",
    "            for i in l:\n",
    "                if stack and i == y and stack[-1] == x:\n",
    "                    stack.pop()\n",
    "                    stack.append(z)\n",
    "                else:\n",
    "                    stack.append(i)\n",
    "            return stack\n",
    "\n",
    "        for _ in range(merges):\n",
    "            c = Counter()\n",
    "            for i in corpus:\n",
    "                # generate pairs of adjacent tokens\n",
    "                c.update(zip(i, i[1:]))\n",
    "            x = c.most_common(1)[0][0]\n",
    "            p = token_mapping_inv[x[0]] + token_mapping_inv[x[1]]\n",
    "            new_id = len(token_mapping)\n",
    "            token_mapping[p] = new_id\n",
    "            token_mapping_inv.append(p)\n",
    "            corpus = [do_replace(*x, new_id, i) for i in corpus]\n",
    "        tokens = [{\"piece\": k, \"id\": v} for k, v in token_mapping.items()]\n",
    "        return cls(tokens)\n",
    "\n",
    "# tok_tests.test_bpe_tokenizer_from_corpus(BPETokenizer)\n",
    "\n",
    "from pathlib import Path\n",
    "corpus = open(Path.home() / \"mlab/days/w2d4/shakespeare.txt\").readlines()\n",
    "minicorpus = corpus[5000:6000]\n",
    "tokenizer = BPETokenizer.from_corpus(minicorpus)\n",
    "sentence = \"hello, my name is Macbeth. This is a sentence. The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
