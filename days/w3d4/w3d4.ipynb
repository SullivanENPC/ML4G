{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import transformers\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "ref_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_periods(s):\n",
    "    return s.count(\".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/msimontaylor/gpt-rl/1163d9d8b4da434e9cb6aeac7394db91\n",
      "\n",
      "100%|██████████| 100/100 [00:59<00:00,  1.68it/s, reward=1, loss=-2.19, kl loss=0.676]     \n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/msimontaylor/gpt-rl/1163d9d8b4da434e9cb6aeac7394db91\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [110]   : (-4.979653358459473, 0.6365949511528015)\n",
      "COMET INFO:     reward [100] : (0.3125, 3.25)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     git metadata        : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     model graph         : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO:     text-sample         : 100\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "def train(model, ref_model, tokenizer, device=\"cuda:1\", gen_len = 20, batch_size = 5, epochs = 1, lr = 3e-5,\n",
    "          lr_lambda = lambda e: 1):\n",
    "    experiment = Experiment(\n",
    "        api_key=\"OmfvOU0RmHbt4iMa2WIYGBjBf\",\n",
    "        project_name=\"gpt-rl\",\n",
    "        workspace=\"msimontaylor\",\n",
    "    )\n",
    "    \n",
    "    kl = t.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    optim = t.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.to(device=device)\n",
    "    ref_model.to(device=device)\n",
    "    tbar = tqdm.tqdm(range(epochs))\n",
    "    \n",
    "    scheduler = t.optim.lr_scheduler.LambdaLR(optim, lr_lambda = [lr_lambda])\n",
    "    \n",
    "    for _ in tbar:\n",
    "        # prompt = t.zeros((batch_size, gen_len))\n",
    "        # prompt[:,0] = 50256\n",
    "        samples = None\n",
    "        with t.no_grad():\n",
    "            input_ids = t.tensor([[50256]], device=device)\n",
    "            samples = model.generate(\n",
    "                input_ids,\n",
    "                max_length=gen_len,\n",
    "                min_length=gen_len,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_k=len(tokenizer),\n",
    "                top_p=1.0,\n",
    "                num_return_sequences=batch_size\n",
    "            )\n",
    "            # for i in range(1,N):\n",
    "            #     out = t.distributions.Categorical(model(prompt[:i])).sample()\n",
    "            #     prompt[:,i] = out\n",
    "        optim.zero_grad()\n",
    "        rewards = t.tensor([count_periods(tokenizer.decode(p.cpu())) for p in samples], dtype=t.float, device=device)\n",
    "        experiment.log_text(str(rewards[0].cpu())+tokenizer.decode(samples[0].cpu()))\n",
    "        metrics = {\"reward\":rewards.mean().detach().item()}\n",
    "        rewards = rewards - rewards.mean()\n",
    "        rewards = rewards / (rewards.std() + 1e-5)\n",
    "\n",
    "        log_probs = t.log(t.nn.functional.softmax(model(samples).logits, dim=-1) + 1e-37)\n",
    "        ref_probs = t.nn.functional.softmax(ref_model(samples).logits, dim=-1) + 1e-37\n",
    "        # logprobs = t.log(\n",
    "        #     t.nn.functional.softmax(\n",
    "        #         model(samples).logits, dim = -1\n",
    "        #     )[t.arange(batch_size).unsqueeze(1),t.arange(gen_len), samples]\n",
    "        # )\n",
    "        # logprobs *= rewards.unsqueeze(1)\n",
    "        \n",
    "        # loss = -t.mean(logprobs, dim=-1).mean()\n",
    "        kl_loss = kl(log_probs, ref_probs)/10\n",
    "        logprobs = log_probs[t.arange(batch_size).unsqueeze(1),t.arange(gen_len), samples]\n",
    "        logprobs *= rewards.unsqueeze(1)\n",
    "        loss = -t.mean(logprobs, dim=-1).mean()\n",
    "        loss += kl_loss\n",
    "        # loss = kl_loss\n",
    "        \n",
    "        experiment.log_metric(\"loss\", loss.detach().item())\n",
    "        experiment.log_metric(\"reward\", metrics[\"reward\"])\n",
    "        metrics[\"loss\"] = loss.detach().item()\n",
    "        metrics[\"kl loss\"] = kl_loss.detach().item()\n",
    "        tbar.set_postfix(metrics)\n",
    "        loss.backward()\n",
    "        t.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "    experiment.end()\n",
    "\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "def get_scheduler(max_epochs):\n",
    "    def scheduler(epoch):\n",
    "        if epoch < 0.2*max_epochs:\n",
    "            return epoch/(0.2*max_epochs)\n",
    "        return (max_epochs-epoch)/(0.8*max_epochs)\n",
    "    return scheduler\n",
    "train(model, ref_model, tokenizer, batch_size=32, epochs=100, lr_lambda=get_scheduler(100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTwithMLP(t.nn.Module):\n",
    "    def __init__(self, gpt):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt\n",
    "        self.value_layer = t.nn.Sequential(\n",
    "            t.nn.Linear(768, 1024),\n",
    "            t.nn.ReLU(),\n",
    "            # t.nn.Linear(1024, 1024),\n",
    "            # t.nn.ReLU(),\n",
    "            t.nn.Linear(1024, 1),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        x = None\n",
    "        with t.no_grad():\n",
    "            x = self.gpt.transformer(input).last_hidden_state\n",
    "        return self.value_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpmodel = GPTwithMLP(model)\n",
    "gpmodel.to(device='cuda:1')\n",
    "gpmodel(t.tensor([123, 124], device='cuda:1')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_periods_batch(s, tok):\n",
    "    return [tok.decode([c]).count(\".\") for c in s]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/msimontaylor/gpt-rl/8ad7c77df7c940429bac5b02d37fc09f\n",
      "\n",
      "100%|██████████| 100/100 [01:42<00:00,  1.03s/it, loss=-16] \n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/msimontaylor/gpt-rl/8ad7c77df7c940429bac5b02d37fc09f\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [110] : (-17.460430145263672, 1.211767554283142)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     git metadata        : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "def train_jointly(model, tokenizer, device=\"cuda:1\", gen_len = 20, batch_size = 5, epochs = 1, lr = 3e-5, gamma = 0.99):\n",
    "    kl = t.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    ref_model.to(device=device)\n",
    "    # model.to(device)\n",
    "\n",
    "    gpt_model = model\n",
    "    model = GPTwithMLP(model)\n",
    "    experiment = Experiment(\n",
    "        api_key=\"OmfvOU0RmHbt4iMa2WIYGBjBf\",\n",
    "        project_name=\"gpt-rl\",\n",
    "        workspace=\"msimontaylor\",\n",
    "    )\n",
    "    \n",
    "    optim = t.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.to(device=device)\n",
    "    tbar = tqdm.tqdm(range(epochs))\n",
    "        \n",
    "    for _ in tbar:\n",
    "        samples = None\n",
    "        with t.no_grad():\n",
    "            input_ids = t.tensor([[50256]], device=device)\n",
    "            samples = gpt_model.generate(\n",
    "                input_ids,\n",
    "                max_length=gen_len,\n",
    "                min_length=gen_len,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_k=len(tokenizer),\n",
    "                top_p=1.0,\n",
    "                num_return_sequences=batch_size\n",
    "            )\n",
    "            # for i in range(1,N):\n",
    "            #     out = t.distributions.Categorical(model(prompt[:i])).sample()\n",
    "            #     prompt[:,i] = out\n",
    "        optim.zero_grad()\n",
    "        rewards = t.tensor([count_periods(tokenizer.decode(p.cpu())) for p in samples], dtype=t.float, device=device)\n",
    "        rewards = rewards - rewards.mean()\n",
    "        rewards = rewards / (rewards.std() + 1e-5)\n",
    "\n",
    "\n",
    "        # experiment.log_text(str(rewards[0].cpu())+tokenizer.decode(samples[0].cpu()))\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # value loss\n",
    "        values_t = model(samples).squeeze()\n",
    "        values_t_plus_1 = gamma * values_t\n",
    "        \n",
    "        differences = values_t[:, :-1] - values_t_plus_1[:, 1:]\n",
    "        differences[:,-1] -= rewards\n",
    "        \n",
    "        value_loss = t.mean(differences**2)\n",
    "        \n",
    "        \n",
    "        # KL loss\n",
    "        ref_probs = t.nn.functional.softmax(ref_model(samples).logits, dim=-1) + 1e-37\n",
    "        log_probs = t.log(t.nn.functional.softmax(gpt_model(samples).logits, dim=-1) + 1e-37)\n",
    "\n",
    "        kl_loss = kl(log_probs, ref_probs)/10\n",
    "\n",
    "\n",
    "        \n",
    "        # policy loss\n",
    "        logprobs = log_probs[t.arange(batch_size).unsqueeze(1),t.arange(gen_len), samples]\n",
    "        logprobs *= rewards.unsqueeze(1)\n",
    "        policy_loss = -t.mean(logprobs, dim=-1).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # backprop\n",
    "        loss = 0.3*value_loss + policy_loss + 0.1*kl_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        experiment.log_metric(\"loss\", loss.detach().item())\n",
    "        metrics[\"loss\"] = loss.detach().item()\n",
    "        # metrics[\"kl loss\"] = kl_loss.detach().item()\n",
    "        tbar.set_postfix(metrics)\n",
    "        t.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optim.step()\n",
    "        # scheduler.step()\n",
    "    experiment.end()\n",
    "    return model\n",
    "\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "value_model = train_jointly(model, tokenizer, batch_size=128, epochs=100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The boy 《A referee, meticulists, meticulists, meticula, meticoters, meticula, meticoters, meticula, meticoth, meticore, meticrees, meticled, metic, metic; SI, metic, metic; SI, metic, metic; SI, metic, metic; SI, metic, metic; SI, metic, metic; SI, metic, metic; SI, metic',\n",
       " \"The boy 〉's metic. This is assembling, metic. I want to avoid my oldest (VIDEO) and metic. I want to meet the nation's hardest, meticulists, meticulists, meticulists, meticula, meticula, meticoters, meticoth, meticore; I want to meet the nation's highest, metic, metic. I want to abandon the nation's largest, metic,\",\n",
       " \"The boy 《Doyle's brightest, meticolded, meticolded, meticolded, meticolded, meticolded, meticorned, meticolded, meticored, meticored, meticored, meticored, meticored, meticored, meticore, meticored, metic.\\n\\nBy our nation's brightest, meticolded, meticolded, meticored, meticored, meticored, metic.\\n\\nOur nation's largest\",\n",
       " \"The boy ��s metic, metic-affiliated with Auschwitz, metic-affiliated with Auschwitz, metic-authorized to diagnose, metic-authorized to cope with Auschwitz, metic-authorized to avoid the nation's heaviest, metic.\\n\\nThe nation's brightest, metic-worker, metic-worker, metic-worker, metic-worker, metic-worker, metic-worker, metic-worker, metic-commander, metic-worker\",\n",
       " \"The boy 《New Jersey's newest, meticolded, meticolded, meticolded, meticolded, meticolded, meticolded, meticored, meticored, meticored, meticored, meticored, meticored, meticored, meticored, metic. I want to make a plethora of mosquited, meticured, meticored, meticored, metic. I want to hear from the nation, meticured, meticored\",\n",
       " 'The boy ____, meticulists, meticulists, meticulists, meticulists, meticulists, meticoth, meticoth, meticore, meticoth, meticore, metic. The nation, meticore, metic. I want to hear from the nation, metic. I want to hear from the nation, metic. I want to hear from the nation, metic. I want to hear from the nation,',\n",
       " 'The boy ��s metic and metic-den of mosquiastic systems, metic-den of mosquiastic systems, metic-den of mosquiastics, metic-den of chimpanzees, metic-den of TED, metic-den of methane, metic-den of methane, metic-den of methane, metic-den of methane, metic-den of methane, metic-den of methane, metic-LD, metic-DEF',\n",
       " \"The boy 《Chad Rhee's tallest and largest chemical, meticulists, meticulists, meticulists, meticulists, meticulists, meticulation, meticula, meticoth, meticourage, meticore, meticour, meticour, meticour, meticour, meticour, meticour, metic.\\n\\nThe nation's hardest, metic. I want to hear from the nation, metic\",\n",
       " 'The boy ȝa mathematized with Glouces, meticolded, meticolded, meticolded, meticolded, meticolded, meticored, meticored, meticored, meticored, meticored, meticored, meticored, meticored, meticored, meticored, meticore, meticored, meticored, metic.\\n\\nMy grandfather, meticured, meticured, meticored, meticored, metic.\\n',\n",
       " \"The boy 《A referee, meticlasts, meticlasts, meticlasts, meticoters, meticoters, meticledity, meticledity, meticoring, metic. I want to make a metic, metic. I want to meet my mandate, metic. I want to engage in the nation's oldest, metic. I want to learn, metic, metic. I want to be a lawyer, metic\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = t.tensor([tokenizer.encode(\"The boy \")], device=\"cuda:1\")\n",
    "tokens = value_model.gpt.generate(\n",
    "        encoded,\n",
    "        max_length=88,\n",
    "        min_length=80,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_k=len(tokenizer),\n",
    "        top_p=1.0,\n",
    "        num_return_sequences=10\n",
    "    )\n",
    "[tokenizer.decode(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's ersying to be a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the\",\n",
       " \"It's iced a country, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is\",\n",
       " \"It's ersarding country, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is\",\n",
       " \"It's ersarding country, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is\",\n",
       " \"It's ersying to be a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the\",\n",
       " \"It's icky the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths\",\n",
       " \"It's ersarding country, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is\",\n",
       " \"It's ersive of the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating\",\n",
       " \"It's ersarding country, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is\",\n",
       " \"It's ersying to be a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the United States is investigating the deaths of a businessman, the\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 0.06673453748226166),\n",
       " (\"'s\", 0.15204890072345734),\n",
       " (' been', 0.17932401597499847),\n",
       " (' a', 0.19902653992176056),\n",
       " (' challenging', 0.18835552036762238),\n",
       " (' season', 0.1614348292350769),\n",
       " (' for', 0.1652309000492096),\n",
       " (' the', 0.17407137155532837),\n",
       " (' Patriots', 0.12894541025161743),\n",
       " ('.', 0.18230517208576202),\n",
       " (' The', 0.15000677108764648),\n",
       " (' team', 0.13914750516414642),\n",
       " (\"'s\", 0.15407651662826538),\n",
       " (' defense', 0.1796053647994995),\n",
       " (' is', 0.18408824503421783),\n",
       " (' about', 0.1624753624200821),\n",
       " (' to', 0.19539035856723785),\n",
       " (' be', 0.2309398353099823),\n",
       " (' crushed', 0.15015915036201477),\n",
       " ('.', 0.15930448472499847)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"It's been a challenging season for the Patriots. The team's defense is about to be crushed.\")\n",
    "[( tokenizer.decode(x[0]), x[1].item()) for x in zip(encoded, value_model(t.tensor([encoded], device='cuda:1'))[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_periods_batch(encoded, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
