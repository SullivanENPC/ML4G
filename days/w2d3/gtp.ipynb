{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTP-2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import gpt_tests\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,hidden_size: int, num_heads:int):\n",
    "        super().__init__()\n",
    "        self.attn_lin=nn.Linear(hidden_size,3*hidden_size) #Q,K,V\n",
    "        self.out_lin= nn.Linear(hidden_size,hidden_size) #O\n",
    "        self.head_size= hidden_size//num_heads\n",
    "        self.num_heads=num_heads\n",
    "        self.hidden_size=hidden_size\n",
    "    def forward(self, x:t.Tensor, past_key_values=None,return_key_values=False ):  #num_heads,seq_len,\n",
    "        attention = self.attn_lin(x) \n",
    "        Q= einops.rearrange(attention[:,:,:self.hidden_size],\"b n (nh hs)-> b nh n hs\",nh = self.num_heads,hs = self.head_size)\n",
    "        K= einops.rearrange(attention[:,:,self.hidden_size:2*self.hidden_size],\"b n (nh hs)-> b nh n hs\",nh = self.num_heads,hs = self.head_size)\n",
    "        V= einops.rearrange(attention[:,:,-self.hidden_size:],\"b n (nh hs)-> b nh n hs\",nh = self.num_heads,hs = self.head_size)\n",
    "        if past_key_values is not None:\n",
    "            Kpast=past_key_values[:,:,:self.head_size].unsqueeze(0)\n",
    "            Vpast=past_key_values[:,:,self.head_size:].unsqueeze(0)\n",
    "            K=t.concat((Kpast,K),dim=2)\n",
    "            V=t.concat((Vpast,V),dim=2)\n",
    "        raw_attn = t.einsum(\"bhni,bhmi -> bhnm\",Q,K)/((self.head_size)**.5)#n : seer m: seeee \n",
    "        seq_length=raw_attn.shape[-1]\n",
    "        if past_key_values is None:\n",
    "            to_mask=t.triu(t.ones(seq_length,seq_length),diagonal=1).to(raw_attn.device).bool()\n",
    "            raw_attn=raw_attn.masked_fill_(to_mask,-1e4)\n",
    "        attn = t.softmax(raw_attn,dim=3)\n",
    "        to_output = einops.rearrange(t.einsum(\"bhnm,bhmi->bhni\",attn,V),\"b nh n hi -> b n (nh hi)\")\n",
    "        output = self.out_lin(to_output)\n",
    "        if return_key_values:\n",
    "            if past_key_values is not None:\n",
    "                key_values = t.concat((K[:,:,-1,:],V[:,:,-1,:]),dim=2).unsqueeze(2)\n",
    "            else:\n",
    "                key_values = t.concat((K,V),dim=3)\n",
    "            return output, key_values\n",
    "        else: \n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_unidirectional_attn(GPT2MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float, layer_norm_epsilon: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = GPT2MultiHeadAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size*4)\n",
    "        self.GELU = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x,past_key_values=None,return_key_values=False):\n",
    "        if return_key_values:\n",
    "            res,key_values = self.attn(self.ln1(x),past_key_values,True)\n",
    "            res=x+res\n",
    "        else:\n",
    "            res =x+self.attn(self.ln1(x),past_key_values)\n",
    "        x = self.ln2(res)\n",
    "        x = self.linear1(x)\n",
    "        x = self.GELU(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        if return_key_values:\n",
    "            return res + x,key_values\n",
    "        else:\n",
    "            return res+x\n",
    "\n",
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        dropout: float,\n",
    "        layer_norm_epsilon: int,\n",
    "        use_cache=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon) for _ in range(num_layers)])\n",
    "        self.head_size = hidden_size//num_heads\n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.use_cache=use_cache\n",
    "        self.num_layers=num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size=vocab_size\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        if use_cache:\n",
    "            self.key_values=None\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        if self.use_cache and self.key_values is not None:\n",
    "            tok_embed = self.token_embedding[input_ids[0,-1]]\n",
    "            pos_embed = self.pos_embedding[input_ids.shape[1]-1]\n",
    "        else:\n",
    "            tok_embed = self.token_embedding[input_ids] # [batch_size, seq_len, hidden_size]\n",
    "            pos_embed = self.pos_embedding[t.arange(input_ids.shape[1], device=input_ids.device)] # [seq_len, hidden_size]\n",
    "        x = self.dropout(tok_embed + pos_embed)\n",
    "        if self.use_cache:\n",
    "           \n",
    "            if self.key_values is not None:\n",
    "                x=x.unsqueeze(0).unsqueeze(0)\n",
    "                new_key_values = t.zeros(self.num_layers,self.num_heads,1, 2*self.head_size).to(x.device)\n",
    "            else:\n",
    "                new_key_values = t.zeros(self.num_layers,self.num_heads,input_ids.shape[1], 2*self.head_size).to(x.device)\n",
    "            for i,layer in enumerate(self.blocks):\n",
    "                if self.key_values is None:\n",
    "                    x, layer_key_values=layer(x,None,True)\n",
    "                else:\n",
    "                    x, layer_key_values=layer(x,self.key_values[i],True)\n",
    "                new_key_values[i]=layer_key_values\n",
    "            if self.key_values is None:\n",
    "                self.key_values=new_key_values\n",
    "            else:\n",
    "                self.key_values=t.concat((self.key_values,new_key_values),dim=2)\n",
    "            x=self.ln(x)\n",
    "        else:\n",
    "            x = self.ln(self.blocks(x))\n",
    "        final_encoding = x[:, -1, :]\n",
    "        logits = final_encoding @ self.token_embedding.T\n",
    "        return GPT2Output(final_encoding=final_encoding, logits=logits)\n",
    "\n",
    "    def next_token(self,input_ids,temperature,freq_penalty=2.0):\n",
    "        output = self.forward(input_ids)\n",
    "        freq_counts = t.bincount(input_ids[0], minlength=self.vocab_size).to(output.logits.device)\n",
    "        adj_logits = output.logits / temperature - freq_counts * freq_penalty\n",
    "        probs = adj_logits.softmax(dim=1)\n",
    "        return t.multinomial(probs, 1).item()\n",
    "\n",
    "    def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
    "        self.eval()\n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "        self.key_values = None\n",
    "        device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "        self.to(device)\n",
    "        while ( len(input_ids) <= max_length and\n",
    "                not input_ids[-1] == self.tokenizer.eos_token_id ):\n",
    "            input_ids_tensor = t.tensor(input_ids).unsqueeze(0).to(device)\n",
    "            input_ids.append(self.next_token(input_ids_tensor, temperature, freq_penalty))\n",
    "        return self.tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257,\n",
    "    hidden_size=768, max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, use_cache=True)\n",
    "\n",
    "state_dict = pretrained_gpt.state_dict()\n",
    "state_dict[\"token_embedding\"] = state_dict[\"token_embedding.weight\"]\n",
    "state_dict[\"pos_embedding\"] = state_dict[\"pos_embedding.weight\"]\n",
    "del state_dict[\"token_embedding.weight\"]\n",
    "del state_dict[\"pos_embedding.weight\"]\n",
    "\n",
    "my_gpt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After a long day at work, Franck was started and repressed in his left toe by the fifth judge manner of taking part. It seemed surprisingly mid-week for full-time racism to further aggravate irritation at this type of treatment. So having said that it has been almost 2 months since the verdictswere read (one month already due in December), we'll briefly see how our cohort will fare when Illygard Hill surveys strike stalemate before settling across within S1 celebrations division's parameters\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.generate(\"After a long day at work\", max_length = 100,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1, 32])\n",
      "new sentence\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 3.425s to generate a 500-token sentence without cache and 0.718s with cache.\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt_cache(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
