{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.8/site-packages\")\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.9/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import rl_tests\n",
    "import random\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import Video\n",
    "from video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 14.0\n",
      "num steps taken: 14\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "record = True\n",
    "video_name = \"test_0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "states = 0\n",
    "# Ensure that VideoRecorder is initialized outside\n",
    "video_recorder = VideoRecorder(env,\n",
    "    f\"videos/{video_name}.mp4\"\n",
    ")    \n",
    "while not done:\n",
    "    states += 1\n",
    "    if record:\n",
    "        video_recorder.capture_frame()\n",
    "    else:  \n",
    "        show_state(env)\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # Take a random action\n",
    "    # print(f\"{reward=}\")\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"total reward: {total_reward}\")\n",
    "print(f\"num steps taken: {states}\")\n",
    "if record:\n",
    "    video_recorder.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym.logger.set_level(gym.logger.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/test_0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a saved video \n",
    "Video(\"videos/test_0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_q_net MATCH!!!!!!!!\n",
      " SHAPE (16, 63) MEAN: -0.007192 STD: 0.08323 VALS [0.0643 0.1858 0.06097 0.06114 -0.0363 -0.01642 -0.04007 0.01136 -0.004181 -0.02974...]\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size: int, hidden_size: int, out_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size),\n",
    "        )\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Possibly in a batched manner:\n",
    "            Takes an observation vector (e.g. flattened image),\n",
    "            outputs a vector of rewards for each action\n",
    "        \"\"\"\n",
    "        return self.mlp(obs)\n",
    "    \n",
    "rl_tests.test_q_net(QNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_choice(env: gym.Env, eps: float, net: nn.Module, \n",
    "                obs: torch.Tensor, device: str\n",
    "            ) -> int:\n",
    "    batch_size = obs.shape[0]\n",
    "    # rewards_for_each_action shape: batch_size, num_actions\n",
    "\n",
    "    rewards_for_each_action = net(obs)\n",
    "    actions = torch.where(torch.rand(batch_size) < eps, \n",
    "                    # TODO: Make this batches\n",
    "                    torch.tensor([env.action_space.sample() for _ in range(batch_size)], dtype=torch.long), \n",
    "                    torch.argmax(rewards_for_each_action, dim=-1))\n",
    "    # print(f\"{obs=}\")\n",
    "    # print(f\"{actions=}\")\n",
    "    return actions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, env: gym.Env, eps: float = 0.05, video_name = \"test_1\") -> float:\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    states = 0\n",
    "    # Ensure that VideoRecorder is initialized outside\n",
    "    video_recorder = VideoRecorder(env,\n",
    "        f\"videos/{video_name}.mp4\"\n",
    "    )    \n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            states += 1\n",
    "            if record:\n",
    "                video_recorder.capture_frame()\n",
    "            else:  \n",
    "                show_state(env)\n",
    "            # Batch the observation\n",
    "            obs = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            state, reward, done, _ = env.step(make_choice(env, eps, \n",
    "                                                          model, obs, 'cuda:0').item())\n",
    "            # print(f\"{reward=}\")\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"total reward: {total_reward}\")\n",
    "        print(f\"num steps taken: {states}\")\n",
    "        if record:\n",
    "            video_recorder.close()\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -2.4                    2.4\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.209 rad (-12 deg)    0.209 rad (12 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_pole_geom',\n",
       " 'action_space',\n",
       " 'axle',\n",
       " 'carttrans',\n",
       " 'close',\n",
       " 'force_mag',\n",
       " 'gravity',\n",
       " 'kinematics_integrator',\n",
       " 'length',\n",
       " 'masscart',\n",
       " 'masspole',\n",
       " 'metadata',\n",
       " 'np_random',\n",
       " 'observation_space',\n",
       " 'polemass_length',\n",
       " 'poletrans',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_range',\n",
       " 'seed',\n",
       " 'spec',\n",
       " 'state',\n",
       " 'step',\n",
       " 'steps_beyond_done',\n",
       " 'tau',\n",
       " 'theta_threshold_radians',\n",
       " 'total_mass',\n",
       " 'track',\n",
       " 'unwrapped',\n",
       " 'viewer',\n",
       " 'x_threshold']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward: 10.0\n",
      "num steps taken: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▍                                                                               | 105/2000 [00:03<00:57, 33.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 reward: 10.0\n",
      "num steps taken: 990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▍                                                                           | 202/2000 [00:08<01:30, 19.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 200 reward: 9.0\n",
      "num steps taken: 1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▋                                                                       | 303/2000 [00:13<01:31, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 300 reward: 9.0\n",
      "num steps taken: 2927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████                                                                   | 406/2000 [00:18<00:58, 27.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 400 reward: 9.0\n",
      "num steps taken: 4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▏                                                              | 504/2000 [00:22<01:02, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 500 reward: 17.0\n",
      "num steps taken: 5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▏                                                          | 601/2000 [00:42<05:53,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 600 reward: 37.0\n",
      "num steps taken: 9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▍                                                       | 678/2000 [01:29<02:53,  7.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# FIXME: Currently hangs after ~700 episodes. Did it get really good on at infinite horizon task?\n",
    "# Try changing num steps per episode if possible, or try a different (limited horizon) task like MountainCar???\n",
    "#  CartPole-v1 seems to have 475/500 max episode length: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "num_observations = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "qnetwork = QNetwork(num_observations, 64, num_actions)\n",
    "\n",
    "\n",
    "def train(q: nn.Module, env: gym.Env, episodes: int = 2000, eps: float = 0.05, max_buffer_size=10000, discount=0.98,\n",
    "          train_freq=16, batch_size=128, lr=1e-3, steps=20000, video_name=\"train_0\") -> float:\n",
    "    # (observation, action, reward, done)\n",
    "    # We can index into next_observation: if current observation is at index i then the next observation is at index i+1\n",
    "    buffer = deque()\n",
    "\n",
    "    num_steps_taken = 0\n",
    "    check_for_training = 10000\n",
    "\n",
    "    frozen_q = copy.deepcopy(q)\n",
    "    frozen_q.eval()\n",
    "    q.train()\n",
    "    optimizer = torch.optim.Adam(q.parameters(), lr=lr)\n",
    "\n",
    "    # # Ensure that VideoRecorder is initialized outside\n",
    "    # video_recorder = VideoRecorder(env,\n",
    "    #                                f\"videos/{video_name}.mp4\"\n",
    "    #                                )\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            num_steps_taken += 1\n",
    "\n",
    "            # Commenting out these to speed up execution\n",
    "            # if record:\n",
    "            #     video_recorder.capture_frame()\n",
    "            # else:\n",
    "            #     show_state(env)\n",
    "\n",
    "            # Batch the observation\n",
    "            cur_state = torch.tensor(cur_state, dtype=torch.float).unsqueeze(0)  # This shouldn't throw an error, cur_state shouldn't have been tensor earlier\n",
    "            action = make_choice(env, eps, q, cur_state, 'cuda:0').item()\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # If buffer is full, take oldest time out\n",
    "            if len(buffer) == max_buffer_size:\n",
    "                buffer.popleft()\n",
    "            buffer.append((cur_state, action, reward, done))\n",
    "\n",
    "            cur_state = new_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if num_steps_taken % train_freq == 0 and num_steps_taken > batch_size:\n",
    "                # train_freq += 1\n",
    "\n",
    "                # Get sample indices\n",
    "                # Doing a len(buffer) - 1 so there's always a next sample\n",
    "                buffer_size = len(buffer)\n",
    "                buffer_indices = random.sample(range(buffer_size - 1), batch_size)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "                for index in buffer_indices:\n",
    "                    # FIXME: cur_state, new_state (maybe other variables) overwrites outer scope variables!\n",
    "                    #  Change the variable names here\n",
    "\n",
    "                    state0, action0, reward0, done0 = buffer[index]\n",
    "                    state1, _, _, _ = buffer[index + 1]\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # If we have no next state, then we can't do a max over the Q function\n",
    "                        # So we resort to taking the reward\n",
    "                        if done0:\n",
    "                            target = reward0\n",
    "                        else:\n",
    "                            next_return = torch.max(frozen_q(state1))\n",
    "                            # print(f\"{next_return=}\")\n",
    "                            target = reward0 + discount * next_return.item()\n",
    "\n",
    "                    # print(f\"{q(state0)=}\")\n",
    "                    # Get first batch of action rewards\n",
    "                    # FIXME: `predicted = q(state0)[0][action0]` returns an index out of bounds error sometimes :(\n",
    "                    #  probably due to the variable name repeats/confusion above\n",
    "                    predicted = q(state0).squeeze()[action0]\n",
    "                    # print(f\"{target=}\")\n",
    "                    # print(f\"{predicted=}\")\n",
    "                    loss += (target - predicted) ** 2\n",
    "\n",
    "                loss /= batch_size\n",
    "                # loss = torch.autograd.Variable(loss, requires_grad = True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update frozen Q network\n",
    "                frozen_q.load_state_dict(q.state_dict())\n",
    "                # print(f\"{q.mlp[0].weight[0,0].item()=}\")\n",
    "                # print(f\"{frozen_q.mlp[0].weight[0,0].item()=}\")\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"episode {episode} reward: {episode_reward}\")\n",
    "            print(f\"num steps taken: {num_steps_taken}\")\n",
    "        if num_steps_taken >= steps:\n",
    "            break\n",
    "        if num_steps_taken % check_for_training == 0:\n",
    "            print(\"evaluating agent\")\n",
    "            evaluate(frozen_q, env, eps=0, video_name=f\"training_checkpoint_{num_steps_taken}\")\n",
    "\n",
    "    # if record:\n",
    "    #     video_recorder.close()\n",
    "\n",
    "\n",
    "train(qnetwork, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time qnetwork.load_state_dict(qnetwork.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnetwork.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9384)\n",
      "tensor(1.9111)\n",
      "tensor(3.4584)\n",
      "tensor(4.4062)\n",
      "tensor(5.3788)\n",
      "tensor(6.3297)\n",
      "tensor(7.2892)\n",
      "tensor(8.2220)\n",
      "tensor(9.1800)\n",
      "tensor(10.6812)\n",
      "tensor(11.6546)\n",
      "tensor(12.6058)\n",
      "tensor(13.5720)\n",
      "tensor(14.5386)\n",
      "tensor(15.4715)\n",
      "tensor(16.4167)\n",
      "tensor(17.3674)\n",
      "tensor(18.3295)\n",
      "tensor(19.2849)\n",
      "tensor(20.2167)\n",
      "tensor(21.1857)\n",
      "tensor(22.1452)\n",
      "tensor(23.1172)\n",
      "tensor(24.0810)\n",
      "tensor(25.0512)\n",
      "tensor(26.0041)\n",
      "tensor(26.9380)\n",
      "tensor(28.3819)\n",
      "tensor(29.3523)\n",
      "tensor(30.3256)\n",
      "tensor(31.3010)\n",
      "tensor(32.2751)\n",
      "tensor(33.2458)\n",
      "tensor(34.2062)\n",
      "tensor(35.1375)\n",
      "tensor(36.1042)\n",
      "tensor(37.0766)\n",
      "tensor(38.0101)\n",
      "tensor(38.9422)\n",
      "tensor(39.8990)\n",
      "tensor(40.8381)\n",
      "tensor(41.7828)\n",
      "tensor(42.7556)\n",
      "tensor(43.7280)\n",
      "tensor(44.6630)\n",
      "tensor(45.6255)\n",
      "tensor(46.5844)\n",
      "tensor(47.5563)\n",
      "tensor(48.5146)\n",
      "tensor(49.9904)\n",
      "tensor(50.9570)\n",
      "tensor(51.9220)\n",
      "tensor(52.8846)\n",
      "tensor(54.4310)\n",
      "tensor(55.3993)\n",
      "tensor(56.8978)\n",
      "tensor(57.8704)\n",
      "tensor(58.8045)\n",
      "tensor(59.7439)\n",
      "tensor(61.2917)\n",
      "tensor(62.2496)\n",
      "tensor(63.1893)\n",
      "tensor(64.1497)\n",
      "tensor(65.0963)\n",
      "tensor(66.0427)\n",
      "tensor(67.0118)\n",
      "tensor(67.9619)\n",
      "tensor(68.9034)\n",
      "tensor(69.8586)\n",
      "tensor(71.3479)\n",
      "tensor(72.3084)\n",
      "tensor(73.2694)\n",
      "tensor(74.2328)\n",
      "tensor(75.1967)\n",
      "tensor(76.1683)\n",
      "tensor(77.1409)\n",
      "tensor(78.0863)\n",
      "tensor(79.0190)\n",
      "tensor(79.9895)\n",
      "tensor(80.9592)\n",
      "tensor(81.8973)\n",
      "tensor(82.8698)\n",
      "tensor(83.8211)\n",
      "tensor(84.7957)\n",
      "tensor(85.7589)\n",
      "tensor(86.7346)\n",
      "tensor(87.7048)\n",
      "tensor(88.6640)\n",
      "tensor(89.6372)\n",
      "tensor(90.5717)\n",
      "tensor(91.5097)\n",
      "tensor(92.4767)\n",
      "tensor(93.4362)\n",
      "tensor(94.3952)\n",
      "tensor(95.3546)\n",
      "tensor(96.3261)\n",
      "tensor(97.2981)\n",
      "tensor(98.2675)\n",
      "tensor(99.8061)\n",
      "tensor(101.3114)\n",
      "tensor(102.2540)\n",
      "tensor(103.2102)\n",
      "tensor(104.1616)\n",
      "tensor(105.1299)\n",
      "tensor(106.0607)\n",
      "tensor(107.0177)\n",
      "tensor(107.9896)\n",
      "tensor(108.9305)\n",
      "tensor(109.8940)\n",
      "tensor(110.8524)\n",
      "tensor(111.8259)\n",
      "tensor(112.7716)\n",
      "tensor(114.3132)\n",
      "tensor(115.2494)\n",
      "tensor(116.2049)\n",
      "tensor(117.1802)\n",
      "tensor(118.1122)\n",
      "tensor(119.0858)\n",
      "tensor(120.0443)\n",
      "tensor(121.0139)\n",
      "tensor(121.9510)\n",
      "tensor(123.3194)\n",
      "tensor(124.2769)\n",
      "tensor(125.2345)\n",
      "tensor(126.1932)\n",
      "tensor(127.1644)\n",
      "tensor(128.6614)\n",
      "tensor(129.6340)\n",
      "tensor(1.0128)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-9a38a8a5dbcd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(q, env, episodes, eps, max_buffer_size, discount, train_freq, batch_size, lr, steps, video_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time train(qnetwork, env, episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1145, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnetwork.mlp[0].weight[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train the QNetwork from the buffer\n",
    "buffer_indices  = random.random_indices(num_samples, (0, buf_size-1))\n",
    "obs, action, reward, done = buffer[i]\n",
    "next_obs, _, _, _ = buffer[i+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
