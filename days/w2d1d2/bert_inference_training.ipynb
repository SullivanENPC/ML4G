{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.0006189 STD: 0.1052 VALS [-0.1194 -0.0409 0.2213 0.125 -0.2653 -0.02375 -0.056 0.07237 -0.03704 0.01106...]\n",
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.000583 STD: 0.1108 VALS [0.016 -0.01431 0.1939 -0.007942 0.08188 -0.02789 0.07227 0.07465 0.1954 -0.2553...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n",
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n",
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n",
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n",
      "True\n",
      "False\n",
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [0.2316 0.08455 -0.5146 1.436 2.029 -1.117 2.775 -0.5305 -0.4485 -0.2485...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%run bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert():\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "        num_heads=12, num_layers=12, num_classes=2\n",
    "    )\n",
    "    pretrained_state_dict = pretrained_bert.state_dict()\n",
    "\n",
    "    #pretrained_keys = pretrained_state_dict.keys()\n",
    "    #my_keys = my_bert.state_dict().keys()\n",
    "    #print(set(my_keys) - set(pretrained_keys))\n",
    "\n",
    "    # we could load these but actually we want to be fine-tuning this stuff in w2d2\n",
    "    # so let's not take these \n",
    "    del pretrained_state_dict[\"classification_head.weight\"]\n",
    "    del pretrained_state_dict[\"classification_head.bias\"]\n",
    "\n",
    "    my_bert.load_state_dict(pretrained_state_dict, strict=False)\n",
    "    \n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fish', 0.2), ('eggs', 0.08), ('meat', 0.07), (',', 0.07), ('apples', 0.03)]\n",
      "[('fish', 0.14), ('eggs', 0.07), ('meat', 0.06), ('potatoes', 0.04), ('vegetables', 0.03)]\n",
      "[('fish', 0.1), ('meat', 0.05), ('eggs', 0.03), ('salt', 0.03), ('bread', 0.02)]\n",
      "[('fish', 0.08), ('eggs', 0.03), ('meat', 0.03), ('food', 0.02), ('salt', 0.02)]\n",
      "[('fish', 0.06), ('eggs', 0.03), ('salt', 0.02), ('meat', 0.02), ('vegetables', 0.02)]\n",
      "[('fish', 0.06), ('salt', 0.02), ('meat', 0.02), ('food', 0.02), ('water', 0.01)]\n",
      "[('fish', 0.07), ('more', 0.03), ('honey', 0.02), ('meat', 0.02), ('salt', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    bert = make_bert()\n",
    "    token_ids = torch.Tensor(tokenizer.encode(sentence)).long().unsqueeze(0)\n",
    "    mask_idxs = set()\n",
    "    for idx, token_id in enumerate(token_ids[0]):\n",
    "        if token_id == 103: # 103 == [MASK]\n",
    "            mask_idxs.add(idx)\n",
    "    unnormalized_output, _classifications = bert(token_ids)\n",
    "    output = torch.log_softmax(unnormalized_output, dim=-1)\n",
    "    top_k = torch.topk(output, 5, dim=-1)\n",
    "    results = []\n",
    "    for seq_i, seq_top_k in enumerate(top_k.indices[0]):\n",
    "        if seq_i in mask_idxs:\n",
    "            results.append(list(zip(tokenizer.convert_ids_to_tokens(seq_top_k), [round(x.item(), 2) for x in top_k.values[0][seq_i].exp()])))\n",
    "    for l in results:\n",
    "        print(l)\n",
    "\n",
    "ascii_art_probs(\"The fish loves to eat [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationBERT():\n",
    "#     def __init__(self, num_classes):\n",
    "#         VOCAB_SIZE = 28996\n",
    "#         DROPOUT=0.1\n",
    "#         self.bert = Bert(\n",
    "#             vocab_size=VOCAB_SIZE, hidden_size=768, max_position_embeddings=512, \n",
    "#             type_vocab_size=2, dropout=DROPOUT, intermediate_size=3072, \n",
    "#             num_heads=12, num_layers=12\n",
    "#         )\n",
    "#         self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "#         # TODO attach this to the last block of BERT as a hidden_size x num_classes linear layer,\n",
    "#         # not on top of the vocab_size output of BERT\n",
    "#         self.classification_head = torch.nn.Linear(VOCAB_SIZE, num_classes)\n",
    "\n",
    "#         pretrained_state_dict = pretrained_bert.state_dict()\n",
    "#         del pretrained_state_dict[\"classification_head.weight\"]\n",
    "#         del pretrained_state_dict[\"classification_head.bias\"]\n",
    "#         self.bert.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         return self.classification_head(self.dropout(self.bert(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train_list = list(data_train)\n",
    "random.shuffle(data_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batches(data_train_list, batch_size, max_seq_len):\n",
    "    batches = []\n",
    "    for batch_start in range(0, len(data_train_list), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(data_train_list))\n",
    "        ratings, sentences = [list(tup) for tup in list(zip(*data_train_list[batch_start:batch_end]))]\n",
    "        tokens = tokenizer(sentences, padding='longest', max_length=max_seq_len, truncation=True)\n",
    "        ratings01 = torch.tensor([ rating == 'pos' for rating in ratings ], dtype=torch.long)\n",
    "        tokens_tensor = torch.tensor(tokens.input_ids, dtype=torch.long)\n",
    "        batches.append((tokens_tensor, ratings01))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] It is a great tragedy that both Richard Harris and John Derek are no longer with us. But that shouldn't blind anybody to the fact that in 1981, a pretty ugly blotch appears on both men's CVs. No doubt John Derek conceived this movie doing for his wife what'Some Like it Hot'and'One Million Years BC'did for Maryln Monroe and Raquel Welsh respectively, creating an iconic sex symbol for the new decade. Having run to embrace Dudley [SEP] tensor([0])\n",
      "[CLS] How this movie escaped the wrath of MST3K I'll never know. \" Gymkata \" is a ridiculous action movie, filled ( or is that empty? ) with paper - thin plots, dumb characters, and preposterous situations. But take it from me, if you enjoy watching poor, yet goofy, movies, you will enjoy \" Gymkata \" a great deal. < br / > < br / > The action centers around a gymnast [SEP] tensor([0])\n",
      "[CLS] I have to admit when I went to see this movie, I didn't really have high expectations. But even with my low expectations I was totally and utterly disappointed... < br / > < br / > Basically Luke Wilson is a hot shot who tends to go out with slightly crazy girlfriends. There's slight mention of a girl stalking him but that's pretty much it for that character. Which i don't quite mind cause it would probably be just as [SEP] tensor([0])\n",
      "[CLS] I knew nothing of this film before I was convinced to see it by a friend who had heard it was a \" non - stop epic battle scene from beginning to end \". That couldn't have been further from the truth. This was one of the most boring, poorly written, amateurishly directed, horribly acted films I've ever had the misfortune to lay my eyes upon. I'd rank it up there with the movie I consider to be the worst [SEP] tensor([0])\n",
      "[CLS] Way back in 1955, the British made a comedy called Simon and Laura, with Peter Finch and the brilliant Kay Kendall. To this day, it stands as one of the finest examples of British comedy and, more particularly, about how television sitcoms become so popular. It was, and is, an excellent example also of self - referential cinema. < br / > < br / > So also Soapdish, a film I'd never heard about until a few nights ago when [SEP] tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in make_batches(data_train_list[:5], 1, 100):\n",
    "    print(tokenizer.decode(review[0]), sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = make_batches(data_train_list, 16, 512)\n",
    "bert = make_bert().cuda()\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(bert.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa921c5c351f4830a0f4a0fc3f2c6a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "  0%|          | 0/1563 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torchviz\n",
    "viz = None\n",
    "def train(model, optimizer, data, lossfn, epochs=1):\n",
    "    global viz\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in tqdm(data):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            X = X.cuda()\n",
    "            _lm_output, y_hat = model(X, only_classification=True)\n",
    "            loss = lossfn(y_hat, y.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            i += 1\n",
    "            if i % 50 == 0:\n",
    "                print(loss)\n",
    "            #print(tokenizer.batch_decode(X[:2]), y_hat, y)\n",
    "train(bert, optimizer, data, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0837, 0.9163]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(torch.tensor(tokenizer.encode(\"I highly recommend this movie to my worst enemy.\")).cuda().unsqueeze(0))[1].softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}