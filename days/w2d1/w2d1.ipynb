{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.0001405 STD: 0.1122 VALS [0.05109 -0.1041 0.1956 -0.02055 -0.06346 0.05726 0.0002961 0.004551 -0.1318 -0.07806...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    K = rearrange(project_key(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "    Q = rearrange(project_query(token_activations), \"b s (n h) -> b s n h\", n = num_heads)\n",
    "\n",
    "    KbyQ = t.einsum(\"bsnh,btnh -> bnst\", K, Q)\n",
    "\n",
    "    d_k = token_activations.shape[2]/num_heads\n",
    "\n",
    "    out = KbyQ/t.sqrt(t.tensor([d_k]))\n",
    "    return out\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001522 STD: 0.118 VALS [0.167 0.06539 0.1114 0.2801 -0.006858 -0.258 0.09339 -0.165 0.3178 -0.04242...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    s = t.nn.Softmax(dim=2)\n",
    "\n",
    "    out = s(attention_pattern) # batch_size, head_num, key_token, query_token\n",
    "\n",
    "    out = rearrange(out, \"b n k q -> b n q k 1\")\n",
    "\n",
    "    V = rearrange(project_value(token_activations), \"b k (n h) -> b n 1 k h\", n = num_heads) # batch_size, num_heads, 1, key, head_size\n",
    "\n",
    "    out = einsum(\"bnqkh,bnqkh -> bnqh\", out, V)\n",
    "\n",
    "    out = rearrange(out, \"b n q h -> b q (n h)\")\n",
    "\n",
    "    return project_output(out)\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "\n",
    "        hidden_dim = num_heads * hidden_size\n",
    "\n",
    "        self.query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = t.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        attention_scores = raw_attention_pattern(input, self.num_heads, self.query, self.key)\n",
    "\n",
    "        attention = bert_attention(input, self.num_heads, attention_scores, self.value, self.output)\n",
    "\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    out = linear_1(token_activations)\n",
    "    out = t.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear_1 = t.nn.Linear(input_size,intermediate_size)\n",
    "        self.linear_2 = t.nn.Linear(intermediate_size,input_size)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return bert_mlp(input,self.linear_1,self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        eps = 1e-05\n",
    "        mean = input.mean(-1).unsqueeze(-1)\n",
    "        mean.detach()\n",
    "        stdev = input.std(-1,unbiased = False).unsqueeze(-1)\n",
    "        stdev.detach()\n",
    "        out = (input - mean)/t.sqrt(stdev**2 + eps) \n",
    "        return out*self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.484e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm1 = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.mhsa(input)\n",
    "        out = self.layer_norm1(input + out)\n",
    "        residual = out\n",
    "        out = self.mlp(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm2(residual + out)\n",
    "        return out\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):        \n",
    "    positions = repeat(t.arange(input_ids.shape[1]), \"p -> b p\", b = input_ids.shape[0])\n",
    "    \n",
    "    if input_ids.is_cuda:\n",
    "        device = input_ids.get_device()\n",
    "        positions.to(device = device)\n",
    "    \n",
    "    out = token_embedding(input_ids) + token_type_embedding(token_type_ids) + position_embedding(positions)\n",
    "    out = layer_norm(out)\n",
    "    return dropout(out)\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-0.7424 -0.3008 -0.8746 0.4806 -0.4385 0.7631 0.6443 0.0507 0.9098 -0.3144...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = t.zeros_like(input_ids)\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = t.nn.Sequential(\n",
    "            BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout),\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "            t.nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        return self.model(input_ids)\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding.layer_norm.weight',\n",
       " 'embedding.layer_norm.bias',\n",
       " 'transformer.0.layer_norm.weight',\n",
       " 'transformer.0.layer_norm.bias',\n",
       " 'transformer.0.residual.layer_norm.weight',\n",
       " 'transformer.0.residual.layer_norm.bias',\n",
       " 'transformer.1.layer_norm.weight',\n",
       " 'transformer.1.layer_norm.bias',\n",
       " 'transformer.1.residual.layer_norm.weight',\n",
       " 'transformer.1.residual.layer_norm.bias',\n",
       " 'transformer.2.layer_norm.weight',\n",
       " 'transformer.2.layer_norm.bias',\n",
       " 'transformer.2.residual.layer_norm.weight',\n",
       " 'transformer.2.residual.layer_norm.bias',\n",
       " 'transformer.3.layer_norm.weight',\n",
       " 'transformer.3.layer_norm.bias',\n",
       " 'transformer.3.residual.layer_norm.weight',\n",
       " 'transformer.3.residual.layer_norm.bias',\n",
       " 'transformer.4.layer_norm.weight',\n",
       " 'transformer.4.layer_norm.bias',\n",
       " 'transformer.4.residual.layer_norm.weight',\n",
       " 'transformer.4.residual.layer_norm.bias',\n",
       " 'transformer.5.layer_norm.weight',\n",
       " 'transformer.5.layer_norm.bias',\n",
       " 'transformer.5.residual.layer_norm.weight',\n",
       " 'transformer.5.residual.layer_norm.bias',\n",
       " 'transformer.6.layer_norm.weight',\n",
       " 'transformer.6.layer_norm.bias',\n",
       " 'transformer.6.residual.layer_norm.weight',\n",
       " 'transformer.6.residual.layer_norm.bias',\n",
       " 'transformer.7.layer_norm.weight',\n",
       " 'transformer.7.layer_norm.bias',\n",
       " 'transformer.7.residual.layer_norm.weight',\n",
       " 'transformer.7.residual.layer_norm.bias',\n",
       " 'transformer.8.layer_norm.weight',\n",
       " 'transformer.8.layer_norm.bias',\n",
       " 'transformer.8.residual.layer_norm.weight',\n",
       " 'transformer.8.residual.layer_norm.bias',\n",
       " 'transformer.9.layer_norm.weight',\n",
       " 'transformer.9.layer_norm.bias',\n",
       " 'transformer.9.residual.layer_norm.weight',\n",
       " 'transformer.9.residual.layer_norm.bias',\n",
       " 'transformer.10.layer_norm.weight',\n",
       " 'transformer.10.layer_norm.bias',\n",
       " 'transformer.10.residual.layer_norm.weight',\n",
       " 'transformer.10.residual.layer_norm.bias',\n",
       " 'transformer.11.layer_norm.weight',\n",
       " 'transformer.11.layer_norm.bias',\n",
       " 'transformer.11.residual.layer_norm.weight',\n",
       " 'transformer.11.residual.layer_norm.bias',\n",
       " 'lm_head.layer_norm.weight',\n",
       " 'lm_head.layer_norm.bias']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def mapkey(key):\n",
    "\n",
    "    # embedding = model.0\n",
    "    key = re.sub(r'^embedding', 'model.0', key)\n",
    "\n",
    "    # transformer.i = model.(i+1)\n",
    "    key = re.sub(r'transformer\\.(\\d+)', lambda expr: \"model.{}\".format(int(expr.groups()[0]) + 1), key)\n",
    "\n",
    "    # attention = mhsa\n",
    "    key = re.sub('.attention.', '.mhsa.', key)\n",
    "\n",
    "    key = re.sub('lm_head.mlp', 'model.13', key)\n",
    "    key = re.sub('lm_head.unembedding', 'model.15', key)\n",
    "    key = re.sub('lm_head.layer_norm', 'model.16', key)\n",
    "\n",
    "    key = re.sub('_embedding.weight', '_embedding.embedding', key)\n",
    "\n",
    "    key = re.sub(r'pattern\\.project_(key|query)', lambda expr: \"{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    key = re.sub('project_out', 'output', key)\n",
    "    key = re.sub('project_value', 'value', key)\n",
    "\n",
    "    key = re.sub(r'residual\\.mlp(1|2)', lambda expr: \"mlp.linear_{}\".format(expr.groups()[0]), key)\n",
    "\n",
    "    # key = re.sub(r'^((?!residual).)*\\.layer_norm', 'layer_norm1', key)\n",
    "    key = re.sub(r'residual\\.layer_norm', 'layer_norm2', key)\n",
    "    key = re.sub(r'layer_norm\\.', 'layer_norm1.', key)\n",
    "\n",
    "    key = re.sub(r'model\\.0\\.layer_norm1', 'model.0.layer_norm', key)\n",
    "\n",
    "    key = re.sub(r'model\\.15', 'model.temp', key)\n",
    "    key = re.sub(r'model\\.16', 'model.15', key)\n",
    "    key = re.sub(r'model\\.temp', 'model.16',key)\n",
    "\n",
    "\n",
    "    return key\n",
    "\n",
    "mapkey('embedding gesgse')\n",
    "mapkey('transformer.1')\n",
    "mapkey('.attention.')\n",
    "mapkey('lm_head.layer_norm')\n",
    "\n",
    "# False 15 model.1.mlp.linear_1.weight model.1.residual.mlp1.weight\n",
    "# False 16 model.1.mlp.linear_1.bias model.1.residual.mlp1.bias\n",
    "\n",
    "\n",
    "def matching(xs, pattern):\n",
    "    return [s for s in xs if re.search(pattern, s)]\n",
    "\n",
    "matching(list(pretrained_bert.state_dict()), 'layer_norm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0 model.0.token_embedding.embedding model.0.token_embedding.embedding\n",
      "True 1 model.0.position_embedding.embedding model.0.position_embedding.embedding\n",
      "True 2 model.0.token_type_embedding.embedding model.0.token_type_embedding.embedding\n",
      "True 3 model.0.layer_norm.weight model.0.layer_norm.weight\n",
      "True 4 model.0.layer_norm.bias model.0.layer_norm.bias\n",
      "False 5 model.1.mhsa.query.weight model.1.layer_norm1.weight\n",
      "False 6 model.1.mhsa.query.bias model.1.layer_norm1.bias\n",
      "False 7 model.1.mhsa.key.weight model.1.mhsa.query.weight\n",
      "False 8 model.1.mhsa.key.bias model.1.mhsa.query.bias\n",
      "False 9 model.1.mhsa.value.weight model.1.mhsa.key.weight\n",
      "False 10 model.1.mhsa.value.bias model.1.mhsa.key.bias\n",
      "False 11 model.1.mhsa.output.weight model.1.mhsa.value.weight\n",
      "False 12 model.1.mhsa.output.bias model.1.mhsa.value.bias\n",
      "False 13 model.1.layer_norm1.weight model.1.mhsa.output.weight\n",
      "False 14 model.1.layer_norm1.bias model.1.mhsa.output.bias\n",
      "True 15 model.1.mlp.linear_1.weight model.1.mlp.linear_1.weight\n",
      "True 16 model.1.mlp.linear_1.bias model.1.mlp.linear_1.bias\n",
      "True 17 model.1.mlp.linear_2.weight model.1.mlp.linear_2.weight\n",
      "True 18 model.1.mlp.linear_2.bias model.1.mlp.linear_2.bias\n",
      "True 19 model.1.layer_norm2.weight model.1.layer_norm2.weight\n",
      "True 20 model.1.layer_norm2.bias model.1.layer_norm2.bias\n",
      "False 21 model.2.mhsa.query.weight model.2.layer_norm1.weight\n",
      "False 22 model.2.mhsa.query.bias model.2.layer_norm1.bias\n",
      "False 23 model.2.mhsa.key.weight model.2.mhsa.query.weight\n",
      "False 24 model.2.mhsa.key.bias model.2.mhsa.query.bias\n",
      "False 25 model.2.mhsa.value.weight model.2.mhsa.key.weight\n",
      "False 26 model.2.mhsa.value.bias model.2.mhsa.key.bias\n",
      "False 27 model.2.mhsa.output.weight model.2.mhsa.value.weight\n",
      "False 28 model.2.mhsa.output.bias model.2.mhsa.value.bias\n",
      "False 29 model.2.layer_norm1.weight model.2.mhsa.output.weight\n",
      "False 30 model.2.layer_norm1.bias model.2.mhsa.output.bias\n",
      "True 31 model.2.mlp.linear_1.weight model.2.mlp.linear_1.weight\n",
      "True 32 model.2.mlp.linear_1.bias model.2.mlp.linear_1.bias\n",
      "True 33 model.2.mlp.linear_2.weight model.2.mlp.linear_2.weight\n",
      "True 34 model.2.mlp.linear_2.bias model.2.mlp.linear_2.bias\n",
      "True 35 model.2.layer_norm2.weight model.2.layer_norm2.weight\n",
      "True 36 model.2.layer_norm2.bias model.2.layer_norm2.bias\n",
      "False 37 model.3.mhsa.query.weight model.3.layer_norm1.weight\n",
      "False 38 model.3.mhsa.query.bias model.3.layer_norm1.bias\n",
      "False 39 model.3.mhsa.key.weight model.3.mhsa.query.weight\n",
      "False 40 model.3.mhsa.key.bias model.3.mhsa.query.bias\n",
      "False 41 model.3.mhsa.value.weight model.3.mhsa.key.weight\n",
      "False 42 model.3.mhsa.value.bias model.3.mhsa.key.bias\n",
      "False 43 model.3.mhsa.output.weight model.3.mhsa.value.weight\n",
      "False 44 model.3.mhsa.output.bias model.3.mhsa.value.bias\n",
      "False 45 model.3.layer_norm1.weight model.3.mhsa.output.weight\n",
      "False 46 model.3.layer_norm1.bias model.3.mhsa.output.bias\n",
      "True 47 model.3.mlp.linear_1.weight model.3.mlp.linear_1.weight\n",
      "True 48 model.3.mlp.linear_1.bias model.3.mlp.linear_1.bias\n",
      "True 49 model.3.mlp.linear_2.weight model.3.mlp.linear_2.weight\n",
      "True 50 model.3.mlp.linear_2.bias model.3.mlp.linear_2.bias\n",
      "True 51 model.3.layer_norm2.weight model.3.layer_norm2.weight\n",
      "True 52 model.3.layer_norm2.bias model.3.layer_norm2.bias\n",
      "False 53 model.4.mhsa.query.weight model.4.layer_norm1.weight\n",
      "False 54 model.4.mhsa.query.bias model.4.layer_norm1.bias\n",
      "False 55 model.4.mhsa.key.weight model.4.mhsa.query.weight\n",
      "False 56 model.4.mhsa.key.bias model.4.mhsa.query.bias\n",
      "False 57 model.4.mhsa.value.weight model.4.mhsa.key.weight\n",
      "False 58 model.4.mhsa.value.bias model.4.mhsa.key.bias\n",
      "False 59 model.4.mhsa.output.weight model.4.mhsa.value.weight\n",
      "False 60 model.4.mhsa.output.bias model.4.mhsa.value.bias\n",
      "False 61 model.4.layer_norm1.weight model.4.mhsa.output.weight\n",
      "False 62 model.4.layer_norm1.bias model.4.mhsa.output.bias\n",
      "True 63 model.4.mlp.linear_1.weight model.4.mlp.linear_1.weight\n",
      "True 64 model.4.mlp.linear_1.bias model.4.mlp.linear_1.bias\n",
      "True 65 model.4.mlp.linear_2.weight model.4.mlp.linear_2.weight\n",
      "True 66 model.4.mlp.linear_2.bias model.4.mlp.linear_2.bias\n",
      "True 67 model.4.layer_norm2.weight model.4.layer_norm2.weight\n",
      "True 68 model.4.layer_norm2.bias model.4.layer_norm2.bias\n",
      "False 69 model.5.mhsa.query.weight model.5.layer_norm1.weight\n",
      "False 70 model.5.mhsa.query.bias model.5.layer_norm1.bias\n",
      "False 71 model.5.mhsa.key.weight model.5.mhsa.query.weight\n",
      "False 72 model.5.mhsa.key.bias model.5.mhsa.query.bias\n",
      "False 73 model.5.mhsa.value.weight model.5.mhsa.key.weight\n",
      "False 74 model.5.mhsa.value.bias model.5.mhsa.key.bias\n",
      "False 75 model.5.mhsa.output.weight model.5.mhsa.value.weight\n",
      "False 76 model.5.mhsa.output.bias model.5.mhsa.value.bias\n",
      "False 77 model.5.layer_norm1.weight model.5.mhsa.output.weight\n",
      "False 78 model.5.layer_norm1.bias model.5.mhsa.output.bias\n",
      "True 79 model.5.mlp.linear_1.weight model.5.mlp.linear_1.weight\n",
      "True 80 model.5.mlp.linear_1.bias model.5.mlp.linear_1.bias\n",
      "True 81 model.5.mlp.linear_2.weight model.5.mlp.linear_2.weight\n",
      "True 82 model.5.mlp.linear_2.bias model.5.mlp.linear_2.bias\n",
      "True 83 model.5.layer_norm2.weight model.5.layer_norm2.weight\n",
      "True 84 model.5.layer_norm2.bias model.5.layer_norm2.bias\n",
      "False 85 model.6.mhsa.query.weight model.6.layer_norm1.weight\n",
      "False 86 model.6.mhsa.query.bias model.6.layer_norm1.bias\n",
      "False 87 model.6.mhsa.key.weight model.6.mhsa.query.weight\n",
      "False 88 model.6.mhsa.key.bias model.6.mhsa.query.bias\n",
      "False 89 model.6.mhsa.value.weight model.6.mhsa.key.weight\n",
      "False 90 model.6.mhsa.value.bias model.6.mhsa.key.bias\n",
      "False 91 model.6.mhsa.output.weight model.6.mhsa.value.weight\n",
      "False 92 model.6.mhsa.output.bias model.6.mhsa.value.bias\n",
      "False 93 model.6.layer_norm1.weight model.6.mhsa.output.weight\n",
      "False 94 model.6.layer_norm1.bias model.6.mhsa.output.bias\n",
      "True 95 model.6.mlp.linear_1.weight model.6.mlp.linear_1.weight\n",
      "True 96 model.6.mlp.linear_1.bias model.6.mlp.linear_1.bias\n",
      "True 97 model.6.mlp.linear_2.weight model.6.mlp.linear_2.weight\n",
      "True 98 model.6.mlp.linear_2.bias model.6.mlp.linear_2.bias\n",
      "True 99 model.6.layer_norm2.weight model.6.layer_norm2.weight\n",
      "True 100 model.6.layer_norm2.bias model.6.layer_norm2.bias\n",
      "False 101 model.7.mhsa.query.weight model.7.layer_norm1.weight\n",
      "False 102 model.7.mhsa.query.bias model.7.layer_norm1.bias\n",
      "False 103 model.7.mhsa.key.weight model.7.mhsa.query.weight\n",
      "False 104 model.7.mhsa.key.bias model.7.mhsa.query.bias\n",
      "False 105 model.7.mhsa.value.weight model.7.mhsa.key.weight\n",
      "False 106 model.7.mhsa.value.bias model.7.mhsa.key.bias\n",
      "False 107 model.7.mhsa.output.weight model.7.mhsa.value.weight\n",
      "False 108 model.7.mhsa.output.bias model.7.mhsa.value.bias\n",
      "False 109 model.7.layer_norm1.weight model.7.mhsa.output.weight\n",
      "False 110 model.7.layer_norm1.bias model.7.mhsa.output.bias\n",
      "True 111 model.7.mlp.linear_1.weight model.7.mlp.linear_1.weight\n",
      "True 112 model.7.mlp.linear_1.bias model.7.mlp.linear_1.bias\n",
      "True 113 model.7.mlp.linear_2.weight model.7.mlp.linear_2.weight\n",
      "True 114 model.7.mlp.linear_2.bias model.7.mlp.linear_2.bias\n",
      "True 115 model.7.layer_norm2.weight model.7.layer_norm2.weight\n",
      "True 116 model.7.layer_norm2.bias model.7.layer_norm2.bias\n",
      "False 117 model.8.mhsa.query.weight model.8.layer_norm1.weight\n",
      "False 118 model.8.mhsa.query.bias model.8.layer_norm1.bias\n",
      "False 119 model.8.mhsa.key.weight model.8.mhsa.query.weight\n",
      "False 120 model.8.mhsa.key.bias model.8.mhsa.query.bias\n",
      "False 121 model.8.mhsa.value.weight model.8.mhsa.key.weight\n",
      "False 122 model.8.mhsa.value.bias model.8.mhsa.key.bias\n",
      "False 123 model.8.mhsa.output.weight model.8.mhsa.value.weight\n",
      "False 124 model.8.mhsa.output.bias model.8.mhsa.value.bias\n",
      "False 125 model.8.layer_norm1.weight model.8.mhsa.output.weight\n",
      "False 126 model.8.layer_norm1.bias model.8.mhsa.output.bias\n",
      "True 127 model.8.mlp.linear_1.weight model.8.mlp.linear_1.weight\n",
      "True 128 model.8.mlp.linear_1.bias model.8.mlp.linear_1.bias\n",
      "True 129 model.8.mlp.linear_2.weight model.8.mlp.linear_2.weight\n",
      "True 130 model.8.mlp.linear_2.bias model.8.mlp.linear_2.bias\n",
      "True 131 model.8.layer_norm2.weight model.8.layer_norm2.weight\n",
      "True 132 model.8.layer_norm2.bias model.8.layer_norm2.bias\n",
      "False 133 model.9.mhsa.query.weight model.9.layer_norm1.weight\n",
      "False 134 model.9.mhsa.query.bias model.9.layer_norm1.bias\n",
      "False 135 model.9.mhsa.key.weight model.9.mhsa.query.weight\n",
      "False 136 model.9.mhsa.key.bias model.9.mhsa.query.bias\n",
      "False 137 model.9.mhsa.value.weight model.9.mhsa.key.weight\n",
      "False 138 model.9.mhsa.value.bias model.9.mhsa.key.bias\n",
      "False 139 model.9.mhsa.output.weight model.9.mhsa.value.weight\n",
      "False 140 model.9.mhsa.output.bias model.9.mhsa.value.bias\n",
      "False 141 model.9.layer_norm1.weight model.9.mhsa.output.weight\n",
      "False 142 model.9.layer_norm1.bias model.9.mhsa.output.bias\n",
      "True 143 model.9.mlp.linear_1.weight model.9.mlp.linear_1.weight\n",
      "True 144 model.9.mlp.linear_1.bias model.9.mlp.linear_1.bias\n",
      "True 145 model.9.mlp.linear_2.weight model.9.mlp.linear_2.weight\n",
      "True 146 model.9.mlp.linear_2.bias model.9.mlp.linear_2.bias\n",
      "True 147 model.9.layer_norm2.weight model.9.layer_norm2.weight\n",
      "True 148 model.9.layer_norm2.bias model.9.layer_norm2.bias\n",
      "False 149 model.10.mhsa.query.weight model.10.layer_norm1.weight\n",
      "False 150 model.10.mhsa.query.bias model.10.layer_norm1.bias\n",
      "False 151 model.10.mhsa.key.weight model.10.mhsa.query.weight\n",
      "False 152 model.10.mhsa.key.bias model.10.mhsa.query.bias\n",
      "False 153 model.10.mhsa.value.weight model.10.mhsa.key.weight\n",
      "False 154 model.10.mhsa.value.bias model.10.mhsa.key.bias\n",
      "False 155 model.10.mhsa.output.weight model.10.mhsa.value.weight\n",
      "False 156 model.10.mhsa.output.bias model.10.mhsa.value.bias\n",
      "False 157 model.10.layer_norm1.weight model.10.mhsa.output.weight\n",
      "False 158 model.10.layer_norm1.bias model.10.mhsa.output.bias\n",
      "True 159 model.10.mlp.linear_1.weight model.10.mlp.linear_1.weight\n",
      "True 160 model.10.mlp.linear_1.bias model.10.mlp.linear_1.bias\n",
      "True 161 model.10.mlp.linear_2.weight model.10.mlp.linear_2.weight\n",
      "True 162 model.10.mlp.linear_2.bias model.10.mlp.linear_2.bias\n",
      "True 163 model.10.layer_norm2.weight model.10.layer_norm2.weight\n",
      "True 164 model.10.layer_norm2.bias model.10.layer_norm2.bias\n",
      "False 165 model.11.mhsa.query.weight model.11.layer_norm1.weight\n",
      "False 166 model.11.mhsa.query.bias model.11.layer_norm1.bias\n",
      "False 167 model.11.mhsa.key.weight model.11.mhsa.query.weight\n",
      "False 168 model.11.mhsa.key.bias model.11.mhsa.query.bias\n",
      "False 169 model.11.mhsa.value.weight model.11.mhsa.key.weight\n",
      "False 170 model.11.mhsa.value.bias model.11.mhsa.key.bias\n",
      "False 171 model.11.mhsa.output.weight model.11.mhsa.value.weight\n",
      "False 172 model.11.mhsa.output.bias model.11.mhsa.value.bias\n",
      "False 173 model.11.layer_norm1.weight model.11.mhsa.output.weight\n",
      "False 174 model.11.layer_norm1.bias model.11.mhsa.output.bias\n",
      "True 175 model.11.mlp.linear_1.weight model.11.mlp.linear_1.weight\n",
      "True 176 model.11.mlp.linear_1.bias model.11.mlp.linear_1.bias\n",
      "True 177 model.11.mlp.linear_2.weight model.11.mlp.linear_2.weight\n",
      "True 178 model.11.mlp.linear_2.bias model.11.mlp.linear_2.bias\n",
      "True 179 model.11.layer_norm2.weight model.11.layer_norm2.weight\n",
      "True 180 model.11.layer_norm2.bias model.11.layer_norm2.bias\n",
      "False 181 model.12.mhsa.query.weight model.12.layer_norm1.weight\n",
      "False 182 model.12.mhsa.query.bias model.12.layer_norm1.bias\n",
      "False 183 model.12.mhsa.key.weight model.12.mhsa.query.weight\n",
      "False 184 model.12.mhsa.key.bias model.12.mhsa.query.bias\n",
      "False 185 model.12.mhsa.value.weight model.12.mhsa.key.weight\n",
      "False 186 model.12.mhsa.value.bias model.12.mhsa.key.bias\n",
      "False 187 model.12.mhsa.output.weight model.12.mhsa.value.weight\n",
      "False 188 model.12.mhsa.output.bias model.12.mhsa.value.bias\n",
      "False 189 model.12.layer_norm1.weight model.12.mhsa.output.weight\n",
      "False 190 model.12.layer_norm1.bias model.12.mhsa.output.bias\n",
      "True 191 model.12.mlp.linear_1.weight model.12.mlp.linear_1.weight\n",
      "True 192 model.12.mlp.linear_1.bias model.12.mlp.linear_1.bias\n",
      "True 193 model.12.mlp.linear_2.weight model.12.mlp.linear_2.weight\n",
      "True 194 model.12.mlp.linear_2.bias model.12.mlp.linear_2.bias\n",
      "True 195 model.12.layer_norm2.weight model.12.layer_norm2.weight\n",
      "True 196 model.12.layer_norm2.bias model.12.layer_norm2.bias\n",
      "True 197 model.13.weight model.13.weight\n",
      "True 198 model.13.bias model.13.bias\n",
      "False 199 model.15.weight model.16.weight\n",
      "False 200 model.15.bias model.16.bias\n",
      "False 201 model.16.weight model.15.weight\n",
      "False 202 model.16.bias model.15.bias\n"
     ]
    }
   ],
   "source": [
    "pretrained_params = list(pretrained_bert.state_dict())\n",
    "\n",
    "for i, our_param in enumerate(my_bert.state_dict()):\n",
    "    print(our_param == mapkey(pretrained_params[i]), i, our_param, mapkey(pretrained_params[i]))\n",
    "\n",
    "# for pretrained_param in pretrained_bert.state_dict():\n",
    "#     if mapkey(pretrained_param) not in my_bert.state_dict():\n",
    "#         print(mapkey(pretrained_param))\n",
    "\n",
    "# print(list(pretrained_bert.state_dict()))\n",
    "# print(list(my_bert.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for pretrained_param in pretrained_bert.state_dict():\n",
    "    if pretrained_param != \"classification_head.weight\" and pretrained_param != \"classification_head.bias\":\n",
    "        new_state_dict[mapkey(pretrained_param)] = pretrained_bert.state_dict()[pretrained_param]\n",
    "\n",
    "my_bert.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
