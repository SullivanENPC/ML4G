{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum, nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.01187 STD: 0.1058 VALS [-0.08359 0.04135 -0.3284 -0.07717 -0.08166 -0.06112 0.167 -0.09143 -0.05247 0.08712...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):   \n",
    "    queries = project_query(token_activations)\n",
    "    keys = project_key(token_activations)\n",
    "    queries = rearrange(queries, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    keys = rearrange(keys, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    result = einsum(\"bhnc,bhmc->bhnm\", keys, queries)\n",
    "    return result / math.sqrt(queries.shape[-1])\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.005397 STD: 0.1188 VALS [-0.1328 -0.03846 0.0552 0.05028 -0.2345 0.2486 -0.05643 -0.1067 -0.05023 0.2406...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    softmax_fn = torch.nn.Softmax(dim=-2)\n",
    "    attention_pattern = softmax_fn(attention_pattern)\n",
    "    values = project_value(token_activations)\n",
    "    values = rearrange(values, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    output = einsum(\"bhkq, bhkc -> bhqc\", attention_pattern, values)\n",
    "    output = rearrange(output, \"b h n c -> b n (h c)\")\n",
    "    result = project_output(output)\n",
    "    return result\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class RawAttentionPattern(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.project_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_key = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, token_activations, num_heads):\n",
    "        return raw_attention_pattern(token_activations, num_heads, self.project_query, self.project_key)\n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pattern = RawAttentionPattern(hidden_size)       \n",
    "        self.project_value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, token_activations):\n",
    "        raw_attention = self.pattern(token_activations, self.num_heads)\n",
    "        return bert_attention(token_activations, self.num_heads, raw_attention, self.project_value, self.project_out)\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    return linear_2(nn.GELU()(linear_1(token_activations)))\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, input_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return bert_mlp(X, self.linear1, self.linear2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim, eps=1e-05):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(normalized_dim))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(normalized_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        var = torch.var(X.detach(), -1, unbiased=False)\n",
    "        mean = torch.mean(X.detach(), -1)\n",
    "\n",
    "        return (X.detach() - mean.unsqueeze(-1)) / torch.sqrt(var.unsqueeze(-1) + self.eps) * self.weight + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertResidual(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.mlp1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.mlp2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        identity = torch.clone(X)\n",
    "        X = bert_mlp(X, self.mlp1, self.mlp2)\n",
    "        return self.layer_norm(self.dropout(X) + identity)\n",
    "\n",
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "\n",
    "        # attention: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # layer_norm: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # mlp: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # layer_norm: batches tokens heads * channels\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        # self.mlp = BertMLP(hidden_size, intermediate_size) # hidden_size = heads * channels\n",
    "        self.residual = BertResidual(hidden_size, intermediate_size, dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        residual1 = torch.clone(X)\n",
    "        output1 = self.layer_norm(self.attention(X) + residual1)\n",
    "        return self.residual(output1)\n",
    "\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return (self.weight[X, :]).to(X.device)\n",
    "\n",
    "bert_tests.test_embedding(Embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# cool stuff!!!!\n",
    "\n",
    "x = torch.randint(0, 10, (2, 3)).cuda()\n",
    "print(x.is_cuda)\n",
    "emb1 = Embedding(10, 5)\n",
    "print(emb1(x).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [0.2316 0.08455 -0.5146 1.436 2.029 -1.117 2.775 -0.5305 -0.4485 -0.2485...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "    input_ids, # batch_size x max sentence length -> token_id\n",
    "    token_type_ids, # batch_size x max sentence length -> token_type\n",
    "    position_embedding, # [positions] -> [embeddings for each position]\n",
    "    token_embedding, # token_ids -> [embeddings for each token]\n",
    "    token_type_embedding, # token_type -> \n",
    "    layer_norm, # keeps dimensions\n",
    "    dropout # keeps dimensions\n",
    "):\n",
    "    token_embed = token_embedding(input_ids).to(input_ids.device)\n",
    "    token_type_embed = token_type_embedding(token_type_ids).to(input_ids.device)\n",
    "    position_embed = position_embedding(torch.arange(input_ids.shape[1])).to(input_ids.device)\n",
    "    return dropout(layer_norm(token_embed + token_type_embed + position_embed)).to(input_ids.device)\n",
    "    \n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(\n",
    "            input_ids, \n",
    "            token_type_ids, \n",
    "            self.position_embedding, \n",
    "            self.token_embedding,\n",
    "            self.token_type_embedding,\n",
    "            self.layer_norm,\n",
    "            self.dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "class BertUnembed(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.unembedding = nn.Linear(hidden_size, vocab_size)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, activations):\n",
    "        return self.unembedding(self.layer_norm(self.gelu(self.mlp(activations))))\n",
    "\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size,\n",
    "        dropout, intermediate_size, num_heads, num_layers, num_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.ModuleList([BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.lm_head = BertUnembed(hidden_size, vocab_size)\n",
    "        self.classification_head = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, only_classification=False):\n",
    "        token_type_ids = torch.zeros_like(input_ids, device=input_ids.device)\n",
    "\n",
    "        embeddings = self.embedding(input_ids, token_type_ids)\n",
    "\n",
    "        post_bert_blocks = embeddings\n",
    "        for block in self.transformer:\n",
    "            post_bert_blocks = block(post_bert_blocks)\n",
    "\n",
    "        lm_output = None if only_classification else self.lm_head(post_bert_blocks)\n",
    "        return lm_output, self.classification_head(post_bert_blocks[:,0])\n",
    "\n",
    "#bert_tests.test_bert(Bert)\n",
    "bert_tests.test_bert_classification(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "_IncompatibleKeys(missing_keys=['classification_head.weight', 'classification_head.bias'], unexpected_keys=[])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_state_dict = pretrained_bert.state_dict()\n",
    "\n",
    "#pretrained_keys = pretrained_state_dict.keys()\n",
    "#my_keys = my_bert.state_dict().keys()\n",
    "#print(set(my_keys) - set(pretrained_keys))\n",
    "\n",
    "# we could load these but actually we want to be fine-tuning this stuff in w2d2\n",
    "# so let's not take these \n",
    "del pretrained_state_dict[\"classification_head.weight\"]\n",
    "del pretrained_state_dict[\"classification_head.bias\"]\n",
    "\n",
    "my_bert.load_state_dict(pretrained_state_dict, strict=False)\n",
    "#bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}