{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_sol as bert\n",
    "import bert_tests\n",
    "import gpt\n",
    "from hook_handler import HookHandler\n",
    "\n",
    "import torch as t\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert vs GPT-2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bert = bert.Bert(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512,\n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072,\n",
    "        num_heads=12, num_layers=12\n",
    "    )\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n",
    "mapped_params = {bert.mapkey(k): v for k, v in pretrained_bert.state_dict().items()\n",
    "                if not k.startswith('classification_head')}\n",
    "my_bert.load_state_dict(mapped_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = gpt.get_gpt_with_pretrained_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.blocks[-1] tensor([[-0.2722,  0.2118,  0.1717,  ...,  0.4553, -0.0064,  0.2353],\n",
      "        [-0.3859,  0.1262,  0.1194,  ...,  0.8246,  0.1501,  0.1782],\n",
      "        [-0.3540,  0.1779,  0.0127,  ...,  0.6782,  0.2399,  0.1714],\n",
      "        [-0.3492,  0.1421,  0.2156,  ...,  0.7240,  0.4313,  0.0808],\n",
      "        [-0.3356,  0.0488,  0.0885,  ...,  0.6900,  0.5473,  0.1323]],\n",
      "       device='cuda:0')\n",
      "gpt.blocks[-1] tensor([[10.4828,  2.3441,  3.3644,  ...,  3.3975, -4.6225,  0.4826],\n",
      "        [10.4828,  2.3441,  3.3644,  ...,  3.3975, -4.6225,  0.4826],\n",
      "        [10.4828,  2.3441,  3.3644,  ...,  3.3975, -4.6225,  0.4826],\n",
      "        [10.4828,  2.3441,  3.3644,  ...,  3.3975, -4.6225,  0.4826],\n",
      "        [10.4828,  2.3441,  3.3644,  ...,  3.3975, -4.6225,  0.4826]],\n",
      "       device='cuda:0')\n",
      "All hooks removed!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "my_bert.cuda()\n",
    "my_gpt.cuda()\n",
    "with HookHandler() as hh:\n",
    "    hh.add_save_activation_hook(my_bert.blocks[-1], \"bert.blocks[-1]\")\n",
    "    hh.add_save_activation_hook(my_gpt.gpt_blocks[-1], \"gpt.blocks[-1]\")\n",
    "\n",
    "    xs = t.tensor(\n",
    "        tokenizer(\n",
    "            [\n",
    "                \"My life motto:\",\n",
    "                \"My life motto: Fortune\",\n",
    "                \"My life motto: Fortune favors\",\n",
    "                \"My life motto: Fortune favors the\",\n",
    "                \"My life motto: Fortune favors the bold\",\n",
    "            ],\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"],\n",
    "        dtype=t.long,\n",
    "        device=my_gpt.device,\n",
    "    )\n",
    "\n",
    "    my_gpt.eval()\n",
    "    my_bert.eval()\n",
    "\n",
    "    my_gpt(xs)\n",
    "    my_bert(xs)\n",
    "\n",
    "    print(\"bert.blocks[-1]\", hh.activations[\"bert.blocks[-1]\"][:, 4])\n",
    "    print(\"gpt.blocks[-1]\", hh.activations[\"gpt.blocks[-1]\"][:, 4])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
