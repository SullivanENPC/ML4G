{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from typing import *\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import gpt_tests\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from math import sqrt\n",
    "import bert_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the GPT-2 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size/num_heads\n",
    "        self.attn_lin = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.out_lin = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        product = self.attn_lin(x)\n",
    "        seq_len = x.shape[1]\n",
    "        good_format = rearrange(product, 'b n (qkv h p) -> qkv b h n p', qkv = 3, h = self.num_heads)\n",
    "        queries, keys, values = good_format[0], good_format[1], good_format[2]\n",
    "        attn_score = t.einsum('bhfp,bhtp -> bhft', keys, queries) / sqrt(self.head_size)\n",
    "        \n",
    "        arange = t.arange(seq_len, device=x.device)\n",
    "        arange_rows = repeat(arange, 'a -> b a', b = seq_len)\n",
    "        arange_cols = repeat(arange, 'a -> a b', b = seq_len)\n",
    "        attn_score[:,:,arange_rows < arange_cols] = -1e4\n",
    "        \n",
    "        attn_pattn = t.softmax(attn_score, dim=-2)\n",
    "        # attn_pattn: b h n n; values: b h n p\n",
    "        out_by_head = t.einsum('bhft,bhfp->bhtp', attn_pattn, values)\n",
    "        out = rearrange(out_by_head, 'b h t p -> b t (h p)') # b n hidden_size\n",
    "        return self.out_lin(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_unidirectional_attn(MultiHeadedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size*4, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = self.attn(x1) + x\n",
    "        x3 = self.ln2(x2)\n",
    "        return self.mlp(x3) + x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        \n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        final_encoding = encoding[:,-1]\n",
    "        \n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        \n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_replace(s):\n",
    "    s = s.replace(\"embedding.weight\", \"embedding\")\n",
    "    s = s.replace(\"linear1\", \"mlp.0\")\n",
    "    s = s.replace(\"linear2\", \"mlp.2\")\n",
    "    s = s.replace(\"ln.\", \"blocks.12.\")\n",
    "    return s\n",
    "\n",
    "their_dict = pretrained_gpt.state_dict()\n",
    "for k in list(their_dict.keys()):\n",
    "    their_dict[string_replace(k)] = their_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    top_logit_idxs = t.argsort(logits, descending=True)[0,:top_k]\n",
    "    top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "    print(top_logit_words)\n",
    "    print(probs[0,top_logit_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " experience learning opportunity program process course training time work challenge\n",
      "tensor([0.1915, 0.0389, 0.0338, 0.0290, 0.0174, 0.0165, 0.0162, 0.0151, 0.0129,\n",
      "        0.0129], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pretrained_gpt.cpu()\n",
    "feed_gpt(pretrained_gpt, \"Students at the machine learning bootcamp really enjoyed the\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt_top(model: nn.Module, input_ids: List[int], top_k: int = 10):\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    return t.argsort(logits, descending=True)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that  the  machine  learning  boot camp  was  actually  a  very  bad  idea . \n",
      " \n",
      " I  was  not  alone .  I  was  also  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my "
     ]
    }
   ],
   "source": [
    "start_str = \"The machine learning bootcamp started out nicely. But soon, I got an ominous feeling. Shockingly, I discovered\"\n",
    "input_ids = tokenizer(start_str)[\"input_ids\"]\n",
    "for i in range(100):\n",
    "    new_token = feed_gpt_top(my_gpt, input_ids, 1)\n",
    "    print(tokenizer.decode(new_token), end = \" \")\n",
    "    input_ids.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Modified(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        self.encoding = encoding\n",
    "        final_encoding = encoding[:,-1]\n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt_modified = GPT2Modified(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt_modified.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_thing():\n",
    "    s = 'My life motto: Fortune favors the bold'\n",
    "    ids = tokenizer(s)[\"input_ids\"]\n",
    "    return [ids[:i]+[0]*(10-i) for i in range(4,9)]\n",
    "\n",
    "test_batch = t.tensor(create_padded_thing())\n",
    "\n",
    "def decode_batch(batch):\n",
    "    print([tokenizer.decode(batch[i]) for i in range(len(batch))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.0323, -0.3814],\n",
       "         [-0.0505, -0.2386],\n",
       "         [ 0.0306, -0.1637],\n",
       "         [ 0.0751, -0.1712],\n",
       "         [ 0.0916, -0.1977],\n",
       "         [ 0.1046, -0.2181]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.3443, -0.1450],\n",
       "         [ 0.2800, -0.1220],\n",
       "         [ 0.2185, -0.1960],\n",
       "         [ 0.2100, -0.1533],\n",
       "         [ 0.2249, -0.1559]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [ 0.4627, -0.2614],\n",
       "         [ 0.3536, -0.2568],\n",
       "         [ 0.1176, -0.2094],\n",
       "         [ 0.1202, -0.1308]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [-0.0704,  0.0556],\n",
       "         [ 0.1992, -0.2992],\n",
       "         [ 0.1801, -0.1881],\n",
       "         [-0.2132, -0.0607]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [-0.0704,  0.0556],\n",
       "         [-0.6858, -0.9147],\n",
       "         [ 0.3317, -0.2639],\n",
       "         [ 0.0736, -0.2989]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt_modified.eval()\n",
    "my_gpt_modified(test_batch);\n",
    "my_gpt_modified.encoding[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
