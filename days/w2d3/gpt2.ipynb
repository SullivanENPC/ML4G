{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import gpt_tests\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the GPT-2 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size/num_heads\n",
    "        self.attn_lin = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.out_lin = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        product = self.attn_lin(x)\n",
    "        seq_len = x.shape[1]\n",
    "        good_format = rearrange(product, 'b n (qkv h p) -> qkv b h n p', qkv = 3, h = self.num_heads)\n",
    "        queries, keys, values = good_format[0], good_format[1], good_format[2]\n",
    "        attn_score = t.einsum('bhfp,bhtp -> bhft', keys, queries) / sqrt(self.head_size)\n",
    "        \n",
    "        arange = t.arange(seq_len, device=x.device)\n",
    "        arange_rows = repeat(arange, 'a -> b a', b = seq_len)\n",
    "        arange_cols = repeat(arange, 'a -> a b', b = seq_len)\n",
    "        attn_score[:,:,arange_rows < arange_cols] = -1e4\n",
    "        \n",
    "        attn_pattn = t.softmax(attn_score, dim=-2)\n",
    "        # attn_pattn: b h n n; values: b h n p\n",
    "        out_by_head = t.einsum('bhft,bhfp->bhtp', attn_pattn, values)\n",
    "        out = rearrange(out_by_head, 'b h t p -> b t (h p)') # b n hidden_size\n",
    "        return self.out_lin(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_unidirectional_attn(MultiHeadedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size*4, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = self.attn(x1) + x\n",
    "        x3 = self.ln2(x2)\n",
    "        return self.mlp(x3) + x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        \n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        final_encoding = encoding[:,-1]\n",
    "        \n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        \n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_replace(s):\n",
    "    s = s.replace(\"embedding.weight\", \"embedding\")\n",
    "    s = s.replace(\"linear1\", \"mlp.0\")\n",
    "    s = s.replace(\"linear2\", \"mlp.2\")\n",
    "    s = s.replace(\"ln.\", \"blocks.12.\")\n",
    "    return s\n",
    "\n",
    "their_dict = pretrained_gpt.state_dict()\n",
    "for k in list(their_dict.keys()):\n",
    "    their_dict[string_replace(k)] = their_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding\n",
      "pos_embedding\n",
      "blocks.0.ln1.weight\n",
      "blocks.0.ln1.bias\n",
      "blocks.0.attn.attn_lin.weight\n",
      "blocks.0.attn.attn_lin.bias\n",
      "blocks.0.attn.out_lin.weight\n",
      "blocks.0.attn.out_lin.bias\n",
      "blocks.0.ln2.weight\n",
      "blocks.0.ln2.bias\n",
      "blocks.0.mlp.0.weight\n",
      "blocks.0.mlp.0.bias\n",
      "blocks.0.mlp.2.weight\n",
      "blocks.0.mlp.2.bias\n",
      "blocks.1.ln1.weight\n",
      "blocks.1.ln1.bias\n",
      "blocks.1.attn.attn_lin.weight\n",
      "blocks.1.attn.attn_lin.bias\n",
      "blocks.1.attn.out_lin.weight\n",
      "blocks.1.attn.out_lin.bias\n",
      "blocks.1.ln2.weight\n",
      "blocks.1.ln2.bias\n",
      "blocks.1.mlp.0.weight\n",
      "blocks.1.mlp.0.bias\n",
      "blocks.1.mlp.2.weight\n",
      "blocks.1.mlp.2.bias\n",
      "blocks.2.ln1.weight\n",
      "blocks.2.ln1.bias\n",
      "blocks.2.attn.attn_lin.weight\n",
      "blocks.2.attn.attn_lin.bias\n",
      "blocks.2.attn.out_lin.weight\n",
      "blocks.2.attn.out_lin.bias\n",
      "blocks.2.ln2.weight\n",
      "blocks.2.ln2.bias\n",
      "blocks.2.mlp.0.weight\n",
      "blocks.2.mlp.0.bias\n",
      "blocks.2.mlp.2.weight\n",
      "blocks.2.mlp.2.bias\n",
      "blocks.3.ln1.weight\n",
      "blocks.3.ln1.bias\n",
      "blocks.3.attn.attn_lin.weight\n",
      "blocks.3.attn.attn_lin.bias\n",
      "blocks.3.attn.out_lin.weight\n",
      "blocks.3.attn.out_lin.bias\n",
      "blocks.3.ln2.weight\n",
      "blocks.3.ln2.bias\n",
      "blocks.3.mlp.0.weight\n",
      "blocks.3.mlp.0.bias\n",
      "blocks.3.mlp.2.weight\n",
      "blocks.3.mlp.2.bias\n",
      "blocks.4.ln1.weight\n",
      "blocks.4.ln1.bias\n",
      "blocks.4.attn.attn_lin.weight\n",
      "blocks.4.attn.attn_lin.bias\n",
      "blocks.4.attn.out_lin.weight\n",
      "blocks.4.attn.out_lin.bias\n",
      "blocks.4.ln2.weight\n",
      "blocks.4.ln2.bias\n",
      "blocks.4.mlp.0.weight\n",
      "blocks.4.mlp.0.bias\n",
      "blocks.4.mlp.2.weight\n",
      "blocks.4.mlp.2.bias\n",
      "blocks.5.ln1.weight\n",
      "blocks.5.ln1.bias\n",
      "blocks.5.attn.attn_lin.weight\n",
      "blocks.5.attn.attn_lin.bias\n",
      "blocks.5.attn.out_lin.weight\n",
      "blocks.5.attn.out_lin.bias\n",
      "blocks.5.ln2.weight\n",
      "blocks.5.ln2.bias\n",
      "blocks.5.mlp.0.weight\n",
      "blocks.5.mlp.0.bias\n",
      "blocks.5.mlp.2.weight\n",
      "blocks.5.mlp.2.bias\n",
      "blocks.6.ln1.weight\n",
      "blocks.6.ln1.bias\n",
      "blocks.6.attn.attn_lin.weight\n",
      "blocks.6.attn.attn_lin.bias\n",
      "blocks.6.attn.out_lin.weight\n",
      "blocks.6.attn.out_lin.bias\n",
      "blocks.6.ln2.weight\n",
      "blocks.6.ln2.bias\n",
      "blocks.6.mlp.0.weight\n",
      "blocks.6.mlp.0.bias\n",
      "blocks.6.mlp.2.weight\n",
      "blocks.6.mlp.2.bias\n",
      "blocks.7.ln1.weight\n",
      "blocks.7.ln1.bias\n",
      "blocks.7.attn.attn_lin.weight\n",
      "blocks.7.attn.attn_lin.bias\n",
      "blocks.7.attn.out_lin.weight\n",
      "blocks.7.attn.out_lin.bias\n",
      "blocks.7.ln2.weight\n",
      "blocks.7.ln2.bias\n",
      "blocks.7.mlp.0.weight\n",
      "blocks.7.mlp.0.bias\n",
      "blocks.7.mlp.2.weight\n",
      "blocks.7.mlp.2.bias\n",
      "blocks.8.ln1.weight\n",
      "blocks.8.ln1.bias\n",
      "blocks.8.attn.attn_lin.weight\n",
      "blocks.8.attn.attn_lin.bias\n",
      "blocks.8.attn.out_lin.weight\n",
      "blocks.8.attn.out_lin.bias\n",
      "blocks.8.ln2.weight\n",
      "blocks.8.ln2.bias\n",
      "blocks.8.mlp.0.weight\n",
      "blocks.8.mlp.0.bias\n",
      "blocks.8.mlp.2.weight\n",
      "blocks.8.mlp.2.bias\n",
      "blocks.9.ln1.weight\n",
      "blocks.9.ln1.bias\n",
      "blocks.9.attn.attn_lin.weight\n",
      "blocks.9.attn.attn_lin.bias\n",
      "blocks.9.attn.out_lin.weight\n",
      "blocks.9.attn.out_lin.bias\n",
      "blocks.9.ln2.weight\n",
      "blocks.9.ln2.bias\n",
      "blocks.9.mlp.0.weight\n",
      "blocks.9.mlp.0.bias\n",
      "blocks.9.mlp.2.weight\n",
      "blocks.9.mlp.2.bias\n",
      "blocks.10.ln1.weight\n",
      "blocks.10.ln1.bias\n",
      "blocks.10.attn.attn_lin.weight\n",
      "blocks.10.attn.attn_lin.bias\n",
      "blocks.10.attn.out_lin.weight\n",
      "blocks.10.attn.out_lin.bias\n",
      "blocks.10.ln2.weight\n",
      "blocks.10.ln2.bias\n",
      "blocks.10.mlp.0.weight\n",
      "blocks.10.mlp.0.bias\n",
      "blocks.10.mlp.2.weight\n",
      "blocks.10.mlp.2.bias\n",
      "blocks.11.ln1.weight\n",
      "blocks.11.ln1.bias\n",
      "blocks.11.attn.attn_lin.weight\n",
      "blocks.11.attn.attn_lin.bias\n",
      "blocks.11.attn.out_lin.weight\n",
      "blocks.11.attn.out_lin.bias\n",
      "blocks.11.ln2.weight\n",
      "blocks.11.ln2.bias\n",
      "blocks.11.mlp.0.weight\n",
      "blocks.11.mlp.0.bias\n",
      "blocks.11.mlp.2.weight\n",
      "blocks.11.mlp.2.bias\n",
      "blocks.12.weight\n",
      "blocks.12.bias\n"
     ]
    }
   ],
   "source": [
    "for k in their_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
