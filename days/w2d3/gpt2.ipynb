{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from typing import *\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import gpt_tests\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from math import sqrt\n",
    "import bert_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the GPT-2 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size/num_heads\n",
    "        self.attn_lin = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.out_lin = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        product = self.attn_lin(x)\n",
    "        seq_len = x.shape[1]\n",
    "        good_format = rearrange(product, 'b n (qkv h p) -> qkv b h n p', qkv = 3, h = self.num_heads)\n",
    "        queries, keys, values = good_format[0], good_format[1], good_format[2]\n",
    "        attn_score = t.einsum('bhfp,bhtp -> bhft', keys, queries) / sqrt(self.head_size)\n",
    "        \n",
    "        arange = t.arange(seq_len, device=x.device)\n",
    "        arange_rows = repeat(arange, 'a -> b a', b = seq_len)\n",
    "        arange_cols = repeat(arange, 'a -> a b', b = seq_len)\n",
    "        attn_score[:,:,arange_rows < arange_cols] = -1e4\n",
    "        \n",
    "        attn_pattn = t.softmax(attn_score, dim=-2)\n",
    "        # attn_pattn: b h n n; values: b h n p\n",
    "        out_by_head = t.einsum('bhft,bhfp->bhtp', attn_pattn, values)\n",
    "        out = rearrange(out_by_head, 'b h t p -> b t (h p)') # b n hidden_size\n",
    "        return self.out_lin(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_unidirectional_attn(MultiHeadedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size*4, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = self.attn(x1) + x\n",
    "        x3 = self.ln2(x2)\n",
    "        return self.mlp(x3) + x2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt_block(GPT2Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        \n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        final_encoding = encoding[:,-1]\n",
    "        \n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        \n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_replace(s):\n",
    "    s = s.replace(\"embedding.weight\", \"embedding\")\n",
    "    s = s.replace(\"linear1\", \"mlp.0\")\n",
    "    s = s.replace(\"linear2\", \"mlp.2\")\n",
    "    s = s.replace(\"ln.\", \"blocks.12.\")\n",
    "    return s\n",
    "\n",
    "their_dict = pretrained_gpt.state_dict()\n",
    "for k in list(their_dict.keys()):\n",
    "    their_dict[string_replace(k)] = their_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    top_logit_idxs = t.argsort(logits, descending=True)[0,:top_k]\n",
    "    top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "    print(top_logit_words)\n",
    "    print(probs[0,top_logit_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " experience learning opportunity program process course training time work challenge\n",
      "tensor([0.1915, 0.0389, 0.0338, 0.0290, 0.0174, 0.0165, 0.0162, 0.0151, 0.0129,\n",
      "        0.0129], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pretrained_gpt.cpu()\n",
    "feed_gpt(pretrained_gpt, \"Students at the machine learning bootcamp really enjoyed the\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_gpt_top(model: nn.Module, input_ids: List[int], top_k: int = 10):\n",
    "    logits = model(t.tensor([input_ids], dtype=t.long)).logits\n",
    "    probs = t.softmax(logits, dim=-1)\n",
    "    return t.argsort(logits, descending=True)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that  the  machine  learning  boot camp  was  actually  a  very  bad  idea . \n",
      " \n",
      " I  was  not  alone .  I  was  also  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my  own  experience . \n",
      " \n",
      " I  was  not  alone  in  my  own  experience .  I  was  also  not  alone  in  my "
     ]
    }
   ],
   "source": [
    "start_str = \"The machine learning bootcamp started out nicely. But soon, I got an ominous feeling. Shockingly, I discovered\"\n",
    "input_ids = tokenizer(start_str)[\"input_ids\"]\n",
    "for i in range(100):\n",
    "    new_token = feed_gpt_top(my_gpt, input_ids, 1)\n",
    "    print(tokenizer.decode(new_token), end = \" \")\n",
    "    input_ids.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Modified(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)],\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        embedding = self.token_embedding[x] + self.pos_embedding[:seq_len]\n",
    "        encoding = self.blocks(self.dropout(embedding))\n",
    "        self.encoding = encoding\n",
    "        final_encoding = encoding[:,-1]\n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt_modified = GPT2Modified(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt_modified.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_thing(tokenizer):\n",
    "    s = 'My life motto: Fortune favors the bold'\n",
    "    ids = tokenizer(s)[\"input_ids\"]\n",
    "    return [ids[:i]+[0]*(10-i) for i in range(4,9)]\n",
    "\n",
    "test_batch = t.tensor(create_padded_thing(tokenizer))\n",
    "bert_batch = t.tensor(create_padded_thing(transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")))\n",
    "\n",
    "def decode_batch(batch, tokenizer):\n",
    "    print([tokenizer.decode(batch[i]) for i in range(len(batch))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.0323, -0.3814],\n",
       "         [-0.0505, -0.2386],\n",
       "         [ 0.0306, -0.1637],\n",
       "         [ 0.0751, -0.1712],\n",
       "         [ 0.0916, -0.1977],\n",
       "         [ 0.1046, -0.2181]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.3443, -0.1450],\n",
       "         [ 0.2800, -0.1220],\n",
       "         [ 0.2185, -0.1960],\n",
       "         [ 0.2100, -0.1533],\n",
       "         [ 0.2249, -0.1559]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [ 0.4627, -0.2614],\n",
       "         [ 0.3536, -0.2568],\n",
       "         [ 0.1176, -0.2094],\n",
       "         [ 0.1202, -0.1308]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [-0.0704,  0.0556],\n",
       "         [ 0.1992, -0.2992],\n",
       "         [ 0.1801, -0.1881],\n",
       "         [-0.2132, -0.0607]],\n",
       "\n",
       "        [[-0.0340, -0.0429],\n",
       "         [ 0.2075,  0.3059],\n",
       "         [-0.0342, -0.2292],\n",
       "         [ 0.3671, -0.0707],\n",
       "         [-0.1335,  0.9349],\n",
       "         [ 0.1363,  0.0811],\n",
       "         [-0.0704,  0.0556],\n",
       "         [-0.6858, -0.9147],\n",
       "         [ 0.3317, -0.2639],\n",
       "         [ 0.0736, -0.2989]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt_modified.eval()\n",
    "my_gpt_modified(test_batch);\n",
    "my_gpt_modified.encoding[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bert = bert_sol.Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "my_bert.eval()\n",
    "my_bert(bert_batch);\n",
    "my_bert._enc[:,:,:2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMultiHeadedAttention(Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size/num_heads\n",
    "        self.attn_lin = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.out_lin = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, past_key_values=None, return_key_values=False):\n",
    "        product = self.attn_lin(x)\n",
    "        seq_len = x.shape[1]\n",
    "        good_format = rearrange(product, 'b n (qkv h p) -> qkv b h n p', qkv = 3, h = self.num_heads)\n",
    "        queries, keys, values = good_format[0], good_format[1], good_format[2]\n",
    "\n",
    "        if past_key_values != None:\n",
    "            # x has shape [b=1, n=1, hidden_size]\n",
    "            past_keys, past_values = t.split(past_key_values, past_key_values.shape[-1]//2, dim=-1)\n",
    "            keys = t.cat((t.unsqueeze(past_keys, dim=0), keys), dim=2)\n",
    "            values = t.cat((t.unsqueeze(past_values, dim = 0), values), dim=2)            \n",
    "        \n",
    "        attn_score = t.einsum('bhfp,bhtp -> bhft', keys, queries) / sqrt(self.head_size)\n",
    "        \n",
    "        if past_key_values is None:\n",
    "            arange = t.arange(seq_len, device=x.device)\n",
    "            arange_rows = repeat(arange, 'a -> b a', b = seq_len)\n",
    "            arange_cols = repeat(arange, 'a -> a b', b = seq_len)\n",
    "            attn_score[:,:,arange_rows < arange_cols] = -1e4\n",
    "        \n",
    "        attn_pattn = t.softmax(attn_score, dim=-2)\n",
    "        # attn_pattn: b h n n; values: b h n p\n",
    "        out_by_head = t.einsum('bhft,bhfp->bhtp', attn_pattn, values)\n",
    "        out = rearrange(out_by_head, 'b h t p -> b t (h p)') # b n hidden_size\n",
    "\n",
    "        encoding = self.out_lin(out)\n",
    "        if return_key_values:\n",
    "            if past_key_values is None:\n",
    "                return encoding, t.cat((keys, values), dim=-1)\n",
    "            return encoding, t.cat((keys[:,:,-1:], values[:,:,-1:]), dim=-1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking encoding:\n",
      "Congrats! You've passed the test!\n",
      "Checking new key and value:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_attn_cache(FastMultiHeadedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGPT2Block(Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = FastMultiHeadedAttention(hidden_size, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size*4, hidden_size))\n",
    "        \n",
    "    def forward(self, x, past_key_values=None, return_key_values=False):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = self.attn(x1, past_key_values, return_key_values)\n",
    "        if return_key_values:\n",
    "            x3 = x2[0] + x\n",
    "        else:\n",
    "            x3 = x2 + x\n",
    "        x4 = self.ln2(x3)\n",
    "        if return_key_values:\n",
    "            return self.mlp(x4) + x3, x2[1]\n",
    "        return self.mlp(x4) + x3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class FastGPT2(Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size,\n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon, use_cache=False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.use_cache = use_cache\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
    "        self.pos_embedding = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
    "        self.clear_cache()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [FastGPT2Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "              for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        \n",
    "    def clear_cache(self):\n",
    "        self.cache = t.zeros(self.num_layers, self.num_heads, 0, 2*self.hidden_size//self.num_heads)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        pre_seq_len = x.shape[1]\n",
    "\n",
    "        if self.cache.shape[-2] != 0:\n",
    "            x = x[:,-1:]\n",
    "            pos_embedding = self.pos_embedding[pre_seq_len-1:pre_seq_len]\n",
    "        else:\n",
    "            pos_embedding = self.pos_embedding[:pre_seq_len]\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        encoding = self.token_embedding[x] + pos_embedding\n",
    "        encoding = self.dropout(encoding)\n",
    "\n",
    "        if self.use_cache:\n",
    "            if self.cache.shape[-2] == 0:\n",
    "                new_cache = t.zeros(self.num_layers, self.num_heads, seq_len, 2*self.hidden_size//self.num_heads)\n",
    "                for i,block in enumerate(self.blocks):\n",
    "                    encoding, new_cache[i] = block(encoding, past_key_values=None, return_key_values=True)\n",
    "            else:\n",
    "                new_cache = t.zeros(self.num_layers, self.num_heads, 1, 2*self.hidden_size//self.num_heads)\n",
    "                for i,block in enumerate(self.blocks):\n",
    "                    encoding, new_cache[i] = block(encoding, past_key_values=self.cache[i], return_key_values=True)\n",
    "            self.cache = t.cat((self.cache, new_cache), dim=-2) #wip\n",
    "        else:\n",
    "            for i,block in enumerate(self.blocks):\n",
    "                encoding = block(encoding)\n",
    "\n",
    "        final_encoding = self.layernorm(encoding)[:,-1]\n",
    "        \n",
    "        logits = t.einsum('vc,bc->bv', self.token_embedding, final_encoding)\n",
    "        \n",
    "        return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
    "    \n",
    "    def next_token(self, input_ids, temperature, freq_penalty=2.0):\n",
    "        logits = self.forward(t.tensor(input_ids).reshape(1,len(input_ids))).logits\n",
    "\n",
    "        freqs = t.zeros(self.vocab_size)\n",
    "        for i in input_ids:\n",
    "            freqs[i] += 1\n",
    "\n",
    "        probs = t.softmax(logits/temperature - freqs*freq_penalty, dim=1)\n",
    "        return np.random.choice(probs.flatten().shape[0], p=probs.flatten().detach().numpy())\n",
    "\n",
    "    def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
    "        ids = self.tokenizer(text)['input_ids']\n",
    "        self.clear_cache()\n",
    "        while len(ids) < max_length:\n",
    "            new_token = self.next_token(ids, temperature, freq_penalty)\n",
    "            ids.append(new_token)\n",
    "        print(self.tokenizer.decode(ids))\n",
    "        return ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 2.351s to generate a 500-token sentence without cache and 0.756s with cache.\n"
     ]
    }
   ],
   "source": [
    "# gpt_tests.test_gpt_block(GPT2Block)\n",
    "# gpt_tests.test_gpt(FastGPT2)\n",
    "gpt_tests.test_gpt_cache(FastGPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = FastGPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768,\n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, use_cache=True)\n",
    "# pretrained_gpt = gpt_tests.get_pretrained_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_replace(s):\n",
    "    s = s.replace(\"embedding.weight\", \"embedding\")\n",
    "    s = s.replace(\"linear1\", \"mlp.0\")\n",
    "    s = s.replace(\"linear2\", \"mlp.2\")\n",
    "    s = s.replace(\"ln.\", \"layernorm.\")\n",
    "    return s\n",
    "\n",
    "their_dict = pretrained_gpt.state_dict()\n",
    "for k in list(their_dict.keys()):\n",
    "    their_dict[string_replace(k)] = their_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.load_state_dict(their_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite emoticon is: Michael Jackson ANT\n",
      "\n",
      "I like the NFL as a whole since it's hipsters vs. men, but @\n"
     ]
    }
   ],
   "source": [
    "my_gpt.eval()\n",
    "my_gpt.generate('My favorite emoticon is:', freq_penalty=2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
