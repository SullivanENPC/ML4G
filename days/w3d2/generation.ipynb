{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch as t\n",
    "import transformers\n",
    "\n",
    "from w3d2_tests import get_minigpt\n",
    "import generation as gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "minigpt = get_minigpt(\"model.pt\")\n",
    "minigpt.eval()\n",
    "minigpt.to(\"cuda:3\")\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer._add_tokens([\"[BEGIN]\", \"[END]\"])\n",
    "tokenizer.pad_token = \"[END]\"\n",
    "tokenizer.eos_token = \"[END]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGIN] So far, we've developed a theoretical model for understanding two-layer attention-only models. We have an overall equation describing the logits (the OV circuit), and then an equation describing how each attention head's attention pattern is computed (the QK circuit). But how do we understand them in practice? In this section, we'll ircle of the current model.\n",
      "\n",
      "The model is based on the model of the model, which is based on a model of the model.\n"
     ]
    }
   ],
   "source": [
    "# PROMPT = \"When I grow up I want to be a\"\n",
    "PROMPT = \"So far, we've developed a theoretical model for understanding two-layer attention-only models. We have an overall equation describing the logits (the OV circuit), and then an equation describing how each attention head's attention pattern is computed (the QK circuit). But how do we understand them in practice? In this section, we'll \" #reverse engineer a single two-layer model.\"\n",
    "# PROMPT = \"and and and and and \"\n",
    "# PROMPT = \"[BEGIN] Fathom Events and Warner Bros. Entertainment bring the beloved film, The NeverEnding Story back to select cinemas nationwide for a special two-day event on Sunday, September 4\"\n",
    "# t.manual_seed(12)\n",
    "# PROMPT = \"[END] [END] [END] [END] [END] [END]\"\n",
    "\n",
    "print(\n",
    "    gen.gen_completion(\n",
    "        model=minigpt,\n",
    "        tokenizer=tokenizer,\n",
    "        base_text=PROMPT,\n",
    "        n_tokens_to_gen=30,\n",
    "        temperature=0.2,\n",
    "        freq_penalty=1.0,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
