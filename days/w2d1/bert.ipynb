{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ = W_Q @ input\\nK = W_K @ input\\nV = W_V @ input\\nattn_pat = normalised_softmax(Q @ K^T)\\nattention = attn_patn @ V\\nO = W_O @ attention\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q = W_Q @ input\n",
    "K = W_K @ input\n",
    "V = W_V @ input\n",
    "attn_pat = normalised_softmax(Q @ K^T)\n",
    "attention = attn_patn @ V\n",
    "O = W_O @ attention\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.01464 STD: 0.1089 VALS [-0.0608 0.1288 -0.03011 -0.07367 0.1024 0.0883 -0.1601 0.03394 -0.2108 0.03387...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    dk = t.tensor(64) # num heads\n",
    "    # print(token_activations.shape) # [batch_size, (num_heads), input_length, hidden_size]\n",
    "    Q = project_query(token_activations) # W_Q: [hidden_size, num_heads * head_size], [batch_size, input_length, num_heads * head_size]\n",
    "    K = project_key(token_activations)\n",
    "    # print(project_query, project_key)\n",
    "    # print(Q.shape, K.shape)\n",
    "    Q = rearrange(Q, \"... n (h s) -> ... h n s\", h = num_heads)\n",
    "    K = rearrange(K, \"... n (h s) -> ... h n s\", h = num_heads)\n",
    "    # res = t.einsum('...qc,...kc -> ...kq', Q, K)/t.sqrt(dk)\n",
    "    res = K.matmul(rearrange(Q, '... q c -> ... c q'))/t.sqrt(dk)\n",
    "    # print(res.shape)\n",
    "    return res\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.0002383 STD: 0.1209 VALS [-0.03247 0.1209 -0.2009 0.01448 0.04943 -0.0147 0.2965 0.02802 0.07579 -0.1073...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    softmaxed = t.nn.functional.softmax(attention_pattern, dim=-2)\n",
    "    V = project_value(token_activations)\n",
    "    V = rearrange(V, \"... n (h s) -> ... h n s\", h=num_heads)\n",
    "    #print(V.shape, softmaxed.shape)\n",
    "    #print((softmaxed @ V).shape)\n",
    "    res = project_output(rearrange(t.einsum(\"...htf,...hts->...hfs\", softmaxed, V), \"... h n s -> ... n (h s)\"))\n",
    "    #print(res.shape)\n",
    "    return res\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pattern = nn.ModuleDict({\n",
    "            'project_query': nn.Linear(hidden_size, hidden_size),\n",
    "            'project_key': nn.Linear(hidden_size, hidden_size)\n",
    "        })\n",
    "        self.project_value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_out = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape, self.num_heads, self.hidden_size)\n",
    "        attn_pattern = raw_attention_pattern(x, self.num_heads, self.pattern['project_query'], self.pattern['project_key'])\n",
    "        return bert_attention(x, self.num_heads, attn_pattern, self.project_value, self.project_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.0006734 STD: 0.1067 VALS [-0.006726 0.07883 -0.07867 -0.1464 -0.1546 0.00825 0.1647 0.08232 -0.01181 0.02754...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    return linear_2(t.nn.functional.gelu(linear_1(token_activations)))\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Linear(input_size, intermediate_size)\n",
    "        self.mlp2 = nn.Linear(intermediate_size, input_size)\n",
    "        self.layer_norm = LayerNorm(input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_norm(bert_mlp(x, self.mlp1, self.mlp2) + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: 0 STD: 1.003 VALS [-0.9841 -0.5669 -0.7758 1.43 -1.739 0.3949 0.09413 0.895 -0.1765 1.429...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    EPS = 1e-5\n",
    "    def __init__(self, normalized_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(t.ones(normalized_dim))\n",
    "        self.bias = nn.Parameter(t.zeros(normalized_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = x - x.mean(dim=-1, keepdim=True).detach()\n",
    "        x = x/(x.var(dim=-1, unbiased=False, keepdim=True).detach() + self.EPS).sqrt()\n",
    "        # print(self.weight.shape, self.bias.shape, x.shape)\n",
    "        # print(self.weight)\n",
    "        # print(self.bias)\n",
    "        # print(x)\n",
    "        # print(t.einsum('...i,i->...i', x, self.weight) + self.bias)\n",
    "        return t.einsum('...i,i->...i', x, self.weight) + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -2.897e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size : int, intermediate_size : int, num_heads : int, dropout : float):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.residual = BertMLP(hidden_size, intermediate_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        y = self.layer_norm(y + x)\n",
    "        z = self.residual(y)\n",
    "        z = self.dropout(z)\n",
    "        return z\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.weight = nn.Parameter(t.randn(vocab_size, embed_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight[input]\n",
    "\n",
    "#bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):\n",
    "    embeddings = token_embedding(input_ids)\n",
    "    embeddings += position_embedding(t.arange(0, input_ids.shape[-1]).to(input_ids.device))\n",
    "    embeddings += token_type_embedding(token_type_ids)\n",
    "    return dropout(layer_norm(embeddings))\n",
    "\n",
    "#bert_tests.test_bert_embedding_fn(bert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_position_embeddings: int, type_vocab_size: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "\n",
    "#bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "max_position_embeddings: int, type_vocab_size: int, \n",
    "dropout: float, intermediate_size: int, num_heads: int, \n",
    "num_layers: int\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.ModuleDict({\n",
    "            'mlp': nn.Linear(hidden_size, hidden_size),\n",
    "            'gelu': nn.GELU(),\n",
    "            'unembedding': nn.Linear(hidden_size, vocab_size),\n",
    "            'layer_norm': LayerNorm(hidden_size),\n",
    "        })\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids, t.zeros_like(input_ids))\n",
    "        x = self.transformer(x)\n",
    "        x = self.lm_head['mlp'](x)\n",
    "        x = self.lm_head['gelu'](x)\n",
    "        x = self.lm_head['layer_norm'](x)\n",
    "        x = self.lm_head['unembedding'](x)\n",
    "        return x\n",
    "\n",
    "# bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# my_bert = Bert(\n",
    "#     vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "#     type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "#     num_heads=12, num_layers=12\n",
    "# )\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_state_dict = pretrained_bert.state_dict()\n",
    "# del pretrained_state_dict['classification_head.weight']\n",
    "# del pretrained_state_dict['classification_head.bias']\n",
    "# my_bert.load_state_dict(pretrained_state_dict)\n",
    "# bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] colleges 天 largest happened smile donation [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "encoded = uncased_tokenizer.encode(\"Hi, my name is bert\")\n",
    "tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    mask_encoding = tokenizer.encode('[MASK]')[1]\n",
    "\n",
    "    my_bert.eval()\n",
    "\n",
    "    encoding = t.tensor(tokenizer.encode(sentence))\n",
    "\n",
    "    logits = my_bert(encoding)[encoding == mask_encoding]\n",
    "\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    probs, word_indices = probs.sort(descending=True, dim=-1)\n",
    "\n",
    "    probs = probs[:, :10]\n",
    "    word_indices = word_indices[:, :10]\n",
    "\n",
    "    words = [[tokenizer.decode(word) for word in word_options] for word_options in word_indices]\n",
    "    if len(words) > 1:\n",
    "        print(\"please don't double mask\")\n",
    "    words_with_probs = zip(words[0], probs[0])\n",
    "    \n",
    "    sentence_for_display = sentence.replace('[MASK]', '---')\n",
    "    print(sentence_for_display)\n",
    "    for word, prob in words_with_probs:\n",
    "        print(f\"%{sentence_for_display.index('---') - 2}.1d%% %s\" % (prob * 100, word))\n",
    "# ascii_art_probs(\"The fish loves to eat [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBert(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, \n",
    "max_position_embeddings: int, type_vocab_size: int, \n",
    "dropout: float, intermediate_size: int, num_heads: int, \n",
    "num_layers: int, num_classes : int\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "        )\n",
    "        self.lm_head = nn.ModuleDict({\n",
    "            'mlp': nn.Linear(hidden_size, hidden_size),\n",
    "            'gelu': nn.GELU(),\n",
    "            'unembedding': nn.Linear(hidden_size, vocab_size),\n",
    "            'layer_norm': LayerNorm(hidden_size),\n",
    "        })\n",
    "        self.classification_head = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids, t.zeros_like(input_ids))\n",
    "        x = self.transformer(x)\n",
    "        # x = self.lm_head['mlp'](x)\n",
    "        # x = self.lm_head['gelu'](x)\n",
    "        # x = self.lm_head['layer_norm'](x)\n",
    "        # x = self.lm_head['unembedding'](x)\n",
    "        x = self.classification_head(self.dropout(x))\n",
    "        x = t.nn.functional.softmax(x, dim=-1)\n",
    "        return x[:,0,1]\n",
    "\n",
    "#bert_tests.test_bert(CBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchtext\n",
    "# data_train, data_test = torchtext.datasets.IMDB(\n",
    "#     root='.data',\n",
    "#     split=('train', 'test')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x- shuffle\n",
    "- pad\n",
    "- truncate\n",
    "- tokenize\n",
    "- convert neg/pos to 0/1\n",
    "- \n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "def pads_sequences(dataset, max_seq_len, batch_size=64):\n",
    "    all_data = [data for data in dataset]\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    batched_data = []\n",
    "    for i in range(0, len(all_data), batch_size):\n",
    "        batch = all_data[i:min(len(all_data), i+batch_size)]\n",
    "        classifications, sentences = zip(*batch)\n",
    "\n",
    "        classifications = [1 if classification == 'pos' else 0 for classification in classifications]\n",
    "        # print(classifications)\n",
    "        # print(sentences)\n",
    "        sentences = tokenizer(list(sentences), padding=\"longest\", max_length=max_seq_len, truncation=True)\n",
    "        # print(sentences)\n",
    "\n",
    "        batched_data.append((classifications, sentences))\n",
    "    return batched_data\n",
    "\n",
    "# processed_train_data = pads_sequences(data_train, 512, batch_size=32)\n",
    "# processed_test_data = pads_sequences(data_test, 512, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, data_train, data_test, n_iter=500, epochs=1):\n",
    "    perf_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "\n",
    "    loss_fn = t.nn.BCELoss()\n",
    "    optimizer = t.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        n = 0\n",
    "        model.train()\n",
    "        for batch in tqdm(data_train):\n",
    "            n += 1\n",
    "            if n > n_iter:\n",
    "                break\n",
    "            print(n)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(t.tensor(batch[1]['input_ids']).to('cuda'))\n",
    "            loss = loss_fn(out, t.tensor(batch[0]).float().to('cuda'))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            perf_history.append(loss.detach().clone())\n",
    "\n",
    "\n",
    "        with t.no_grad():\n",
    "            model.test()\n",
    "            test_loss = 0\n",
    "            for batch in data_test:\n",
    "                test_out = model(t.tensor(batch[1]['input_ids']).to('cuda'))\n",
    "                test_loss += loss_fn(test_out, t.tensor(batch[0]).clone().detach().float().to('cuda'))\n",
    "            test_loss_history.append(test_loss / len(data_test))\n",
    "\n",
    "    return perf_history, test_loss_history\n",
    "\n",
    "# cbert = CBert(\n",
    "#     vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "#     type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "#     num_heads=12, num_layers=12, num_classes=2).to('cuda')\n",
    "# pretrained_state_dict = pretrained_bert.state_dict()\n",
    "# cbert.load_state_dict(pretrained_state_dict)\n",
    "# perf_history, test_loss_history = train(cbert, processed_train_data, processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(perf_history)\n",
    "# plt.plot(test_loss_history*len(perf_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = processed_test_data[47]\n",
    "\n",
    "# input = t.tensor(batch[1]['input_ids']).to('cuda')\n",
    "# target = batch[0]\n",
    "# output = cbert(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0.3053981363773346 | Target: 1   INCORRECT\n",
      "[CLS] If all movies had to be destroyed and only one could be spared, Death in Venice would have to be it. It is a monument in movie history. Much criticized for being slow, boring and too obvious in stating it's point ( an old man discovers beauty in a young boy and is tragically destroyed, first mentally, then physically ), we should appreciate this movie for what it is.'Morte a Venezia'was shot over 30 years ago, and it portrays a period even further back, at the beginning of the twentieth century. < br / > < br / > Life was slow then, compared to now. People were supposed to behave in a certain way, hiding their true emotions even from themselves. Director Visconti and Dirk Bogarde, the leading actor, admirably succeed in showing how the aging composer Von Aschenbach discovers his romantic interest in a young boy. For a man like Von Aschenbach, in his time, this must have been a shock too powerful to come to terms with. We see his inner struggle, mostly on the face of Bogarde, against the beautiful backdrop of Venice and accompanied by the most wonderful music, composed by Gustav Mahler. < br / > < br / > This movie is slow, there is no denying it. No special effects, car chases or fights to keep the audience pinned to their seats. No perverted sex scenes either ; the interaction between man and boy is limited to stolen glances from afar and the occasional smile. < br / > < br / > So, basically, nothing much happens in this movie? Not if you want your senses to be hit like a base drum. If you want them to be played like the strings of a violin in a romantic concerto, this is the movie to do it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted: 0.6054341197013855 | Target: 0   INCORRECT\n",
      "[CLS] Carole Lombard stars in this transition period film. This film is a typical example of a very early \" talkie \" ( First practical sound film was \" The Jazz Singer \", 1927 ). Overall, the acting in this film tends to be extremely broad and very melodramatic. < br / > < br / > The viewer may easily note that the actors are still \" acting \" for a silent film, and this combined with the overly pronounced, overly earnest dialog ( It seems most likely a diction - elocution - drama coach was employed extensively to teach the \" silent \" actors to speak lines ), creates some rather comical scenes which were not at all intended to be comical. < br / > < br / > Carole Lombard's later great acting ability is all but unrecognizable underneath all the broad gestures, melodrama, and eager earnestness. < br / > < br / > Mainly interesting as an historical curiosity of the period, and for it's completely unintended comedy - camp value. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted: 0.5140627026557922 | Target: 0   INCORRECT\n",
      "[CLS] Having had more than a few mates suggest i check Rise of the Footsoldier out, i eventually got round to it last night. Undoubtedly the story Colton Leach has to tell ( and did so in his autobiography ) is a compelling tale of one mans ascent from Terrace boot boy to connected underworld villain. This film sadly compromised in quality by miscasts, appalling accents and woeful acting. Ricci Harnett in the lead role of Leach does a reasonable job of conveying the transition from thuggery to serious criminal but his accent is all over the place. As his voice provides the stories narration it is something that after ten minutes was driving me nuts. Terry Stone as Tony Tucker provided the unintentional comedy with an ill fitting wig ( or the worst Barnett going ) dialogue that was so expletive riddled it bordered on juvenile and an over the top vehemence in line delivery reducing Tucker to parody. What troubled me most about this film was that the events leading up to the shooting in Rettenden, Essex and the formative years of Leach are of genuine interest to crime fans and fans of football hooliganism so, to have this story sabotaged by a lack of credible accents and acting left me feeling an opportunity had been missed. Roland Manookian and Frank Harper provide the films only source of authenticity. On the positive side some of the films pacing and construct flowed well and kept the attention. The violence was well choreographed and aside from an over reliance on projectile red syrup for blood spatterings was on the whole realistic. If you enjoyed the film then it is worth checking out Essex Boys telling a similar tale from fictionalised viewpoint and also featuring Billy Murray. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted: 0.5042034387588501 | Target: 0   INCORRECT\n",
      "[CLS] A couple of clarifying comments are in order. Herschell Gordon Lewis contributed a brief introduction to the video release of DOCTOR GORE ( aka THE BODY SHOP ), wherein he touched upon his collaborative efforts with J. G. \" Pat \" Patterson, director and star of DOCTOR GORE. Patterson concocted the \" gore effects \" for THE GRUESOME TWOSOME and a few other Lewis movies in the late 60s. Lewis remarks that whereas 2, 000 MANIACS was a \" five gallon \" film ( referring to the amount of stage blood required ), the Lewis - Patterson productions were \" fifteen gallon \" pictures. Lewis does not describe DOCTOR GORE as a \" fifteen gallon \" film - - he's only talking about the films he & Patterson made together. Lewis has confessed ( elsewhere ) that his introduction to DOCTOR GORE was improvised before he'd even seen Patterson's film! So take it with a grain of salt. < br / > < br / > This may be an \" unfinished \" film, but like some unfinished novels it does have an \" ending. \" It's just missing some connective tissue. < br / > < br / > Patterson has definite stage presence & a dry sense of humor, helping to make this simplistic show somewhat more watchable than it should be. There's an extremely bare - bones plot - - even BLOOD FEAST is more complex - - and a gratingly repetitive musical score by William Girdler. A bit of nudity & lots of skin. The entire middle section of the film involves the construction of a \" perfect woman ; \" this is concentrated gore for the bloodthirsty, and laughable. < br / > < br / > Patterson the director is in way over his head, but he tries hard to tell his story creatively, if it's possible to use Frankenstein clichés creatively. But the best reason to see this film ( on Something Weird's DVD, if possible ) is that it features a perfect Nashville weeper, Bill Hicks'\" A Heart Dies Every Minute. \" Ain't it the truth! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted: 0.13543471693992615 | Target: 1   INCORRECT\n",
      "[CLS] Reality before reality TV? Copy of \" Fast Times at Ridgemont High \"? A precursor to \" Say Anything \" that's grittier? I can't decide, but the soundtrack * is * the 80's - - Blondie, Journey, REO Speedwagon, Devo, Lionel Richie, AND U2 - - I can't believe this, they would never throw all those genres together in a teen movie of today. < br / > < br / > I remembered this like a teenager - - mainly the sex parts and not a hint of the altruism. Why? I was a horny teenager in the 80's. Watching it again, I just can't describe how much I love that Rose, play by Kimberly Richardson, turns out to be the voice of \" Pepper Ann \" in the 90's, and she was almost 30 when she was in Last American Virgin, playing alongside 16 year - olds - - fantastic! Complete cheese, reality, fantasy, and comedy - - with a sincere cherry on top. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# out = list(output)\n",
    "# for i in range(len(out)):\n",
    "#     correct = (out[i] > 0.5) == (target[i] > 0.5)\n",
    "#     if not correct:\n",
    "#         print(f\"Predicted: {out[i]} | Target: {target[i]}\" + (\"\" if correct else \"   INCORRECT\"))\n",
    "#     if not correct:\n",
    "#         print(tokenizer.decode(batch[1][i].ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unused1] [unused6] [unused5]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([1, 6, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde = pretrained_bert.embedding.token_embedding.weight.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdue = pretrained_bert.lm_head.unembedding.weight.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc95c729ee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(stde.clone().detach(), stdue.clone().detach(), alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(std - stdue).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'CBert' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5b1322e5e9ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cbert.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'CBert' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "cbert = t.load('cbert.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import torchtext\n",
    "\n",
    "def pads_sequences(dataset, max_seq_len, batch_size=64):\n",
    "    all_data = [data for data in dataset]\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    batched_data = []\n",
    "    for i in range(0, len(all_data), batch_size):\n",
    "        batch = all_data[i:min(len(all_data), i+batch_size)]\n",
    "\n",
    "        sentences = tokenizer(list(batch), padding=\"longest\", max_length=max_seq_len, truncation=True)\n",
    "\n",
    "        batched_data.append( sentences['input_ids'])\n",
    "    return batched_data\n",
    "\n",
    "data_train, data_valid, data_test = torchtext.datasets.WikiText2()\n",
    "\n",
    "processed_train_data = pads_sequences(data_train, 512, batch_size=32)\n",
    "processed_test_data = pads_sequences(data_valid, 512, batch_size=32)\n",
    "processed_valid_data = pads_sequences(data_test, 512, batch_size=32)\n",
    "\n",
    "processed_train_data = [t.tensor(batch) for batch in processed_train_data]\n",
    "processed_test_data = [t.tensor(batch) for batch in processed_test_data]\n",
    "processed_valid_data = [t.tensor(batch) for batch in processed_valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dist = t.distributions.Bernoulli(0.15)\n",
    "mask_token = tokenizer.encode('[MASK]')[1]\n",
    "pad_token = tokenizer.encode('[PAD]')[1]\n",
    "\n",
    "def mask(example):\n",
    "    samples = dist.sample(example.shape).bool()\n",
    "    samples[example == pad_token] = False\n",
    "    correct_results = example.clone()\n",
    "    example[..., samples] = mask_token\n",
    "    return example, correct_results\n",
    "\n",
    "\n",
    "masked_train_data = [mask(batch) for batch in processed_train_data]\n",
    "masked_test_data = [mask(batch) for batch in processed_test_data]\n",
    "masked_valid_data = [mask(batch) for batch in processed_valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f07cd0e4c414ff9b76bc9224e01997e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: tensor(10.4522, device='cuda:0')\n",
      "train loss: tensor(9.8628, device='cuda:0')\n",
      "train loss: tensor(9.5307, device='cuda:0')\n",
      "train loss: tensor(9.2347, device='cuda:0')\n",
      "train loss: tensor(9.0300, device='cuda:0')\n",
      "train loss: tensor(8.7935, device='cuda:0')\n",
      "train loss: tensor(8.3899, device='cuda:0')\n",
      "train loss: tensor(8.1531, device='cuda:0')\n",
      "train loss: tensor(7.9743, device='cuda:0')\n",
      "train loss: tensor(7.7833, device='cuda:0')\n",
      "train loss: tensor(7.8750, device='cuda:0')\n",
      "train loss: tensor(7.7391, device='cuda:0')\n",
      "train loss: tensor(7.3487, device='cuda:0')\n",
      "train loss: tensor(7.6247, device='cuda:0')\n",
      "train loss: tensor(7.2639, device='cuda:0')\n",
      "train loss: tensor(7.7294, device='cuda:0')\n",
      "train loss: tensor(6.9170, device='cuda:0')\n",
      "train loss: tensor(8.1456, device='cuda:0')\n",
      "train loss: tensor(7.9316, device='cuda:0')\n",
      "train loss: tensor(8.0964, device='cuda:0')\n",
      "train loss: tensor(7.5393, device='cuda:0')\n",
      "train loss: tensor(7.4387, device='cuda:0')\n",
      "train loss: tensor(7.4651, device='cuda:0')\n",
      "train loss: tensor(7.0538, device='cuda:0')\n",
      "train loss: tensor(7.7850, device='cuda:0')\n",
      "train loss: tensor(7.5559, device='cuda:0')\n",
      "train loss: tensor(12.7153, device='cuda:0')\n",
      "train loss: tensor(8.3181, device='cuda:0')\n",
      "train loss: tensor(7.7136, device='cuda:0')\n",
      "train loss: tensor(7.6046, device='cuda:0')\n",
      "train loss: tensor(7.5948, device='cuda:0')\n",
      "train loss: tensor(7.7018, device='cuda:0')\n",
      "train loss: tensor(7.9035, device='cuda:0')\n",
      "train loss: tensor(7.3845, device='cuda:0')\n",
      "train loss: tensor(7.7940, device='cuda:0')\n",
      "train loss: tensor(7.0982, device='cuda:0')\n",
      "train loss: tensor(7.1220, device='cuda:0')\n",
      "train loss: tensor(7.5165, device='cuda:0')\n",
      "train loss: tensor(7.8386, device='cuda:0')\n",
      "train loss: tensor(7.3880, device='cuda:0')\n",
      "train loss: tensor(7.5254, device='cuda:0')\n",
      "train loss: tensor(7.4716, device='cuda:0')\n",
      "train loss: tensor(7.3351, device='cuda:0')\n",
      "train loss: tensor(7.7930, device='cuda:0')\n",
      "train loss: tensor(7.1659, device='cuda:0')\n",
      "train loss: tensor(6.9613, device='cuda:0')\n",
      "train loss: tensor(7.3982, device='cuda:0')\n",
      "train loss: tensor(7.1671, device='cuda:0')\n",
      "train loss: tensor(7.4488, device='cuda:0')\n",
      "train loss: tensor(7.2267, device='cuda:0')\n",
      "train loss: tensor(7.3034, device='cuda:0')\n",
      "train loss: tensor(7.6760, device='cuda:0')\n",
      "train loss: tensor(7.3184, device='cuda:0')\n",
      "train loss: tensor(7.4888, device='cuda:0')\n",
      "train loss: tensor(7.4474, device='cuda:0')\n",
      "train loss: tensor(7.4195, device='cuda:0')\n",
      "train loss: tensor(7.6143, device='cuda:0')\n",
      "train loss: tensor(7.4966, device='cuda:0')\n",
      "train loss: tensor(7.1493, device='cuda:0')\n",
      "train loss: tensor(7.4132, device='cuda:0')\n",
      "train loss: tensor(7.2452, device='cuda:0')\n",
      "train loss: tensor(7.5960, device='cuda:0')\n",
      "train loss: tensor(7.1272, device='cuda:0')\n",
      "train loss: tensor(7.8114, device='cuda:0')\n",
      "train loss: tensor(8.0496, device='cuda:0')\n",
      "train loss: tensor(7.3472, device='cuda:0')\n",
      "train loss: tensor(7.2640, device='cuda:0')\n",
      "train loss: tensor(7.5002, device='cuda:0')\n",
      "train loss: tensor(8.6832, device='cuda:0')\n",
      "train loss: tensor(7.4291, device='cuda:0')\n",
      "train loss: tensor(7.6787, device='cuda:0')\n",
      "train loss: tensor(7.3672, device='cuda:0')\n",
      "train loss: tensor(7.1778, device='cuda:0')\n",
      "train loss: tensor(7.3218, device='cuda:0')\n",
      "train loss: tensor(7.3289, device='cuda:0')\n",
      "train loss: tensor(15.5611, device='cuda:0')\n",
      "train loss: tensor(7.2742, device='cuda:0')\n",
      "train loss: tensor(7.6806, device='cuda:0')\n",
      "train loss: tensor(7.4848, device='cuda:0')\n",
      "train loss: tensor(7.3903, device='cuda:0')\n",
      "train loss: tensor(7.5371, device='cuda:0')\n",
      "train loss: tensor(7.1126, device='cuda:0')\n",
      "train loss: tensor(7.3735, device='cuda:0')\n",
      "train loss: tensor(7.2901, device='cuda:0')\n",
      "train loss: tensor(7.1361, device='cuda:0')\n",
      "train loss: tensor(7.1300, device='cuda:0')\n",
      "train loss: tensor(7.2277, device='cuda:0')\n",
      "train loss: tensor(7.1166, device='cuda:0')\n",
      "train loss: tensor(7.4838, device='cuda:0')\n",
      "train loss: tensor(7.5259, device='cuda:0')\n",
      "train loss: tensor(7.3957, device='cuda:0')\n",
      "train loss: tensor(7.4332, device='cuda:0')\n",
      "train loss: tensor(6.8589, device='cuda:0')\n",
      "train loss: tensor(7.1923, device='cuda:0')\n",
      "train loss: tensor(7.0301, device='cuda:0')\n",
      "train loss: tensor(7.0006, device='cuda:0')\n",
      "train loss: tensor(7.4647, device='cuda:0')\n",
      "train loss: tensor(7.5260, device='cuda:0')\n",
      "train loss: tensor(7.4070, device='cuda:0')\n",
      "train loss: tensor(7.5225, device='cuda:0')\n",
      "train loss: tensor(7.0984, device='cuda:0')\n",
      "train loss: tensor(7.3242, device='cuda:0')\n",
      "train loss: tensor(7.2341, device='cuda:0')\n",
      "train loss: tensor(7.4237, device='cuda:0')\n",
      "train loss: tensor(7.4384, device='cuda:0')\n",
      "train loss: tensor(6.8281, device='cuda:0')\n",
      "train loss: tensor(7.1112, device='cuda:0')\n",
      "train loss: tensor(7.4673, device='cuda:0')\n",
      "train loss: tensor(7.2029, device='cuda:0')\n",
      "train loss: tensor(7.4231, device='cuda:0')\n",
      "train loss: tensor(7.2742, device='cuda:0')\n",
      "train loss: tensor(7.0567, device='cuda:0')\n",
      "train loss: tensor(6.9470, device='cuda:0')\n",
      "train loss: tensor(7.1926, device='cuda:0')\n",
      "train loss: tensor(7.4982, device='cuda:0')\n",
      "train loss: tensor(7.0239, device='cuda:0')\n",
      "train loss: tensor(7.4805, device='cuda:0')\n",
      "train loss: tensor(7.3265, device='cuda:0')\n",
      "train loss: tensor(7.4844, device='cuda:0')\n",
      "train loss: tensor(7.2080, device='cuda:0')\n",
      "train loss: tensor(6.9904, device='cuda:0')\n",
      "train loss: tensor(7.3388, device='cuda:0')\n",
      "train loss: tensor(7.4204, device='cuda:0')\n",
      "train loss: tensor(7.3865, device='cuda:0')\n",
      "train loss: tensor(7.4105, device='cuda:0')\n",
      "train loss: tensor(7.1116, device='cuda:0')\n",
      "train loss: tensor(6.8383, device='cuda:0')\n",
      "train loss: tensor(7.1426, device='cuda:0')\n",
      "train loss: tensor(7.3917, device='cuda:0')\n",
      "train loss: tensor(7.2061, device='cuda:0')\n",
      "train loss: tensor(7.1177, device='cuda:0')\n",
      "train loss: tensor(7.1417, device='cuda:0')\n",
      "train loss: tensor(7.1522, device='cuda:0')\n",
      "train loss: tensor(7.0619, device='cuda:0')\n",
      "train loss: tensor(7.1437, device='cuda:0')\n",
      "train loss: tensor(7.2846, device='cuda:0')\n",
      "train loss: tensor(7.3740, device='cuda:0')\n",
      "train loss: tensor(7.1531, device='cuda:0')\n",
      "train loss: tensor(7.4410, device='cuda:0')\n",
      "train loss: tensor(7.0121, device='cuda:0')\n",
      "train loss: tensor(7.4749, device='cuda:0')\n",
      "train loss: tensor(7.2024, device='cuda:0')\n",
      "train loss: tensor(6.9975, device='cuda:0')\n",
      "train loss: tensor(7.2420, device='cuda:0')\n",
      "train loss: tensor(7.1692, device='cuda:0')\n",
      "train loss: tensor(7.3576, device='cuda:0')\n",
      "train loss: tensor(7.7878, device='cuda:0')\n",
      "train loss: tensor(6.9253, device='cuda:0')\n",
      "train loss: tensor(7.2980, device='cuda:0')\n",
      "train loss: tensor(7.7666, device='cuda:0')\n",
      "train loss: tensor(7.4126, device='cuda:0')\n",
      "train loss: tensor(7.3839, device='cuda:0')\n",
      "train loss: tensor(7.3478, device='cuda:0')\n",
      "train loss: tensor(7.5445, device='cuda:0')\n",
      "train loss: tensor(7.1347, device='cuda:0')\n",
      "train loss: tensor(7.4101, device='cuda:0')\n",
      "train loss: tensor(7.2453, device='cuda:0')\n",
      "train loss: tensor(7.3958, device='cuda:0')\n",
      "train loss: tensor(7.5405, device='cuda:0')\n",
      "train loss: tensor(6.9974, device='cuda:0')\n",
      "train loss: tensor(6.9788, device='cuda:0')\n",
      "train loss: tensor(7.0303, device='cuda:0')\n",
      "train loss: tensor(7.6555, device='cuda:0')\n",
      "train loss: tensor(7.1636, device='cuda:0')\n",
      "train loss: tensor(7.1250, device='cuda:0')\n",
      "train loss: tensor(7.1595, device='cuda:0')\n",
      "train loss: tensor(7.3602, device='cuda:0')\n",
      "train loss: tensor(7.0591, device='cuda:0')\n",
      "train loss: tensor(7.1645, device='cuda:0')\n",
      "train loss: tensor(7.1868, device='cuda:0')\n",
      "train loss: tensor(7.2148, device='cuda:0')\n",
      "train loss: tensor(7.2687, device='cuda:0')\n",
      "train loss: tensor(7.2856, device='cuda:0')\n",
      "train loss: tensor(7.3455, device='cuda:0')\n",
      "train loss: tensor(7.3110, device='cuda:0')\n",
      "train loss: tensor(7.3583, device='cuda:0')\n",
      "train loss: tensor(7.1174, device='cuda:0')\n",
      "train loss: tensor(7.3008, device='cuda:0')\n",
      "train loss: tensor(7.1038, device='cuda:0')\n",
      "train loss: tensor(7.4320, device='cuda:0')\n",
      "train loss: tensor(6.8258, device='cuda:0')\n",
      "train loss: tensor(7.4681, device='cuda:0')\n",
      "train loss: tensor(7.3523, device='cuda:0')\n",
      "train loss: tensor(7.0669, device='cuda:0')\n",
      "train loss: tensor(7.0874, device='cuda:0')\n",
      "train loss: tensor(7.2911, device='cuda:0')\n",
      "train loss: tensor(7.2790, device='cuda:0')\n",
      "train loss: tensor(7.3612, device='cuda:0')\n",
      "train loss: tensor(7.4999, device='cuda:0')\n",
      "train loss: tensor(7.2859, device='cuda:0')\n",
      "train loss: tensor(7.0524, device='cuda:0')\n",
      "train loss: tensor(6.9605, device='cuda:0')\n",
      "train loss: tensor(7.2612, device='cuda:0')\n",
      "train loss: tensor(7.3751, device='cuda:0')\n",
      "train loss: tensor(7.1392, device='cuda:0')\n",
      "train loss: tensor(7.1755, device='cuda:0')\n",
      "train loss: tensor(7.1944, device='cuda:0')\n",
      "train loss: tensor(7.4972, device='cuda:0')\n",
      "train loss: tensor(7.6921, device='cuda:0')\n",
      "train loss: tensor(7.4859, device='cuda:0')\n",
      "train loss: tensor(7.1310, device='cuda:0')\n",
      "train loss: tensor(7.3211, device='cuda:0')\n",
      "train loss: tensor(7.1892, device='cuda:0')\n",
      "train loss: tensor(7.1016, device='cuda:0')\n",
      "train loss: tensor(6.8234, device='cuda:0')\n",
      "train loss: tensor(7.0986, device='cuda:0')\n",
      "train loss: tensor(7.5553, device='cuda:0')\n",
      "train loss: tensor(6.7722, device='cuda:0')\n",
      "train loss: tensor(6.9564, device='cuda:0')\n",
      "train loss: tensor(7.1655, device='cuda:0')\n",
      "train loss: tensor(7.2386, device='cuda:0')\n",
      "train loss: tensor(7.3411, device='cuda:0')\n",
      "train loss: tensor(6.9685, device='cuda:0')\n",
      "train loss: tensor(7.1123, device='cuda:0')\n",
      "train loss: tensor(7.2327, device='cuda:0')\n",
      "train loss: tensor(7.2279, device='cuda:0')\n",
      "train loss: tensor(7.1641, device='cuda:0')\n",
      "train loss: tensor(7.0587, device='cuda:0')\n",
      "train loss: tensor(7.0330, device='cuda:0')\n",
      "train loss: tensor(7.1162, device='cuda:0')\n",
      "train loss: tensor(7.0014, device='cuda:0')\n",
      "train loss: tensor(7.2109, device='cuda:0')\n",
      "train loss: tensor(7.2490, device='cuda:0')\n",
      "train loss: tensor(7.1408, device='cuda:0')\n",
      "train loss: tensor(6.9221, device='cuda:0')\n",
      "train loss: tensor(7.3566, device='cuda:0')\n",
      "train loss: tensor(7.1259, device='cuda:0')\n",
      "train loss: tensor(7.2089, device='cuda:0')\n",
      "train loss: tensor(7.5282, device='cuda:0')\n",
      "train loss: tensor(6.8935, device='cuda:0')\n",
      "train loss: tensor(7.3903, device='cuda:0')\n",
      "train loss: tensor(6.8151, device='cuda:0')\n",
      "train loss: tensor(7.4675, device='cuda:0')\n",
      "train loss: tensor(7.1256, device='cuda:0')\n",
      "train loss: tensor(7.1866, device='cuda:0')\n",
      "train loss: tensor(7.8062, device='cuda:0')\n",
      "train loss: tensor(6.8586, device='cuda:0')\n",
      "train loss: tensor(7.5396, device='cuda:0')\n",
      "train loss: tensor(6.9788, device='cuda:0')\n",
      "train loss: tensor(7.1460, device='cuda:0')\n",
      "train loss: tensor(7.2560, device='cuda:0')\n",
      "train loss: tensor(7.1102, device='cuda:0')\n",
      "train loss: tensor(7.3705, device='cuda:0')\n",
      "train loss: tensor(7.2375, device='cuda:0')\n",
      "train loss: tensor(7.1142, device='cuda:0')\n",
      "train loss: tensor(7.3149, device='cuda:0')\n",
      "train loss: tensor(7.0377, device='cuda:0')\n",
      "train loss: tensor(7.4306, device='cuda:0')\n",
      "train loss: tensor(7.1987, device='cuda:0')\n",
      "train loss: tensor(7.2492, device='cuda:0')\n",
      "train loss: tensor(7.2383, device='cuda:0')\n",
      "train loss: tensor(6.7816, device='cuda:0')\n",
      "train loss: tensor(6.9581, device='cuda:0')\n",
      "train loss: tensor(7.1716, device='cuda:0')\n",
      "train loss: tensor(7.0280, device='cuda:0')\n",
      "train loss: tensor(7.4111, device='cuda:0')\n",
      "train loss: tensor(7.1846, device='cuda:0')\n",
      "train loss: tensor(6.8655, device='cuda:0')\n",
      "train loss: tensor(7.4850, device='cuda:0')\n",
      "train loss: tensor(6.8744, device='cuda:0')\n",
      "train loss: tensor(7.4012, device='cuda:0')\n",
      "train loss: tensor(7.4884, device='cuda:0')\n",
      "train loss: tensor(6.9745, device='cuda:0')\n",
      "train loss: tensor(7.3177, device='cuda:0')\n",
      "train loss: tensor(7.4842, device='cuda:0')\n",
      "train loss: tensor(6.7760, device='cuda:0')\n",
      "train loss: tensor(7.1316, device='cuda:0')\n",
      "train loss: tensor(7.1831, device='cuda:0')\n",
      "train loss: tensor(7.1108, device='cuda:0')\n",
      "train loss: tensor(7.2651, device='cuda:0')\n",
      "train loss: tensor(7.2854, device='cuda:0')\n",
      "train loss: tensor(7.4781, device='cuda:0')\n",
      "train loss: tensor(7.1597, device='cuda:0')\n",
      "train loss: tensor(7.1798, device='cuda:0')\n",
      "train loss: tensor(6.8923, device='cuda:0')\n",
      "train loss: tensor(7.2255, device='cuda:0')\n",
      "train loss: tensor(7.1060, device='cuda:0')\n",
      "train loss: tensor(6.9689, device='cuda:0')\n",
      "train loss: tensor(7.6539, device='cuda:0')\n",
      "train loss: tensor(7.2888, device='cuda:0')\n",
      "train loss: tensor(7.3988, device='cuda:0')\n",
      "train loss: tensor(7.2366, device='cuda:0')\n",
      "train loss: tensor(7.3568, device='cuda:0')\n",
      "train loss: tensor(7.1730, device='cuda:0')\n",
      "train loss: tensor(7.5741, device='cuda:0')\n",
      "train loss: tensor(7.1240, device='cuda:0')\n",
      "train loss: tensor(7.1187, device='cuda:0')\n",
      "train loss: tensor(7.2342, device='cuda:0')\n",
      "train loss: tensor(7.0686, device='cuda:0')\n",
      "train loss: tensor(6.8988, device='cuda:0')\n",
      "train loss: tensor(7.3297, device='cuda:0')\n",
      "train loss: tensor(7.1814, device='cuda:0')\n",
      "train loss: tensor(7.0316, device='cuda:0')\n",
      "train loss: tensor(6.7524, device='cuda:0')\n",
      "train loss: tensor(7.2039, device='cuda:0')\n",
      "train loss: tensor(7.0997, device='cuda:0')\n",
      "train loss: tensor(7.2775, device='cuda:0')\n",
      "train loss: tensor(6.8975, device='cuda:0')\n",
      "train loss: tensor(7.5038, device='cuda:0')\n",
      "train loss: tensor(7.3677, device='cuda:0')\n",
      "train loss: tensor(7.0473, device='cuda:0')\n",
      "train loss: tensor(6.8960, device='cuda:0')\n",
      "train loss: tensor(7.3496, device='cuda:0')\n",
      "train loss: tensor(7.6058, device='cuda:0')\n",
      "train loss: tensor(7.4526, device='cuda:0')\n",
      "train loss: tensor(7.2247, device='cuda:0')\n",
      "train loss: tensor(7.6632, device='cuda:0')\n",
      "train loss: tensor(6.8957, device='cuda:0')\n",
      "train loss: tensor(7.1328, device='cuda:0')\n",
      "train loss: tensor(7.4653, device='cuda:0')\n",
      "train loss: tensor(7.5467, device='cuda:0')\n",
      "train loss: tensor(6.6877, device='cuda:0')\n",
      "train loss: tensor(7.1631, device='cuda:0')\n",
      "train loss: tensor(7.3321, device='cuda:0')\n",
      "train loss: tensor(7.4561, device='cuda:0')\n",
      "train loss: tensor(7.4909, device='cuda:0')\n",
      "train loss: tensor(7.4098, device='cuda:0')\n",
      "train loss: tensor(7.4775, device='cuda:0')\n",
      "train loss: tensor(7.4946, device='cuda:0')\n",
      "train loss: tensor(7.2084, device='cuda:0')\n",
      "train loss: tensor(7.3671, device='cuda:0')\n",
      "train loss: tensor(7.2118, device='cuda:0')\n",
      "train loss: tensor(7.4085, device='cuda:0')\n",
      "train loss: tensor(7.2462, device='cuda:0')\n",
      "train loss: tensor(7.3658, device='cuda:0')\n",
      "train loss: tensor(7.5010, device='cuda:0')\n",
      "train loss: tensor(7.2446, device='cuda:0')\n",
      "train loss: tensor(6.9163, device='cuda:0')\n",
      "train loss: tensor(7.1263, device='cuda:0')\n",
      "train loss: tensor(7.1124, device='cuda:0')\n",
      "train loss: tensor(6.9191, device='cuda:0')\n",
      "train loss: tensor(7.2487, device='cuda:0')\n",
      "train loss: tensor(6.6525, device='cuda:0')\n",
      "train loss: tensor(7.2180, device='cuda:0')\n",
      "train loss: tensor(7.6736, device='cuda:0')\n",
      "train loss: tensor(7.2626, device='cuda:0')\n",
      "train loss: tensor(7.6357, device='cuda:0')\n",
      "train loss: tensor(7.3325, device='cuda:0')\n",
      "train loss: tensor(7.5263, device='cuda:0')\n",
      "train loss: tensor(7.1698, device='cuda:0')\n",
      "train loss: tensor(7.0240, device='cuda:0')\n",
      "train loss: tensor(7.2935, device='cuda:0')\n",
      "train loss: tensor(7.2286, device='cuda:0')\n",
      "train loss: tensor(7.1298, device='cuda:0')\n",
      "train loss: tensor(7.3668, device='cuda:0')\n",
      "train loss: tensor(7.0651, device='cuda:0')\n",
      "train loss: tensor(7.2570, device='cuda:0')\n",
      "train loss: tensor(6.9453, device='cuda:0')\n",
      "train loss: tensor(7.5709, device='cuda:0')\n",
      "train loss: tensor(6.8071, device='cuda:0')\n",
      "train loss: tensor(7.3286, device='cuda:0')\n",
      "train loss: tensor(7.3123, device='cuda:0')\n",
      "train loss: tensor(7.3107, device='cuda:0')\n",
      "train loss: tensor(7.5939, device='cuda:0')\n",
      "train loss: tensor(7.1802, device='cuda:0')\n",
      "train loss: tensor(7.5213, device='cuda:0')\n",
      "train loss: tensor(6.9812, device='cuda:0')\n",
      "train loss: tensor(7.0863, device='cuda:0')\n",
      "train loss: tensor(7.4757, device='cuda:0')\n",
      "train loss: tensor(7.2760, device='cuda:0')\n",
      "train loss: tensor(7.0105, device='cuda:0')\n",
      "train loss: tensor(7.2319, device='cuda:0')\n",
      "train loss: tensor(7.7137, device='cuda:0')\n",
      "train loss: tensor(7.3833, device='cuda:0')\n",
      "train loss: tensor(7.3058, device='cuda:0')\n",
      "train loss: tensor(7.1524, device='cuda:0')\n",
      "train loss: tensor(7.2310, device='cuda:0')\n",
      "train loss: tensor(7.1290, device='cuda:0')\n",
      "train loss: tensor(7.2523, device='cuda:0')\n",
      "train loss: tensor(7.1455, device='cuda:0')\n",
      "train loss: tensor(7.1996, device='cuda:0')\n",
      "train loss: tensor(7.2901, device='cuda:0')\n",
      "train loss: tensor(7.4753, device='cuda:0')\n",
      "train loss: tensor(7.0594, device='cuda:0')\n",
      "train loss: tensor(7.0646, device='cuda:0')\n",
      "train loss: tensor(7.1717, device='cuda:0')\n",
      "train loss: tensor(7.0081, device='cuda:0')\n",
      "train loss: tensor(7.7191, device='cuda:0')\n",
      "train loss: tensor(7.2609, device='cuda:0')\n",
      "train loss: tensor(6.9545, device='cuda:0')\n",
      "train loss: tensor(7.6124, device='cuda:0')\n",
      "train loss: tensor(7.2291, device='cuda:0')\n",
      "train loss: tensor(7.2912, device='cuda:0')\n",
      "train loss: tensor(7.4005, device='cuda:0')\n",
      "train loss: tensor(7.1685, device='cuda:0')\n",
      "train loss: tensor(7.4059, device='cuda:0')\n",
      "train loss: tensor(7.1871, device='cuda:0')\n",
      "train loss: tensor(6.8667, device='cuda:0')\n",
      "train loss: tensor(7.1683, device='cuda:0')\n",
      "train loss: tensor(7.0606, device='cuda:0')\n",
      "train loss: tensor(7.2022, device='cuda:0')\n",
      "train loss: tensor(7.0177, device='cuda:0')\n",
      "train loss: tensor(7.2733, device='cuda:0')\n",
      "train loss: tensor(7.3320, device='cuda:0')\n",
      "train loss: tensor(7.2606, device='cuda:0')\n",
      "train loss: tensor(7.1005, device='cuda:0')\n",
      "train loss: tensor(7.0216, device='cuda:0')\n",
      "train loss: tensor(7.5438, device='cuda:0')\n",
      "train loss: tensor(7.0662, device='cuda:0')\n",
      "train loss: tensor(7.0785, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, data_train, data_test, n_iter=None, epochs=3):\n",
    "    perf_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "\n",
    "    loss_fn = t.nn.CrossEntropyLoss() # changeme\n",
    "    optimizer = t.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        n = 0\n",
    "        model.train()\n",
    "        for batch in tqdm(data_train):\n",
    "            n += 1\n",
    "            if n_iter is not None and n > n_iter:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            input = batch[0].to('cuda')\n",
    "            correct_results = batch[1].to('cuda')\n",
    "            \n",
    "            out = model(input)\n",
    "            masked_positions = (input == mask_token)\n",
    "\n",
    "            masked_out = out.to('cuda').masked_select(masked_positions.unsqueeze(-1).to('cuda'))\n",
    "            correct_results = correct_results.masked_select(masked_positions.to('cuda'))\n",
    "            masked_out = masked_out.reshape((correct_results.shape[0], out.shape[-1]))\n",
    "\n",
    "            loss = loss_fn(masked_out, correct_results)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            perf_history.append(loss.detach().clone())\n",
    "            print('train loss:', perf_history[-1])\n",
    "\n",
    "        with t.no_grad():\n",
    "            model.test()\n",
    "            test_loss = 0\n",
    "            for batch in data_test:\n",
    "                input = t.tensor(batch[0]).to('cuda')\n",
    "                test_out = model(input)\n",
    "                masked_positions = input == mask_token\n",
    "                masked_out = test_out[masked_positions,:]\n",
    "                test_loss += loss_fn(masked_out, t.tensor(batch[0]).clone().detach().float().to('cuda'))\n",
    "            test_loss_history.append(test_loss / len(data_test))\n",
    "\n",
    "    return perf_history, test_loss_history\n",
    "\n",
    "tiny_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=384, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=1536, \n",
    "    num_heads=12, num_layers=2\n",
    ")\n",
    "perf_history, test_loss_history = train(tiny_bert, masked_train_data, masked_test_data)\n",
    "t.save(tiny_bert, 'tiny_bert_3epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_art_probs2(sentence):\n",
    "    mask_encoding = tokenizer.encode('[MASK]')[1]\n",
    "\n",
    "    tiny_bert.eval()\n",
    "\n",
    "    encoding = t.tensor(tokenizer.encode(sentence))\n",
    "\n",
    "    logits = tiny_bert(encoding)[encoding == mask_encoding]\n",
    "\n",
    "    probs = nn.functional.softmax(logits, dim=-1)\n",
    "    probs, word_indices = probs.sort(descending=True, dim=-1)\n",
    "\n",
    "    probs = probs[:, :10]\n",
    "    word_indices = word_indices[:, :10]\n",
    "\n",
    "    words = [[tokenizer.decode(word) for word in word_options] for word_options in word_indices]\n",
    "    if len(words) > 1:\n",
    "        print(\"please don't double mask\")\n",
    "    words_with_probs = zip(words[0], probs[0])\n",
    "    \n",
    "    sentence_for_display = sentence.replace('[MASK]', '---')\n",
    "    print(sentence_for_display)\n",
    "    for word, prob in words_with_probs:\n",
    "        print(f\"%{sentence_for_display.index('---') - 2}.1d%% %s\" % (prob * 100, word))\n",
    "# ascii_art_probs(\"The fish loves to eat [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is ---.\n",
      "            4% the\n",
      "            2% .\n",
      "            2% >\n",
      "            2% of\n",
      "            2% un\n",
      "            2% <\n",
      "            2% ,\n",
      "            1% a\n",
      "            1% @\n",
      "            1% and\n"
     ]
    }
   ],
   "source": [
    "ascii_art_probs2(\"Hi, my name is [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(tiny_bert, 'tiny_bert_400.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
