{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "import random\n",
    "import torchtext\n",
    "import transformers\n",
    "\n",
    "from load_bert import BertClassifier, default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset() :\n",
    "    train, test, valid = torchtext.datasets.WikiText2()\n",
    "    return list(train), list(test), list(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(dataset, batch_size=16):\n",
    "    batched_dataset = []\n",
    "    for split in dataset:\n",
    "        # sort split by length\n",
    "        split.sort(key=len)\n",
    "        # make sure split can be batched evenly\n",
    "        if len(split) % batch_size != 0:\n",
    "            split = split[:-(len(split) % batch_size)]\n",
    "        num_batches = len(split) // batch_size\n",
    "        batches = [split[b * batch_size:(b + 1) * batch_size] for b in range(batch_size)]\n",
    "        random.shuffle(batches)\n",
    "        batched_dataset.append(batches)\n",
    "    return tuple(batched_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = batch_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, max_length=512):\n",
    "    tokenized_dataset = []\n",
    "    for split in dataset:\n",
    "        tokenized_split = [] \n",
    "        for batch in split:\n",
    "            tokens = tokenizer(\n",
    "                batch,\n",
    "                padding=\"longest\",\n",
    "                max_length=max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "            tokenized_split.append(tokens[\"input_ids\"])\n",
    "        tokenized_dataset.append(tokenized_split)\n",
    "    return tuple(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = tokenizer.encode(\"[MASK]\")[1]\n",
    "\n",
    "def mask_dataset(dataset, p=.15):\n",
    "    for split in dataset:\n",
    "        for batch in split:\n",
    "            for tokens in batch:\n",
    "                for idx, _ in enumerate(tokens):\n",
    "                    if random.random() < p: tokens[idx] = mask_token\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mask_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorify_dataset(dataset):\n",
    "    tensorified_dataset = []\n",
    "    for split in dataset:\n",
    "        tensorified_dataset.append([t.tensor(batch) for batch in split])\n",
    "    return tuple(tensorified_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tensorify_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\": 2,\n",
    "    'hidden_size': 256,\n",
    "    'intermediate_size': 1024,\n",
    "}\n",
    "config = {**default_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertClassifier(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, valid = dataset\n",
    "while len(train) % 16 != 0: train = train[:-1]\n",
    "while len(test) % 16 != 0: test = test[:-1]\n",
    "while len(valid) % 16 != 0: valid = valid[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-c9ef336cf784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "formatted = []\n",
    "\n",
    "for data in [train, test, valid]:\n",
    "    num_batches = len(data) // batch_size\n",
    "    data.sort(key=len)\n",
    "    formatted.append(-1) # the thing\n",
    "\n",
    "train, test, valid = formatted\n",
    "\n",
    "processed_data = []\n",
    "for i, data in enumerate(data_train):\n",
    "    raw_text = data_train[i][1]\n",
    "    tokens = tokenizer.encode(raw_text)\n",
    "    processed_data.append((tokens, raw_text, t.tensor(0 if data[0]=='neg' else 1))) # device weird stuff?\n",
    "processed_data.sort(key=lambda data: len(data[0]))\n",
    "\n",
    "processed_data_test = []\n",
    "for i, data in enumerate(data_test):\n",
    "    raw_text_test = data_test[i][1]\n",
    "    tokens_test = tokenizer.encode(raw_text_test)\n",
    "    processed_data_test.append((tokens_test, raw_text_test, t.tensor(0 if data[0]=='neg' else 1))) # device weird stuff?\n",
    "processed_data_test.sort(key=lambda data: len(data[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
