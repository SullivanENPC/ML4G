{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch\n",
    "from torch import einsum\n",
    "from torch import nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import mlab_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking encoding:\n",
      "Congrats! You've passed the test!\n",
      "Checking new key and value:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class Attention(t.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_size,hidden_size*3)\n",
    "        self.output_projection = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, past_key_values=None, return_key_values=False):\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            k, v = torch.tensor_split(past_key_values.unsqueeze(0), 2, 3)\n",
    "            out = self.attention(x)\n",
    "            q, k_final, v_final = out.tensor_split(3,dim=-1)\n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            k_final = rearrange(k_final, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            v_final = rearrange(v_final, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            \n",
    "            k = torch.cat((k, k_final), 2)\n",
    "            v = torch.cat((v, v_final), 2)\n",
    "            attention_score = einsum(\"bhtc,bhfc->bhft\", q, k)\n",
    "\n",
    "            \n",
    "        else:     \n",
    "            out = self.attention(x)\n",
    "            q, k, v = out.tensor_split(3,dim=-1)\n",
    "            \n",
    "            q = rearrange(q, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            k = rearrange(k, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "            v = rearrange(v, \"b n (h c) -> b h n c\", h = self.num_heads)\n",
    "\n",
    "            attention_score = einsum(\"bhtc,bhfc->bhft\", q, k)\n",
    "                        \n",
    "            attention_score = t.triu(attention_score) - 1e4*t.tril(t.ones_like(attention_score), diagonal = -1)\n",
    "                \n",
    "        scaled_attn_score = 1/torch.sqrt(t.tensor(self.head_size)) * attention_score\n",
    "                \n",
    "        attention_pattern = t.nn.Softmax(dim = -2)(scaled_attn_score)\n",
    "            \n",
    "        out = einsum(\"bhft,bhfc -> bhtc\", attention_pattern, v)\n",
    "                \n",
    "        out = rearrange(out, \"b h n c -> b n (h c)\")\n",
    "        \n",
    "        out = self.output_projection(out)\n",
    "        \n",
    "        if return_key_values and past_key_values is not None:\n",
    "            return out, torch.cat((k_final, v_final), 3)\n",
    "        else:\n",
    "            return out\n",
    "gpt_tests.test_attn_cache(Attention)\n",
    "# gpt_tests.test_unidirectional_attn(Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our out tensor([[[ 3.2229e-01, -3.7651e-01,  3.6197e-01,  1.8525e-02,  2.0513e-01,\n",
      "          -4.9755e-01, -3.2238e-01,  1.7564e-01,  6.9831e-02, -4.7030e-01,\n",
      "           8.4727e-02, -1.8117e-01,  3.2580e-01,  3.3301e-01, -3.8834e-01,\n",
      "           4.4791e-01, -4.6027e-01, -5.3693e-01,  1.3190e-01, -9.3719e-02,\n",
      "          -1.3495e-01, -5.6973e-01,  3.2986e-01, -3.2485e-03],\n",
      "         [ 4.7690e-01,  1.6798e-01, -1.6052e-03,  1.7268e-01,  3.5946e-01,\n",
      "          -3.4603e-01,  2.7828e-01,  8.2329e-05,  4.7646e-02, -1.1094e-01,\n",
      "           4.2770e-01,  7.3739e-02,  4.5244e-01, -9.9570e-02, -2.9142e-01,\n",
      "           2.8042e-01, -4.2251e-02,  1.3468e-01, -1.6897e-01, -3.7332e-01,\n",
      "           6.9931e-02,  1.4339e-02,  2.0342e-01,  4.3749e-02],\n",
      "         [ 1.4101e-01, -8.8313e-03,  5.5020e-02,  2.6923e-01,  1.0635e-01,\n",
      "          -2.5896e-01,  3.7684e-02,  1.9169e-01,  1.2130e-01, -4.1290e-01,\n",
      "           1.7931e-01, -1.3171e-01,  8.7645e-02, -1.7535e-01, -3.1550e-01,\n",
      "           1.5904e-01,  1.8276e-02,  2.4974e-01,  2.7520e-02, -6.5805e-02,\n",
      "          -6.0863e-02, -8.9087e-02,  2.1445e-01, -1.9835e-01],\n",
      "         [ 2.4136e-01, -5.6333e-02,  1.1756e-01,  1.4084e-01,  1.1906e-01,\n",
      "          -1.5165e-01,  1.4387e-01,  1.2815e-01,  1.0243e-01, -2.7946e-01,\n",
      "           1.0909e-01, -1.4868e-01,  2.8842e-01, -1.3013e-01, -2.2278e-01,\n",
      "           2.0536e-01, -3.7042e-03,  7.3978e-02, -1.5528e-01, -2.7703e-02,\n",
      "           1.3416e-01, -8.0419e-02,  1.1699e-01, -1.8852e-01],\n",
      "         [ 2.0983e-01,  1.4457e-01,  9.4016e-02,  2.8303e-01,  2.0074e-01,\n",
      "          -1.8090e-01,  2.2448e-01,  6.6570e-02, -2.5763e-02, -2.3486e-01,\n",
      "           5.3353e-02,  8.2909e-02,  3.0517e-01, -2.1974e-01, -3.1956e-01,\n",
      "           2.2649e-01, -9.8183e-02,  2.0167e-01,  2.8806e-02, -1.9309e-01,\n",
      "          -1.0667e-02, -1.0810e-01,  8.6762e-02, -1.4190e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "their out tensor([[[ 3.2229e-01, -3.7651e-01,  3.6197e-01,  1.8525e-02,  2.0513e-01,\n",
      "          -4.9755e-01, -3.2238e-01,  1.7564e-01,  6.9831e-02, -4.7030e-01,\n",
      "           8.4727e-02, -1.8117e-01,  3.2580e-01,  3.3301e-01, -3.8834e-01,\n",
      "           4.4791e-01, -4.6027e-01, -5.3693e-01,  1.3190e-01, -9.3719e-02,\n",
      "          -1.3495e-01, -5.6973e-01,  3.2986e-01, -3.2485e-03],\n",
      "         [ 4.7690e-01,  1.6798e-01, -1.6052e-03,  1.7268e-01,  3.5946e-01,\n",
      "          -3.4603e-01,  2.7828e-01,  8.2329e-05,  4.7646e-02, -1.1094e-01,\n",
      "           4.2770e-01,  7.3739e-02,  4.5244e-01, -9.9570e-02, -2.9142e-01,\n",
      "           2.8042e-01, -4.2251e-02,  1.3468e-01, -1.6897e-01, -3.7332e-01,\n",
      "           6.9931e-02,  1.4339e-02,  2.0342e-01,  4.3749e-02],\n",
      "         [ 1.4101e-01, -8.8313e-03,  5.5020e-02,  2.6923e-01,  1.0635e-01,\n",
      "          -2.5896e-01,  3.7684e-02,  1.9169e-01,  1.2130e-01, -4.1290e-01,\n",
      "           1.7931e-01, -1.3171e-01,  8.7645e-02, -1.7535e-01, -3.1550e-01,\n",
      "           1.5904e-01,  1.8276e-02,  2.4974e-01,  2.7520e-02, -6.5805e-02,\n",
      "          -6.0863e-02, -8.9087e-02,  2.1445e-01, -1.9835e-01],\n",
      "         [ 2.4136e-01, -5.6333e-02,  1.1756e-01,  1.4084e-01,  1.1906e-01,\n",
      "          -1.5165e-01,  1.4387e-01,  1.2815e-01,  1.0243e-01, -2.7946e-01,\n",
      "           1.0909e-01, -1.4868e-01,  2.8842e-01, -1.3013e-01, -2.2278e-01,\n",
      "           2.0536e-01, -3.7042e-03,  7.3978e-02, -1.5528e-01, -2.7703e-02,\n",
      "           1.3416e-01, -8.0419e-02,  1.1699e-01, -1.8852e-01],\n",
      "         [ 2.0983e-01,  1.4457e-01,  9.4016e-02,  2.8303e-01,  2.0074e-01,\n",
      "          -1.8090e-01,  2.2448e-01,  6.6570e-02, -2.5764e-02, -2.3486e-01,\n",
      "           5.3353e-02,  8.2909e-02,  3.0517e-01, -2.1974e-01, -3.1956e-01,\n",
      "           2.2649e-01, -9.8183e-02,  2.0167e-01,  2.8806e-02, -1.9309e-01,\n",
      "          -1.0667e-02, -1.0810e-01,  8.6762e-02, -1.4190e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "import gpt_tests\n",
    "\n",
    "gpt_tests.test_unidirectional_attn(Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(self.hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attention = Attention(self.hidden_size, self.num_heads)\n",
    "        self.ln2 = nn.LayerNorm(self.hidden_size, eps=layer_norm_epsilon)\n",
    "        self.MLP = nn.Sequential(nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "                                   nn.GELU(),\n",
    "                                   nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "                                   nn.Dropout(self.dropout)\n",
    "                                  )\n",
    "        \n",
    "    def forward(self, x, past_key_values = None, return_key_values = False):\n",
    "        if return_key_values:\n",
    "            layer1, new_key_values = self.attention(self.ln1(x)) \n",
    "            layer1 = layer1 + x\n",
    "            return self.ML\n",
    "        P(self.ln2(layer1)) + layer1, new_key_values\n",
    "            \n",
    "        layer1 = self.attention(self.ln1(x), past_key_values) + x\n",
    "        return self.MLP(self.ln2(layer1)) + layer1\n",
    "\n",
    "gpt_tests.test_gpt_block(Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 4.069s to generate a 500-token sentence without cache and 2.895s with cache.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "    # L_encoding: TensorType[\"batch_size\", \"hidden_size\"]\n",
    "\n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size, \n",
    "                 max_position_embeddings, dropout, layer_norm_epsilon, L=0, use_cache=False):\n",
    "        super().__init__()\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, num_heads, dropout, layer_norm_epsilon)\n",
    "                                      for _ in range(num_layers)])\n",
    "        \n",
    "        self.ln = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        \n",
    "        self.use_cache = use_cache\n",
    "        if self.use_cache:\n",
    "            self.cache = t.zeros((num_layers, num_heads, 0, 2*self.head_size))\n",
    "        \n",
    "    def forward(self, input_ids): # [batch, seq_len]\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.get_device() \n",
    "                                 if input_ids.get_device() >= 0 else 'cpu')\n",
    "        embeddings = self.token_embeddings(input_ids) + self.position_embeddings(positions)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        if self.use_cache and self.cache.shape[1] == 0:\n",
    "                        \n",
    "            nl, nh, _, hs = self.cache.shape\n",
    "            cache_updates = t.tensor((0, nh, 1, hs))\n",
    "            \n",
    "            if self.cache.shape[2] == 0:\n",
    "                tokens_to_process = range(input_ids.shape[1])\n",
    "            \n",
    "            else:\n",
    "                tokens_to_process = [-1]\n",
    "            \n",
    "            for i in tokens_to_process:\n",
    "                \n",
    "                out = embeddings[:,i]\n",
    "                for block_num, block in enumerate(self.blocks):\n",
    "                    out, new_keys = block(out, cache[block_num], True)\n",
    "                    cache_updates = t.cat((cache_updates,new_keys),0)\n",
    "                \n",
    "                self.cache = t.cat((self.cache, cache_updates), 2)\n",
    "        else:\n",
    "            out = embeddings\n",
    "            for block_num, block in enumerate(self.blocks):\n",
    "                out = block(out)        \n",
    "        \n",
    "        embeddings = self.ln(out)\n",
    "        output = GPT2Output(einsum(\"bnc, vc -> bnv\", embeddings, self.token_embeddings.weight)[:, -1],\n",
    "                            embeddings[:, -1])\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "gpt_tests.test_gpt_cache(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = t.tensor((1,0,2))\n",
    "ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, \n",
    "                 max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, L = 11)\n",
    "\n",
    "pretrained_gpt = gpt_tests.get_pretrained_gpt()\n",
    "pretrained_state_dict = pretrained_gpt.state_dict()\n",
    "#my_gpt.load_state_dict(pretrained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_params = [param for param in zip(pretrained_state_dict.keys(), my_gpt.state_dict().keys())]\n",
    "new_state_dict = {}\n",
    "for pretrained, ours in zipped_params:\n",
    "    new_state_dict[ours] = pretrained_state_dict[pretrained]\n",
    "\n",
    "my_gpt.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1860,  1.5728,  2.9378,  ..., -0.6505, -0.9438,  1.3388],\n",
      "        [ 1.1860,  1.5728,  2.9378,  ..., -0.6505, -0.9438,  1.3388],\n",
      "        [ 1.1860,  1.5728,  2.9378,  ..., -0.6505, -0.9438,  1.3388],\n",
      "        [ 1.1860,  1.5728,  2.9378,  ..., -0.6505, -0.9438,  1.3388],\n",
      "        [ 1.1860,  1.5728,  2.9378,  ..., -0.6505, -0.9438,  1.3388]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1044,  0.0473, -0.3324,  ...,  0.3400, -0.0433,  1.3392],\n",
      "        [-0.0790, -0.0920, -0.2450,  ...,  0.3143, -0.1944,  1.3053],\n",
      "        [ 0.1155, -0.2108, -0.4075,  ...,  0.2970, -0.1911,  1.2119],\n",
      "        [ 0.1917, -0.3081, -0.5358,  ...,  0.3087, -0.3592,  1.1066],\n",
      "        [ 0.1922, -0.5265, -0.5954,  ...,  0.3476, -0.3509,  1.0415]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from bert_sol import Bert\n",
    "\n",
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, L = 11,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "texts = [\"My life motto:\",\n",
    "         \"My life motto: Fortune\",\n",
    "         \"My life motto: Fortune favors\",\n",
    "         \"My life motto: Fortune favors the\",\n",
    "         \"My life motto: Fortune favors the bold\"]\n",
    "\n",
    "\n",
    "def encoding(texts, i):\n",
    "    # inputs = t.zeros((len(texts), len(texts[0])))\n",
    "    tokens = tokenizer(texts, padding = \"longest\")\n",
    "    \n",
    "    inputs = t.tensor(tokens[\"input_ids\"])\n",
    "    \n",
    "    my_gpt.eval()\n",
    "    my_bert.eval()\n",
    "    \n",
    "    gpt_encodings = my_gpt(inputs).L_encoding[:,i]\n",
    "    \n",
    "    bert_encodings = my_bert(inputs).L_encoding[:,i]\n",
    "    \n",
    "    print(gpt_encodings)\n",
    "    print(bert_encodings)\n",
    "    \n",
    "    \n",
    "encoding(texts,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
