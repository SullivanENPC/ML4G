{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "import gin\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from days.w2d1.bert_sol import BertWithClassify, mapkey\n",
    "from days.w2d1 import bert_tests\n",
    "import torchtext\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "my_bert = BertWithClassify(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "        num_heads=12, num_layers=12, num_classes=2)\n",
    "# pretrained_bert = bert_tests.get_pretrained_bert()\n",
    "# mapped_params = {mapkey(k): v for k, v in pretrained_bert.state_dict().items()}\n",
    "# my_bert.load_state_dict(mapped_params)\n",
    "# bert_tests.test_same_output(my_bert, pretrained_bert) # we don't need this - tim\n",
    "loss = t.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = t.optim.Adam(my_bert.parameters(), lr)\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0889, -0.9179],\n",
      "        [-0.0043, -0.5612],\n",
      "        [-0.0508, -0.4782],\n",
      "        [ 0.2407, -0.4111],\n",
      "        [ 0.1006, -0.2400],\n",
      "        [ 0.0350, -0.7863],\n",
      "        [-0.2061, -0.9565],\n",
      "        [-0.1430, -0.3821]], grad_fn=<AddmmBackward0>) tensor([0, 1, 1, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader =  DataLoader(data_train, batch_size=8, shuffle=True)\n",
    "for epoch in range(1):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = t.tensor(tokenizer(list(batch[1]), padding='longest', max_length=512, truncation=True)['input_ids'])\n",
    "        target = t.tensor([1 if x == 'pos' else 0 for x in batch[0]], dtype=t.long)\n",
    "        output = my_bert(input_ids)[1]\n",
    "        print(output, target)\n",
    "        l = loss(output, target)\n",
    "        l.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train = list(data_train)\n",
    "# randomd.shuffle(data_train)\n",
    "# data_train\n",
    "# data_train = TensorDataset(t.tensor(inputs), t.tensor(targets))\n",
    "# train_dataloader =  DataLoader(data_train, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
