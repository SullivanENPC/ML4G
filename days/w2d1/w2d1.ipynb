{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.01314 STD: 0.1147 VALS [-0.1276 0.1329 0.1119 0.06959 -0.05208 0.1612 0.03845 0.0386 -0.1686 0.1203...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    project_query: Callable[[t.Tensor], t.Tensor],\n",
    "    project_key: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)]\n",
    "    project_query: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    project_key:   function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "    \"\"\"\n",
    "\n",
    "    queries = rearrange(\n",
    "        project_query(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "    keys = rearrange(\n",
    "        project_key(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    head_size = t.tensor(keys.shape[-1])\n",
    "    return einsum(\"bhid, bhjd -> bhij\", keys, queries) / t.sqrt(head_size)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001297 STD: 0.1089 VALS [0.1178 0.0506 -0.04344 0.1474 0.1352 0.08401 -0.04605 0.08768 0.1694 -0.05225...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "    token_activations: t.Tensor,\n",
    "    num_heads: int,\n",
    "    attention_pattern: t.Tensor,\n",
    "    project_value: Callable[[t.Tensor], t.Tensor],\n",
    "    project_output: Callable[[t.Tensor], t.Tensor],\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "    num_heads: int,\n",
    "    attention_pattern: Tensor[batch_size,num_heads, seq_length, seq_length],\n",
    "    project_value: function( (Tensor[..., 768]) -> Tensor[..., 768] ),\n",
    "    project_output: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "    return: Tensor[batch_size, seq_length, hidden_size]\n",
    "    \"\"\"\n",
    "\n",
    "    attention_prob = t.softmax(attention_pattern, dim=-2)  # dim: b head s s\n",
    "    values = rearrange(\n",
    "        project_value(token_activations), \"b s (head d) -> b head s d\", head=num_heads\n",
    "    )\n",
    "\n",
    "    output_by_head = einsum(\"bhis, bhid -> bhsd\", attention_prob, values)\n",
    "    concatenated = rearrange(output_by_head, \"b h s d -> b s (h d)\")\n",
    "\n",
    "    return project_output(concatenated)\n",
    "\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_size: int,\n",
    "        attention_dim: int = 64,\n",
    "        per_head_output_dim: int = 64,\n",
    "        output_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if output_dim is None:\n",
    "            output_dim = hidden_size\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.per_head_output_dim = per_head_output_dim\n",
    "        self.output_dim: int = output_dim\n",
    "\n",
    "        self.Q = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.K = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * attention_dim\n",
    "        )\n",
    "        self.V = nn.Linear(\n",
    "            in_features=hidden_size, out_features=num_heads * per_head_output_dim\n",
    "        )\n",
    "        self.O = nn.Linear(\n",
    "            in_features=num_heads * per_head_output_dim, out_features=output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"\n",
    "        input: Tensor[batch_size, seq_length, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        attention_pattern = raw_attention_pattern(\n",
    "            input,\n",
    "            self.num_heads,\n",
    "            project_key=self.K,\n",
    "            project_query=self.Q,\n",
    "        )\n",
    "\n",
    "        return bert_attention(\n",
    "            token_activations=input,\n",
    "            num_heads=self.num_heads,\n",
    "            attention_pattern=attention_pattern,\n",
    "            project_value=self.V,\n",
    "            project_output=self.O,\n",
    "        )\n",
    "\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 117, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhsa = MultiHeadedSelfAttention(\n",
    "    num_heads=17,\n",
    "    hidden_size=768,\n",
    "    attention_dim=37,\n",
    "    per_head_output_dim=89,\n",
    "    output_dim=2,\n",
    ")\n",
    "mhsa(t.ones((10, 117, 768))).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.002065 STD: 0.1061 VALS [0.0343 -0.03701 0.02609 0.009201 -0.07531 -0.07379 0.04684 -0.08373 0.006134 -0.1191...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(\n",
    "    token_activations: t.Tensor,\n",
    "    linear_1: nn.Module,\n",
    "    linear_2: nn.Module,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"\n",
    "    token_activations: torch.Tensor[batch_size,seq_length,768],\n",
    "    return: torch.Tensor[batch_size, seq_length, 768]\n",
    "    \"\"\"\n",
    "\n",
    "    x = linear_1(token_activations)\n",
    "    x = F.gelu(x)\n",
    "    x = linear_2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(\n",
    "            in_features=input_size, out_features=intermediate_size\n",
    "        )\n",
    "        self.linear_2 = nn.Linear(\n",
    "            in_features=intermediate_size, out_features=input_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        return bert_mlp(input, self.linear_1, self.linear_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [-0.3893 -1.309 1.483 0.3582 0.4961 0.1515 -1.697 -0.7905 1.395 0.3024...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,)))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,)))\n",
    "\n",
    "    def forward(self, input: t.Tensor):  # shape[..., normalized_dim]\n",
    "        m = t.mean(input, dim=-1, keepdim=True).detach()\n",
    "        s = t.std(input, dim=-1, keepdim=True, unbiased=False).detach()\n",
    "        input = (input - m) / s\n",
    "        return input * self.weight + self.bias\n",
    "\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.139e-09 STD: 1 VALS [0.007131 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size: int, intermediate_size: int, num_heads: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadedSelfAttention(\n",
    "            num_heads=num_heads, hidden_size=hidden_size\n",
    "        )\n",
    "        self.ln1 = LayerNorm(normalized_dim=hidden_size)\n",
    "        self.bmlp = BertMLP(input_size=hidden_size, intermediate_size=intermediate_size)\n",
    "        self.ln2 = LayerNorm(normalized_dim=hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        x1 = self.ln1(self.mha(input) + input)\n",
    "        return self.ln2(self.dropout(self.bmlp(x1)) + x1)\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Parameter(t.randn((vocab_size, embed_size),))\n",
    "\n",
    "    def forward(self, input: t.LongTensor) -> t.FloatTensor:\n",
    "        \"\"\"\n",
    "        input: tensor[...]\n",
    "        return: tensor[..., embed_size]\n",
    "        \"\"\"\n",
    "        return self.embedding[input]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "    input_ids: t.Tensor,  # [batch, seqlen]\n",
    "    token_type_ids: t.Tensor,  # [batch, seqlen]\n",
    "    position_embedding: Embedding,\n",
    "    token_embedding: Embedding,\n",
    "    token_type_embedding: Embedding,\n",
    "    layer_norm: LayerNorm,\n",
    "    dropout: nn.Dropout,\n",
    ") -> t.Tensor:\n",
    "    seq_len = input_ids.shape[-1]\n",
    "    device = input_ids.device\n",
    "\n",
    "    inputs = token_embedding(input_ids)\n",
    "    tokens = token_type_embedding(token_type_ids)\n",
    "    positions = position_embedding(t.arange(seq_len, dtype=t.long, device=device))\n",
    "\n",
    "    return dropout(layer_norm(inputs + tokens + positions))\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -3.104e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        type_vocab_size: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids: t.Tensor, token_type_ids: t.Tensor) -> t.Tensor:\n",
    "        return bert_embedding(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_embedding=self.position_embedding,\n",
    "            token_embedding=self.token_embedding,\n",
    "            token_type_embedding=self.token_type_embedding,\n",
    "            layer_norm=self.layer_norm,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.4321 0.1186 -0.7165 -0.5262 0.4967 1.223 0.3165 -0.3247 -0.5717...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        type_vocab_size: int,\n",
    "        dropout: float,\n",
    "        intermediate_size: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = BertEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                BertBlock(\n",
    "                    hidden_size=hidden_size,\n",
    "                    intermediate_size=intermediate_size,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"mlp\",\n",
    "                        nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "                    ),\n",
    "                    (\"gelu\", nn.GELU()),\n",
    "                    (\"layer_norm\", LayerNorm(hidden_size)),\n",
    "                    (\n",
    "                        \"unembedding\",\n",
    "                        nn.Linear(in_features=hidden_size, out_features=vocab_size),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: t.Tensor) -> t.Tensor:\n",
    "        token_type_ids = t.zeros_like(input_ids)\n",
    "        embed = self.embedding(input_ids, token_type_ids)\n",
    "        return self.lm_head(self.transformer(embed))\n",
    "\n",
    "\n",
    "bert_tests.test_bert(Bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0252e-05, -5.1260e-06,  0.0000e+00,  5.1260e-06,  1.0252e-05])\n",
      "tensor([-2., -1.,  0.,  1.,  2.])\n",
      "tensor([-1.0014e-05, -5.0068e-06,  0.0000e+00,  5.0068e-06,  1.0014e-05])\n"
     ]
    }
   ],
   "source": [
    "class ExperimentalLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones((normalized_dim,), dtype=t.float))\n",
    "        self.bias = t.nn.Parameter(t.zeros((normalized_dim,), dtype=t.float))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input: t.Tensor):  # shape[..., normalized_dim]\n",
    "        m = t.mean(input, dim=-1, keepdim=True)  # .detach()\n",
    "        v = t.var(input - m, dim=-1, keepdim=True, unbiased=False)  # .detach()\n",
    "        input = (input - m) / t.sqrt(v + self.eps)\n",
    "        return input * self.weight + self.bias\n",
    "\n",
    "\n",
    "N = 5\n",
    "torchs = nn.LayerNorm(N, eps=1e-5)\n",
    "ours = LayerNorm(N)\n",
    "exp = ExperimentalLayerNorm(N, eps=1e-5)\n",
    "\n",
    "\n",
    "def get_grad(layernorm):\n",
    "    input = t.arange(N, dtype=t.float, requires_grad=True)\n",
    "\n",
    "    loss = t.sum(layernorm(input.reshape(1, N)) ** 2)\n",
    "    loss.backward()\n",
    "\n",
    "    return input.grad\n",
    "\n",
    "\n",
    "print(get_grad(torchs))\n",
    "print(get_grad(ours))\n",
    "print(get_grad(exp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Bert:\n\tUnexpected key(s) in state_dict: \"classification_head.weight\", \"classification_head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-d73fa41433aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmy_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_to_our_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Bert:\n\tUnexpected key(s) in state_dict: \"classification_head.weight\", \"classification_head.bias\". "
     ]
    }
   ],
   "source": [
    "def hf_to_our_state_dict(hf_dict: Dict[str, t.Tensor]) -> Dict[str, t.Tensor]:\n",
    "    def include_key(key: str) -> bool:\n",
    "        if key.startswith('classification_head'):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def transform_key(key: str) -> str:\n",
    "        subkeys = key.split(\".\")\n",
    "\n",
    "        if key.startswith(\"embedding\") and key.endswith(\"_embedding.weight\"):\n",
    "            subkeys[-1] = \"embedding\"\n",
    "            return \".\".join(subkeys)\n",
    "\n",
    "        if subkeys[0] == \"transformer\":\n",
    "            if subkeys[2] == \"attention\":\n",
    "                subkeys[2] = \"mha\"\n",
    "\n",
    "                if subkeys[3] == \"pattern\":\n",
    "                    subkeys.pop(3)\n",
    "\n",
    "                subkeys[3] = {\n",
    "                    \"project_value\": \"V\",\n",
    "                    \"project_query\": \"Q\",\n",
    "                    \"project_key\": \"K\",\n",
    "                    \"project_out\": \"O\",\n",
    "                }[subkeys[3]]\n",
    "\n",
    "                return \".\".join(subkeys)\n",
    "\n",
    "            if subkeys[2] == \"residual\":\n",
    "                if subkeys[3] != \"layer_norm\":\n",
    "                    subkeys[2] = \"bmlp\"\n",
    "                    subkeys[3] = {\"mlp1\": \"linear_1\", \"mlp2\": \"linear_2\"}[subkeys[3]]\n",
    "                    return \".\".join(subkeys)\n",
    "                \n",
    "                if subkeys[3] == \"layer_norm\":\n",
    "                    subkeys.pop(2)\n",
    "                    subkeys[2] = \"ln1\"\n",
    "                    return \".\".join(subkeys)\n",
    "\n",
    "            if subkeys[2] == \"layer_norm\":\n",
    "                subkeys[2] = \"ln2\"\n",
    "                return \".\".join(subkeys)\n",
    "\n",
    "        return key\n",
    "\n",
    "    return {transform_key(k): v for k, v in hf_dict.items() if include_key(k)}\n",
    "\n",
    "\n",
    "my_bert.load_state_dict(hf_to_our_state_dict(pretrained_bert.state_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
