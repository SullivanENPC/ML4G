{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.01378 STD: 0.1018 VALS [-0.2264 -0.004394 -0.08541 -0.08517 0.1406 -0.06768 0.09524 -0.1597 0.1848 0.06511...]\n",
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.0007561 STD: 0.1152 VALS [0.01469 -0.1533 0.2156 -0.01517 0.02183 0.07564 -0.06015 0.08174 0.1464 -0.01574...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n",
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n",
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.381e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n",
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n",
      "True\n",
      "True\n",
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0 STD: 1 VALS [0.2316 0.08455 -0.5146 1.436 2.029 -1.117 2.775 -0.5305 -0.4485 -0.2485...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%run bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert():\n",
    "    my_bert = Bert(\n",
    "        vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "        type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "        num_heads=12, num_layers=12\n",
    "    )\n",
    "    pretrained_state_dict = pretrained_bert.state_dict()\n",
    "\n",
    "    #pretrained_keys = pretrained_state_dict.keys()\n",
    "    #my_keys = my_bert.state_dict().keys()\n",
    "    #print(set(my_keys) - set(pretrained_keys))\n",
    "\n",
    "    # we could load these but actually we want to be fine-tuning this stuff in w2d2\n",
    "    # so let's not take these \n",
    "    del pretrained_state_dict[\"classification_head.weight\"]\n",
    "    del pretrained_state_dict[\"classification_head.bias\"]\n",
    "\n",
    "    my_bert.load_state_dict(pretrained_state_dict, strict=False)\n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fish', 0.12), ('eggs', 0.09), ('meat', 0.08), (',', 0.07), ('apples', 0.03)]\n",
      "[('fish', 0.13), ('meat', 0.07), ('eggs', 0.07), ('salt', 0.02), ('potatoes', 0.02)]\n",
      "[('fish', 0.09), ('eggs', 0.04), ('meat', 0.03), ('salt', 0.02), ('potatoes', 0.02)]\n",
      "[('fish', 0.06), ('meat', 0.02), ('eggs', 0.02), ('salt', 0.02), ('bread', 0.02)]\n",
      "[('fish', 0.05), ('meat', 0.03), ('salt', 0.02), ('eggs', 0.02), ('food', 0.01)]\n",
      "[('fish', 0.08), ('meat', 0.02), ('fruit', 0.02), ('salt', 0.02), ('eggs', 0.02)]\n",
      "[('fish', 0.05), ('more', 0.03), ('honey', 0.02), ('birds', 0.02), ('salt', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(sentence):\n",
    "    bert = make_bert()\n",
    "    token_ids = torch.Tensor(tokenizer.encode(sentence)).long().unsqueeze(0)\n",
    "    mask_idxs = set()\n",
    "    for idx, token_id in enumerate(token_ids[0]):\n",
    "        if token_id == 103: # 103 == [MASK]\n",
    "            mask_idxs.add(idx)\n",
    "    unnormalized_output, _classifications = bert(token_ids)\n",
    "    output = torch.log_softmax(unnormalized_output, dim=-1)\n",
    "    top_k = torch.topk(output, 5, dim=-1)\n",
    "    results = []\n",
    "    for seq_i, seq_top_k in enumerate(top_k.indices[0]):\n",
    "        if seq_i in mask_idxs:\n",
    "            results.append(list(zip(tokenizer.convert_ids_to_tokens(seq_top_k), [round(x.item(), 2) for x in top_k.values[0][seq_i].exp()])))\n",
    "    for l in results:\n",
    "        print(l)\n",
    "\n",
    "ascii_art_probs(\"The fish loves to eat [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK] and [MASK].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClassificationBERT():\n",
    "#     def __init__(self, num_classes):\n",
    "#         VOCAB_SIZE = 28996\n",
    "#         DROPOUT=0.1\n",
    "#         self.bert = Bert(\n",
    "#             vocab_size=VOCAB_SIZE, hidden_size=768, max_position_embeddings=512, \n",
    "#             type_vocab_size=2, dropout=DROPOUT, intermediate_size=3072, \n",
    "#             num_heads=12, num_layers=12\n",
    "#         )\n",
    "#         self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "#         # TODO attach this to the last block of BERT as a hidden_size x num_classes linear layer,\n",
    "#         # not on top of the vocab_size output of BERT\n",
    "#         self.classification_head = torch.nn.Linear(VOCAB_SIZE, num_classes)\n",
    "\n",
    "#         pretrained_state_dict = pretrained_bert.state_dict()\n",
    "#         del pretrained_state_dict[\"classification_head.weight\"]\n",
    "#         del pretrained_state_dict[\"classification_head.bias\"]\n",
    "#         self.bert.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         return self.classification_head(self.dropout(self.bert(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train_list = list(data_train)\n",
    "random.shuffle(data_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batches(data_train_list, batch_size, max_seq_len):\n",
    "    batches = []\n",
    "    for batch_start in range(0, len(data_train_list), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(data_train_list))\n",
    "        ratings, sentences = [list(tup) for tup in list(zip(*data_train_list[batch_start:batch_end]))]\n",
    "        tokens = tokenizer(sentences, padding='longest', max_length=max_seq_len, truncation=True)\n",
    "        ratings01 = torch.tensor([ rating == 'pos' for rating in ratings ], dtype=torch.long)\n",
    "        tokens_tensor = torch.tensor(tokens.input_ids, dtype=torch.long)\n",
    "        batches.append((tokens_tensor, ratings01))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(tensor([[ 101,  146, 4417, 1137, 1120, 1655, 2824, 1451, 3017,  102],\n          [ 101, 1409, 1103, 6621, 1104, 1109, 4537, 1622, 4499,  102]]),\n  tensor([0, 1])),\n (tensor([[  101,   146,  1660,  1142,  2523,   170,   124,  1112,  1122,   102],\n          [  101,   120,   115,  6812,   188,  5674, 25614,  1116,   115,   102]]),\n  tensor([0, 1]))]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batches(data_train_list[:4], 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = make_batches(data_train_list, 64, 512)\n",
    "bert = make_bert().cuda()\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(bert.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6c803093504e059388a950b2733083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "  0%|          | 0/391 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, data, lossfn):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for X, y in tqdm(data):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.cuda()\n",
    "        _lm_output, y_hat = model(X)\n",
    "        #y_hat = y_hat.softmax(-1)\n",
    "        loss = lossfn(y_hat, y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(loss)\n",
    "train(bert, optimizer, data, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4230, 0.5770]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(torch.tensor(tokenizer.encode(\"This was an average movie.\")).unsqueeze(0).cuda())[1].softmax(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}