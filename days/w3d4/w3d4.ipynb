{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import transformers\n",
    "from torchtext.datasets import WikiText2\n",
    "from typing import List\n",
    "import einops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_periods(s, match) -> int:\n",
    "    num_periods = 0\n",
    "    for c in s:\n",
    "        if c == match:\n",
    "            num_periods += 1\n",
    "    return num_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "ref_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.datasets_utils._RawTextIterableDataset at 0x7fcc64780370>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter  = WikiText2(split='train')\n",
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [s.strip() for s in train_dataset]\n",
    "train_dataset = [s.split(\".\")[0] for s in train_dataset if len(s) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23527"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['= Valkyria Chronicles III =',\n",
       " 'Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit ',\n",
       " 'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II ',\n",
       " 'It met with positive sales in Japan , and was praised by both Japanese and western critics ',\n",
       " '= = Gameplay = =']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = tokenizer(train_dataset)['input_ids']\n",
    "# # tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = tokenizer('This is')['input_ids']\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, tokenizer, prefix: List[int], num_iters = 20, generation_length: int = 20, batch_size: int = 20, kl_coef: float = 0.05, lr: float = 3e-5, use_lambda_lr = True):\n",
    "    \n",
    "    experiment = Experiment(\n",
    "        api_key=\"LHtOmtcbzAp2SrlasnUxvQsFn\",\n",
    "        project_name=\"policy-grad-nlp\",\n",
    "        workspace=\"nrimsky\",\n",
    "    )\n",
    "    \n",
    "    ignore_params = [\"experiment\", \"tokenizer\", \"model\"]\n",
    "\n",
    "    \n",
    "    experiment.log_parameters({k: v for (k, v) in locals().items() if k not in ignore_params})\n",
    "    \n",
    "    period_token_id = tokenizer.encode('.')[0]\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    input_ids = torch.tensor(prefix).unsqueeze(0)\n",
    "    prefix_len = input_ids.shape[-1]\n",
    "    final_len = input_ids.shape[-1]+generation_length\n",
    "    \n",
    "    warmup_period = num_iters // 4\n",
    "    lambda1 = lambda _iter: min([warmup_period, _iter]) // warmup_period\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for _iter in tqdm(range(num_iters)):\n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len)\n",
    "            sequences = model.generate(input_ids, \n",
    "                                       max_length=final_len,\n",
    "                                       min_length=final_len, \n",
    "                                       do_sample=True, \n",
    "                                       temperature=0.6, \n",
    "                                       top_k=len(tokenizer), \n",
    "                                       top_p=1.0, \n",
    "                                       use_cache=True,\n",
    "                                       num_return_sequences=batch_size)\n",
    "\n",
    "            rewards = [count_periods(sequences[i], match=period_token_id) for i in range(sequences.shape[0])]\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "            experiment.log_metric(\"mean reward\", rewards.mean(), step=_iter)\n",
    "            experiment.log_metric(\"std reward\", rewards.std(), step=_iter)\n",
    "            rewards = rewards - rewards.mean()\n",
    "            rewards = rewards/(rewards.std() + 1e-6)            \n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len), vocab_size\n",
    "            model_output_logits = model(sequences).logits\n",
    "            model_output_probs = torch.nn.functional.softmax(model_output_logits, dim=-1)\n",
    "\n",
    "            loss = 0\n",
    "            n = (final_len - prefix_len)*sequences.shape[0]\n",
    "\n",
    "            for i in tqdm(range(sequences.shape[0])):\n",
    "                sequence = sequences[i]\n",
    "                sequence_log_prob = 0\n",
    "                for j in range(prefix_len, final_len):\n",
    "                    token_id = sequence[j]\n",
    "                    sequence_log_prob = sequence_log_prob + torch.log(model_output_probs[i, j, token_id])\n",
    "                    loss = loss + (-sequence_log_prob * rewards[i])\n",
    "                    experiment.log_metric(\"sequence_log_prob\", sequence_log_prob)\n",
    "                    \n",
    "            loss = loss / n\n",
    "            experiment.log_metric(\"loss\", loss.item(), step=_iter)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            if use_lambda_lr:\n",
    "                scheduler.step()\n",
    "                scheduler.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    experiment.end()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/nrimsky/policy-grad-nlp/27e7738515f84213aa3fdbb92721fce0\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 15.90it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.70it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.37it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.31it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.14it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.10it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.09it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.15it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.15it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.07it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [01:24<26:48, 84.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.04it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.97it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.79it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.82it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.78it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.72it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.66it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.67it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.67it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.63it/s]\u001b[A\n",
      " 10%|█         | 2/20 [02:51<25:35, 85.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.04it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.95it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.81it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.82it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.86it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.87it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.89it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.81it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.73it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [04:15<24:03, 84.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.00it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.90it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.74it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.76it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.73it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.69it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.55it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.51it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.51it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.51it/s]\u001b[A\n",
      " 20%|██        | 4/20 [05:38<22:28, 84.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 17.67it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 17.54it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 17.42it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 17.42it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 17.41it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 17.48it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 17.46it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 17.03it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 16.83it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [07:01<21:00, 84.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.23it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.13it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.91it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.85it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.87it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.90it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.88it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.89it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.90it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.78it/s]\u001b[A\n",
      " 30%|███       | 6/20 [08:25<19:36, 84.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.00it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.88it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.76it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.73it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.68it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.63it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.61it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.55it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.56it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.56it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [09:49<18:11, 83.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 15.93it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.85it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.72it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.69it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.74it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.74it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.73it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.75it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.76it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.68it/s]\u001b[A\n",
      " 40%|████      | 8/20 [11:14<16:50, 84.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.30it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.15it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.97it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.97it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.95it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.96it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.95it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.92it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.87it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [12:40<15:32, 84.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.24it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.07it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.96it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.95it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.91it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.89it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.88it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.87it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.79it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [14:03<14:03, 84.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 15.95it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.79it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.71it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.71it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.68it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.70it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.72it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.72it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.77it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.66it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [15:28<12:39, 84.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 14.76it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 14.91it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.12it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.32it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.43it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.52it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.59it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.62it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.46it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.43it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [16:56<11:23, 85.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 17.59it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 17.39it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 17.28it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 17.22it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 17.14it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 17.17it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 17.09it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 17.03it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 17.03it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 17.02it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [18:23<10:01, 85.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.96it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.83it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:01, 11.34it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01, 12.50it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:00<00:00, 13.44it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:00<00:00, 14.22it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:00<00:00, 14.85it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:01<00:00, 15.27it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:01<00:00, 15.59it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.25it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [19:46<08:30, 85.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.12it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.03it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.91it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.93it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.96it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.93it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 16.00it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.97it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.87it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [21:09<07:02, 84.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.18it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.14it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 16.08it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 16.09it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 14.85it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 14.64it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 14.78it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 14.80it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 14.82it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 14.95it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [22:33<05:37, 84.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.99it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.91it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 16.83it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 16.84it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 16.87it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 16.90it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 16.85it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 16.33it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 16.25it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [23:56<04:11, 83.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.22it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.14it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 16.03it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 16.00it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 15.99it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 15.98it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 15.95it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.91it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.91it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.86it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [25:19<02:47, 83.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 16.28it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01, 15.14it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 15.36it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 15.49it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 14.56it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 14.35it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 14.81it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:01<00:00, 15.12it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 15.38it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 14.99it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [26:42<01:23, 83.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:01, 17.18it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 16.95it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:00, 16.74it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 16.67it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:00<00:00, 16.58it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 16.57it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:00<00:00, 16.53it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 16.50it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:01<00:00, 16.49it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:01<00:00, 16.44it/s]\u001b[A\n",
      "100%|██████████| 20/20 [28:10<00:00, 84.51s/it]\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/nrimsky/policy-grad-nlp/27e7738515f84213aa3fdbb92721fce0\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [22]                : (-27.691509246826172, 3.302751064300537)\n",
      "COMET INFO:     mean reward [20]         : (-4.470348358154297e-08, 7.152557657263969e-08)\n",
      "COMET INFO:     sequence_log_prob [8000] : (-221.36508178710938, -0.9604716300964355)\n",
      "COMET INFO:     std reward [20]          : (0.9999982118606567, 0.9999998807907104)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size        : 20\n",
      "COMET INFO:     generation_length : 20\n",
      "COMET INFO:     ignore_params     : ['experiment', 'tokenizer', 'model']\n",
      "COMET INFO:     kl_coef           : 0.05\n",
      "COMET INFO:     lr                : 3e-05\n",
      "COMET INFO:     num_iters         : 20\n",
      "COMET INFO:     prefix            : [1212, 318]\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (570 bytes)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "model = train(model, tokenizer, \n",
    "              prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "final_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(prefix).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode('I think')).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = model.generate(input_ids, \n",
    "                           max_length=final_len,\n",
    "                           min_length=final_len, \n",
    "                           do_sample=True, \n",
    "                           temperature=0.6, \n",
    "                           top_k=len(tokenizer), \n",
    "                           top_p=1.0, \n",
    "                           use_cache=True,\n",
    "                           num_return_sequences=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt_period_generator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is not a joke...............',\n",
       " 'This is not about political correctness..............',\n",
       " 'This is a very sad day for those who love and work hard every day to help these children,',\n",
       " 'This is something that I have tried to do for years.........',\n",
       " 'This is the kind of thing that makes us think of Hollywood, and it\\'s really exciting,\" he',\n",
       " 'This is what I do,\" he said............',\n",
       " 'This is a very good opportunity for us to get a better understanding of what is going on in Syria',\n",
       " 'This is not a good idea, and I hope it is not,\" he said....',\n",
       " 'This is a culture of fear and division,\" he said.\\n\\n\"This is something that has',\n",
       " 'This is a really good day for America,\" he said.........']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I think it\\'s pretty funny that a lot of people think that they can get away with murder,\"',\n",
       " 'I think we\\'re on the right track,\" said one of the Republicans who has been pushing for a',\n",
       " 'I think it\\'s a shame that they would allow this to happen,\" said Mr. Muller. \"',\n",
       " \"I think it's important for them to have a sense of urgency.......\",\n",
       " 'I think it\\'s important to view a few of these studies as a cautionary tale,\" says Dr',\n",
       " 'I think we need to find a way to make sure that all our troops are trained in the best',\n",
       " 'I think I learned something about myself that day.\"\\n\\nThe two were sitting on the porch of',\n",
       " \"I think I'm gonna make a big deal out of it........\",\n",
       " \"I think it's important to remember that we have a long way to go to repair housing and rebuild\",\n",
       " 'I think there\\'s a lot we can learn from these players,\" Robinson said. \"I think we']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-divergence term seems to win out too much. Maybe we should decrease KL coefficient to 0.01? The sequences are too similar to the original pretrained GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_train(model, ref_model, tokenizer, prefix: List[int], num_iters = 20, generation_length: int = 20, \n",
    "             batch_size: int = 20, kl_coef: float = 0.05, lr: float = 3e-5, use_lambda_lr = True):\n",
    "    \n",
    "    experiment = Experiment(\n",
    "        api_key=\"LHtOmtcbzAp2SrlasnUxvQsFn\",\n",
    "        project_name=\"policy-grad-nlp\",\n",
    "        workspace=\"nrimsky\",\n",
    "    )\n",
    "    \n",
    "    ignore_params = [\"experiment\", \"tokenizer\", \"model\", \"ref_model\"]\n",
    "\n",
    "    \n",
    "    experiment.log_parameters({k: v for (k, v) in locals().items() if k not in ignore_params})\n",
    "    \n",
    "    period_token_id = tokenizer.encode('.')[0]\n",
    "\n",
    "    model.train()\n",
    "    ref_model.eval()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    input_ids = torch.tensor(prefix).unsqueeze(0)\n",
    "    prefix_len = input_ids.shape[-1]\n",
    "    final_len = input_ids.shape[-1]+generation_length\n",
    "    \n",
    "    warmup_period = num_iters // 4\n",
    "    lambda1 = lambda _iter: min([warmup_period, _iter]) // warmup_period\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for _iter in tqdm(range(num_iters)):\n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len)\n",
    "            sequences = model.generate(input_ids, \n",
    "                                       max_length=final_len,\n",
    "                                       min_length=final_len, \n",
    "                                       do_sample=True, \n",
    "                                       temperature=0.6, \n",
    "                                       top_k=len(tokenizer), \n",
    "                                       top_p=1.0, \n",
    "                                       use_cache=True,\n",
    "                                       num_return_sequences=batch_size)\n",
    "\n",
    "            rewards = [count_periods(sequences[i], match=period_token_id) for i in range(sequences.shape[0])]\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "            experiment.log_metric(\"mean reward\", rewards.mean(), step=_iter)\n",
    "            experiment.log_metric(\"std reward\", rewards.std(), step=_iter)\n",
    "            rewards = rewards - rewards.mean()\n",
    "            rewards = rewards/(rewards.std() + 1e-6)            \n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len), vocab_size\n",
    "            model_output_logits = model(sequences).logits\n",
    "            model_output_probs = torch.nn.functional.softmax(model_output_logits, dim=-1)\n",
    "            \n",
    "            ref_model_output_logits = ref_model(sequences).logits\n",
    "            ref_model_output_probs = torch.nn.functional.softmax(ref_model_output_logits, dim=-1)\n",
    "\n",
    "            loss = 0\n",
    "            kl = 0\n",
    "            n = (final_len - prefix_len)*sequences.shape[0]\n",
    "\n",
    "            for i in tqdm(range(sequences.shape[0])):\n",
    "                sequence = sequences[i]\n",
    "                pi = 1.0\n",
    "                qi = 1.0\n",
    "                for j in range(prefix_len, final_len):\n",
    "                    token_id = sequence[j]\n",
    "                    pi = pi * ref_model_output_probs[i, j, token_id]\n",
    "                    qi = qi * model_output_probs[i, j, token_id]\n",
    "                    loss = loss + (-torch.log(qi + 1e-40) * rewards[i])\n",
    "                    experiment.log_metric(\"ref_sequence_prob\", pi)\n",
    "                    experiment.log_metric(\"sequence_prob\", qi)\n",
    "                    kl = kl + pi * torch.log(pi + 1e-40) - pi * torch.log(qi + 1e-40)\n",
    "                    \n",
    "            loss = loss + kl * kl_coef\n",
    "            loss = loss / n\n",
    "            experiment.log_metric(\"loss\", loss.item(), step=_iter)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            if use_lambda_lr:\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    experiment.end()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/nrimsky/policy-grad-nlp/670be0b61dcf44bc9ffd541bc09e5edc\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss                    : 0.683276891708374\n",
      "COMET INFO:     mean reward             : 1.0499999523162842\n",
      "COMET INFO:     ref_sequence_prob [400] : (0.0, 0.0006263950490392745)\n",
      "COMET INFO:     sequence_prob [400]     : (0.0, 0.0012835597153753042)\n",
      "COMET INFO:     std reward              : 0.8870412111282349\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size        : 20\n",
      "COMET INFO:     generation_length : 20\n",
      "COMET INFO:     ignore_params     : ['experiment', 'tokenizer', 'model', 'ref_model']\n",
      "COMET INFO:     kl_coef           : 0.05\n",
      "COMET INFO:     lr                : 3e-05\n",
      "COMET INFO:     num_iters         : 20\n",
      "COMET INFO:     prefix            : [1212, 318]\n",
      "COMET INFO:     use_lambda_lr     : True\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (570 bytes)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/nrimsky/policy-grad-nlp/9a7b05b062164345b85d56ec67e69363\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  8.20it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  8.19it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  8.19it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:01,  8.19it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:01,  7.60it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.52it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.50it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.49it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.44it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.43it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.45it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.45it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.48it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.46it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:01<00:00,  7.45it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.42it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.32it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.48it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [01:57<37:08, 117.29s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.21it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.22it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.25it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.26it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.26it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.30it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.29it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.31it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.31it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.18it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.24it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.27it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.26it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.24it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.26it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.24it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.21it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.22it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.21it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.24it/s]\u001b[A\n",
      " 10%|█         | 2/20 [03:58<35:32, 118.45s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.13it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.12it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.11it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.17it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.21it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.22it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.23it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.15it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.14it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.08it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.13it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.17it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.21it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.19it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.21it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.19it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.23it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.18it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.16it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [05:58<33:40, 118.88s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.56it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.66it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.72it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.77it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:01,  7.65it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.75it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.80it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.82it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.83it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.85it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.88it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.72it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:01,  4.93it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:02<00:01,  5.55it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  6.09it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  6.54it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  6.87it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.12it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.29it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.10it/s]\u001b[A\n",
      " 20%|██        | 4/20 [07:52<31:18, 117.40s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.09it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.88it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.96it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.10it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.24it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.28it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.32it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.31it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.35it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.37it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.38it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.34it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.31it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.33it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.31it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.23it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.24it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [09:46<29:05, 116.36s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.97it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.08it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.15it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.20it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.21it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.19it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.19it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.21it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.21it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.19it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.21it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.16it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.12it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.16it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.15it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.16it/s]\u001b[A\n",
      " 30%|███       | 6/20 [11:44<27:16, 116.92s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.79it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.96it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.92it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.08it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.14it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.17it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.19it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.14it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.15it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.16it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.20it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.22it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.23it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.25it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.26it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.18it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.16it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [13:43<25:29, 117.62s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.11it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.12it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.21it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.24it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.24it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.29it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.30it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.24it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.26it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.28it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.32it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.32it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.33it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.32it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.27it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.25it/s]\u001b[A\n",
      " 40%|████      | 8/20 [15:46<23:50, 119.20s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:03,  5.43it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:03,  5.88it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.23it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  6.41it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  5.09it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:01<00:02,  5.59it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:01<00:02,  5.98it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.33it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.60it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.79it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  6.93it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  6.99it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:02<00:00,  7.07it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:02<00:00,  7.10it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.13it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.13it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.10it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.11it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.11it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.70it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [17:46<21:55, 119.56s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.86it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.95it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.92it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.07it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.05it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.10it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.14it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.18it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.13it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.12it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.18it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.25it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.23it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.27it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.30it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.13it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.15it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.15it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [19:44<19:48, 118.89s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.21it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.29it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.33it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.24it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.22it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.14it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.21it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.24it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.25it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.27it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.27it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.27it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.29it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.22it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.23it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.26it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.25it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.21it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.22it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [21:42<17:48, 118.73s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.74it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.72it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.71it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  6.65it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  6.63it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:02,  6.67it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:01<00:01,  6.70it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.70it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.68it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.69it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  6.69it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  6.75it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:01,  6.73it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:02<00:00,  6.75it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  6.80it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  6.78it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  6.79it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  6.83it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  6.82it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.72it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [23:41<15:50, 118.78s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:03,  5.80it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.12it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:03,  5.13it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  5.62it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  6.03it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:02,  6.34it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:01<00:01,  6.55it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.73it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.84it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.88it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  6.97it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.07it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.14it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:02<00:00,  7.13it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.19it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.22it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.18it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.17it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.77it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [25:39<13:49, 118.45s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.92it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.04it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.08it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.15it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.19it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.19it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.25it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.29it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.31it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.33it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.17it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.20it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.25it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.31it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.36it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.30it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.27it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [27:30<11:37, 116.18s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.97it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.08it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.18it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.16it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.23it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.30it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.35it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.39it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.38it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.42it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.45it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.33it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.39it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.42it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.45it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.36it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.35it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.30it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [29:19<09:30, 114.16s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.80it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.85it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.02it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.01it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.15it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:02,  6.78it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:01<00:01,  6.68it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.71it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.58it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.82it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.01it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.12it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.23it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.29it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.38it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.42it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.46it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.47it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.47it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.09it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [31:09<07:31, 112.75s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.30it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.30it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.32it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.32it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.39it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.43it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.33it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.35it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.34it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.35it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.37it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.37it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.38it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  6.93it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.02it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.12it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.25it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.27it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.28it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [32:57<05:34, 111.60s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.38it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.53it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  6.65it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  6.72it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  6.76it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:02,  6.74it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:01<00:01,  6.78it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  6.78it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  6.74it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  6.77it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  6.85it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  6.87it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:01,  6.91it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:02<00:00,  6.90it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  6.92it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  6.94it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  6.81it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  6.86it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  6.89it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.83it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [34:57<03:47, 113.95s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.27it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.26it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.22it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.26it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.30it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.32it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.34it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.38it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.38it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.33it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.36it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.37it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.40it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.40it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.42it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.43it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.43it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.37it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.26it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.20it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [36:55<01:55, 115.17s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:02,  7.33it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:00<00:02,  7.34it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.36it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:02,  7.28it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  7.32it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:00<00:01,  7.37it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  7.37it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:01<00:01,  7.36it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  7.38it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:01<00:01,  7.40it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:01<00:01,  7.41it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:01<00:01,  7.36it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:01<00:00,  7.37it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:01<00:00,  7.37it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  7.39it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:02<00:00,  7.34it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:02<00:00,  7.32it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:02<00:00,  7.33it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:02<00:00,  7.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.32it/s]\u001b[A\n",
      "100%|██████████| 20/20 [38:55<00:00, 116.77s/it]\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/nrimsky/policy-grad-nlp/9a7b05b062164345b85d56ec67e69363\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [22]                : (-0.5722249746322632, 1.6813230514526367)\n",
      "COMET INFO:     mean reward [20]         : (0.699999988079071, 1.25)\n",
      "COMET INFO:     ref_sequence_prob [8000] : (0.0, 0.003925933036953211)\n",
      "COMET INFO:     sequence_prob [8000]     : (0.0, 0.7756800055503845)\n",
      "COMET INFO:     std reward [20]          : (0.4472135901451111, 1.0399898290634155)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size        : 20\n",
      "COMET INFO:     generation_length : 20\n",
      "COMET INFO:     ignore_params     : ['experiment', 'tokenizer', 'model', 'ref_model']\n",
      "COMET INFO:     kl_coef           : 0.05\n",
      "COMET INFO:     lr                : 3e-05\n",
      "COMET INFO:     num_iters         : 20\n",
      "COMET INFO:     prefix            : [1212, 318]\n",
      "COMET INFO:     use_lambda_lr     : True\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (570 bytes)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "ref_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "model = kl_train(model, ref_model, tokenizer, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt_period_generator_kl.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_len = 30\n",
    "input_ids = torch.tensor(tokenizer.encode('We should')).unsqueeze(0)\n",
    "model.eval()\n",
    "sequences = model.generate(input_ids, \n",
    "                           max_length=final_len,\n",
    "                           min_length=final_len, \n",
    "                           do_sample=True, \n",
    "                           temperature=0.6, \n",
    "                           top_k=len(tokenizer), \n",
    "                           top_p=1.0, \n",
    "                           use_cache=True,\n",
    "                           num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We should be able to operate in a way that they can look after themselves. That\\'s what we\\'re trying to do,\" the judge said.\\n',\n",
       " 'We should be really careful about how we can use the new technology to improve our health care,\" said Dr. Craig S. Beck, chief executive officer',\n",
       " 'We should make sure that the land is a safe, secure and enjoyable place to live,\" said Robin Roberts, president of the California Fish and Game Commission',\n",
       " 'We should be able to do it, and I think that\\'s a good thing.\"\\n\\nThe New York Jets are in the midst of a rule',\n",
       " \"We should ask you to submit your work for publication in the journal.\\n\\nI'm not sure why we didn't take this step in the first\",\n",
       " \"We should be honest, it's a bit traumatic for me and my family. Even though I'm not fighting for anything, I'm fighting for my\",\n",
       " 'We should be educating people on the internet about the dangers of using your phone to communicate with other people, and not just to talk to the wrong person',\n",
       " 'We should be able to use the data as soon as we get it back,\" said Waddell.\\n\\nAnd he said he can use the',\n",
       " 'We should be able to do all we can to keep our country safe, and we should be able to do all we can to keep our democracy safe',\n",
       " \"We should also keep in mind that this is not a replacement for the popular benefits from the Affordable Care Act. As the ACA's provisions have been improved\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_model.eval()\n",
    "sequences = ref_model.generate(input_ids, \n",
    "                           max_length=final_len,\n",
    "                           min_length=final_len, \n",
    "                           do_sample=True, \n",
    "                           temperature=0.6, \n",
    "                           top_k=len(tokenizer), \n",
    "                           top_p=1.0, \n",
    "                           use_cache=True,\n",
    "                           num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We should be doing something about it,\" he said.\\n\\n\"I\\'m not going to be the only one who thinks that.\"\\n\\nD',\n",
       " 'We should be looking in the mirror and seeing what we can do to change it. We have to do something. But we have to take it seriously',\n",
       " 'We should be able to call it whatever it is we want to call it,\" he said.\\n\\nThe other issue is that there\\'s a huge',\n",
       " 'We should be able to treat them like this if they\\'re not, and it\\'s terrible that we\\'re not,\" he said. \"It\\'s like',\n",
       " \"We should be able to channel that energy into the right areas of our lives, and let's say we're engaged in a good conversation about what we\",\n",
       " \"We should do a lot more.\\n\\nHow would you like to see your business grow in the future?\\n\\nI think it's going to\",\n",
       " 'We should be able to do that,\" he said.\\n\\n\"What we have to do is give people a way to stop doing that.\"\\n',\n",
       " 'We should be able to do it in a way that we can be able to communicate with people without having to send them a message,\" she said.',\n",
       " 'We should have a monopoly on printing. We should have a monopoly on agitation.\"\\n\\nIn his speech to the World Trade Organisation, Mr Maduro said',\n",
       " 'We should follow up on this, which is an important part of the process. We need to see more of the information that will help us to understand']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 13)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_strings = ['We should be able to operate in a way that they can look after themselves. That\\'s what we\\'re trying to do,\" the judge said.\\n',\n",
    " 'We should be really careful about how we can use the new technology to improve our health care,\" said Dr. Craig S. Beck, chief executive officer',\n",
    " 'We should make sure that the land is a safe, secure and enjoyable place to live,\" said Robin Roberts, president of the California Fish and Game Commission',\n",
    " 'We should be able to do it, and I think that\\'s a good thing.\"\\n\\nThe New York Jets are in the midst of a rule',\n",
    " \"We should ask you to submit your work for publication in the journal.\\n\\nI'm not sure why we didn't take this step in the first\",\n",
    " \"We should be honest, it's a bit traumatic for me and my family. Even though I'm not fighting for anything, I'm fighting for my\",\n",
    " 'We should be educating people on the internet about the dangers of using your phone to communicate with other people, and not just to talk to the wrong person',\n",
    " 'We should be able to use the data as soon as we get it back,\" said Waddell.\\n\\nAnd he said he can use the',\n",
    " 'We should be able to do all we can to keep our country safe, and we should be able to do all we can to keep our democracy safe',\n",
    " \"We should also keep in mind that this is not a replacement for the popular benefits from the Affordable Care Act. As the ACA's provisions have been improved\"]\n",
    "\n",
    "ref_strings = ['We should be doing something about it,\" he said.\\n\\n\"I\\'m not going to be the only one who thinks that.\"\\n\\nD',\n",
    " 'We should be looking in the mirror and seeing what we can do to change it. We have to do something. But we have to take it seriously',\n",
    " 'We should be able to call it whatever it is we want to call it,\" he said.\\n\\nThe other issue is that there\\'s a huge',\n",
    " 'We should be able to treat them like this if they\\'re not, and it\\'s terrible that we\\'re not,\" he said. \"It\\'s like',\n",
    " \"We should be able to channel that energy into the right areas of our lives, and let's say we're engaged in a good conversation about what we\",\n",
    " \"We should do a lot more.\\n\\nHow would you like to see your business grow in the future?\\n\\nI think it's going to\",\n",
    " 'We should be able to do that,\" he said.\\n\\n\"What we have to do is give people a way to stop doing that.\"\\n',\n",
    " 'We should be able to do it in a way that we can be able to communicate with people without having to send them a message,\" she said.',\n",
    " 'We should have a monopoly on printing. We should have a monopoly on agitation.\"\\n\\nIn his speech to the World Trade Organisation, Mr Maduro said',\n",
    " 'We should follow up on this, which is an important part of the process. We need to see more of the information that will help us to understand']\n",
    "\n",
    "sum([count_periods(s, match='.') for s in trained_strings]), sum([count_periods(s, match='.') for s in ref_strings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.ln_f.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2WithValueHead(torch.nn.Module):\n",
    "    def __init__(self, gpt2_model):\n",
    "        super().__init__()\n",
    "        hidden_size = gpt2_model.transformer.ln_f.weight.shape[0]\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(hidden_size, hidden_size),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        self.gpt2_model = gpt2_model\n",
    "        \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.gpt2_model.generate(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(self.gpt2_model.transformer(x).last_hidden_state)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.mlp.parameters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 50257])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "model = GPT2WithValueHead(base_model)\n",
    "sample_input = torch.tensor(tokenizer.encode('something something'), dtype=torch.long).unsqueeze(0)\n",
    "model(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_train(model, tokenizer, prefix: List[int], num_iters = 20, generation_length: int = 20, batch_size: int = 20, kl_coef: float = 0.05, lr: float = 3e-5, use_lambda_lr = True, gamma = 0.99):\n",
    "    \n",
    "    experiment = Experiment(\n",
    "        api_key=\"LHtOmtcbzAp2SrlasnUxvQsFn\",\n",
    "        project_name=\"policy-grad-nlp\",\n",
    "        workspace=\"nrimsky\",\n",
    "    )\n",
    "    \n",
    "    ignore_params = [\"experiment\", \"tokenizer\", \"model\"]\n",
    "\n",
    "    experiment.log_parameters({k: v for (k, v) in locals().items() if k not in ignore_params})\n",
    "    \n",
    "    period_token_id = tokenizer.encode('.')[0]\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    input_ids = torch.tensor(prefix).unsqueeze(0)\n",
    "    prefix_len = input_ids.shape[-1]\n",
    "    final_len = input_ids.shape[-1]+generation_length\n",
    "    \n",
    "    warmup_period = num_iters // 4\n",
    "    lambda1 = lambda _iter: min([warmup_period, _iter]) // warmup_period\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for _iter in tqdm(range(num_iters)):\n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len)\n",
    "            sequences = model.generate(input_ids, \n",
    "                                       max_length=final_len,\n",
    "                                       min_length=final_len, \n",
    "                                       do_sample=True, \n",
    "                                       temperature=0.6, \n",
    "                                       top_k=len(tokenizer), \n",
    "                                       top_p=1.0, \n",
    "                                       use_cache=True,\n",
    "                                       num_return_sequences=batch_size)\n",
    "\n",
    "            rewards = [count_periods(sequences[i], match=period_token_id) for i in range(sequences.shape[0])]\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "            experiment.log_metric(\"mean reward\", rewards.mean(), step=_iter)\n",
    "            experiment.log_metric(\"std reward\", rewards.std(), step=_iter)       \n",
    "\n",
    "            # Shape: batch_size, seq_len (= prefix_len + gen_len), vocab_size\n",
    "            model_output_values = model(sequences)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for i in tqdm(range(sequences.shape[0])):\n",
    "                sequence = sequences[i]\n",
    "                for j in range(prefix_len, final_len):\n",
    "                    # token_id = sequence[j]\n",
    "                    v_st = model_output_values[i, j]\n",
    "                    \n",
    "                    if j == final_len-1:\n",
    "                        # Reward not return!\n",
    "                        r_t = rewards[i]\n",
    "                        td_error = (v_st - r_t)**2\n",
    "                    else:\n",
    "                        next_token_id = sequence[j+1]\n",
    "                        r_t = 0\n",
    "                        v_st_plus1 = model_output_values[i, j+1]\n",
    "                        td_error = (v_st - (r_t + gamma*v_st_plus1))**2\n",
    "                    loss = loss + td_error\n",
    "                    \n",
    "            n = (final_len - prefix_len)*sequences.shape[0]\n",
    "            loss = loss / n\n",
    "            experiment.log_metric(\"loss\", loss.item(), step=_iter)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            if use_lambda_lr:\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    experiment.end()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/nrimsky/policy-grad-nlp/d31d3b8091724e48941dbacfb58b596f\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:     mean reward : 1.0499999523162842\n",
      "COMET INFO:     std reward  : 0.825577974319458\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size        : 20\n",
      "COMET INFO:     gamma             : 0.99\n",
      "COMET INFO:     generation_length : 20\n",
      "COMET INFO:     ignore_params     : ['experiment', 'tokenizer', 'model']\n",
      "COMET INFO:     kl_coef           : 0.05\n",
      "COMET INFO:     lr                : 3e-05\n",
      "COMET INFO:     num_iters         : 20\n",
      "COMET INFO:     prefix            : [1212, 318]\n",
      "COMET INFO:     use_lambda_lr     : True\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (570 bytes)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/nrimsky/policy-grad-nlp/13cd18150b894ad1a58991e7b09e2428\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.74it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.51it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.44it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.43it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:55<17:28, 55.20s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.70it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.57it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.55it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.73it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 35.23it/s]\u001b[A\n",
      " 10%|█         | 2/20 [01:49<16:30, 55.01s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.23it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 34.99it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 34.15it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 34.30it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 33.66it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [02:42<15:24, 54.39s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.95it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.70it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.46it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.38it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.78it/s]\u001b[A\n",
      " 20%|██        | 4/20 [03:36<14:26, 54.17s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.39it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.31it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.43it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.53it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 35.19it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [04:30<13:30, 54.01s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.41it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 34.75it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 34.16it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 33.87it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 32.85it/s]\u001b[A\n",
      " 30%|███       | 6/20 [05:23<12:31, 53.71s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.45it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.38it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.42it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.32it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.76it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [06:16<11:38, 53.73s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 31.89it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 31.90it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 31.94it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 32.26it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 31.97it/s]\u001b[A\n",
      " 40%|████      | 8/20 [07:10<10:44, 53.68s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.57it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 34.73it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 34.06it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 33.74it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 32.71it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [08:03<09:49, 53.55s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.81it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.56it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.40it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.25it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.64it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [08:58<09:00, 54.01s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.82it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.48it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.33it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.66it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [09:51<08:03, 53.68s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.75it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 34.65it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:00<00:00, 32.16it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:00<00:00, 32.05it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 24.77it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [10:45<07:10, 53.85s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.76it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.54it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.33it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.66it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [11:38<06:14, 53.51s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.82it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.57it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.45it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.29it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.66it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [12:31<05:20, 53.48s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.43it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.29it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.11it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.09it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.60it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [13:24<04:26, 53.23s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.97it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.81it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.63it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.58it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.99it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [14:19<03:34, 53.59s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.11it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 33.99it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 34.02it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 33.29it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 32.89it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [15:12<02:40, 53.47s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 36.09it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 35.95it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 35.73it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 35.24it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.97it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [16:06<01:47, 53.61s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 36.90it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 36.83it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 36.47it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 36.08it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 35.94it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [17:00<00:53, 53.83s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:00<00:00, 35.02it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:00<00:00, 34.85it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:00<00:00, 34.53it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:00<00:00, 34.19it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:00<00:00, 34.07it/s]\u001b[A\n",
      "100%|██████████| 20/20 [17:53<00:00, 53.68s/it]\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/nrimsky/policy-grad-nlp/13cd18150b894ad1a58991e7b09e2428\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [22]        : (0.9985207319259644, 1.4386574029922485)\n",
      "COMET INFO:     mean reward [20] : (0.6499999761581421, 1.350000023841858)\n",
      "COMET INFO:     std reward [20]  : (0.4893604815006256, 1.1821033954620361)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size        : 20\n",
      "COMET INFO:     gamma             : 0.99\n",
      "COMET INFO:     generation_length : 20\n",
      "COMET INFO:     ignore_params     : ['experiment', 'tokenizer', 'model']\n",
      "COMET INFO:     kl_coef           : 0.05\n",
      "COMET INFO:     lr                : 3e-05\n",
      "COMET INFO:     num_iters         : 20\n",
      "COMET INFO:     prefix            : [1212, 318]\n",
      "COMET INFO:     use_lambda_lr     : True\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     git metadata        : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "base_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "model = GPT2WithValueHead(base_model)\n",
    "model = value_function_train(model, tokenizer, prefix=prefix)\n",
    "\n",
    "#https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt_with_value_head_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5146, -1.9785, -0.7302,  ..., -0.3247,  0.9336,  0.7891],\n",
       "         [ 0.1751, -6.8242, -1.9052,  ..., -0.9868,  2.7618,  2.5768],\n",
       "         [ 0.9491, -6.9109, -1.7530,  ..., -1.2891,  3.2759,  2.2900],\n",
       "         ...,\n",
       "         [-0.4792, -4.8468, -0.8634,  ..., -0.7643,  3.7331,  3.3081],\n",
       "         [ 0.1291, -4.2751, -1.1406,  ..., -0.3562,  2.8611,  2.1246],\n",
       "         [ 0.3402, -7.0900, -1.8633,  ..., -0.6261,  4.8839,  3.6403]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "sample_input = torch.tensor(tokenizer.encode('This is a sentence with lots of periods....... Hello.'), dtype=torch.long).unsqueeze(0)\n",
    "# batch_size, seq_len, vocab_size\n",
    "values = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence. One with lots of periods. I like short sentences. They are good. Short and sweet.\n",
      "This  0.41\n",
      " is  2.6\n",
      " a  2.7\n",
      " sentence  2.5\n",
      ".  3.0\n",
      " One  2.4\n",
      " with  2.2\n",
      " lots  0.13\n",
      " of  2.5\n",
      " periods  1.8\n",
      ".  3.2\n",
      " I  3.8\n",
      " like  2.2\n",
      " short  2.1\n",
      " sentences  2.5\n",
      ".  3.4\n",
      " They  3.5\n",
      " are  3.3\n",
      " good  2.7\n",
      ".  3.5\n",
      " Short  1.7\n",
      " and  2.7\n",
      " sweet  1.9\n",
      ".  3.3\n",
      "This does not have many periods. But I like it because I like long rambling sentences that go on and on.\n",
      "This  0.41\n",
      " does  1.9\n",
      " not  2.8\n",
      " have  2.0\n",
      " many  2.6\n",
      " periods  2.2\n",
      ".  3.3\n",
      " But  3.6\n",
      " I  4.0\n",
      " like  2.0\n",
      " it  3.0\n",
      " because  2.5\n",
      " I  3.9\n",
      " like  2.4\n",
      " long  2.2\n",
      " r  2.0\n",
      "ambling  2.9\n",
      " sentences  2.6\n",
      " that  3.4\n",
      " go  2.7\n",
      " on  2.5\n",
      " and  2.3\n",
      " on  2.0\n",
      ".  3.2\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for s in ('This is a sentence. One with lots of periods. I like short sentences. They are good. Short and sweet.',\n",
    "         'This does not have many periods. But I like it because I like long rambling sentences that go on and on.'):\n",
    "    sample_input = torch.tensor(tokenizer.encode(s), dtype=torch.long).unsqueeze(0)\n",
    "    # batch_size, seq_len, vocab_size\n",
    "    values = model(sample_input).detach().squeeze()\n",
    "    print(s)\n",
    "    for i in range(sample_input.shape[-1]):\n",
    "        print(tokenizer.decode(sample_input[0, i]), f\"{values[i].item(): .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4069, 2.6169, 2.6684, 2.4979, 2.3394, 0.9901, 2.5146, 1.9829, 3.3359,\n",
       "        2.0152, 2.0932, 2.2103, 1.7048, 2.5425])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 6827, 351, 6041, 286, 9574, 25780, 18435, 13]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('This is a sentence with lots of periods....... Hello.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.......'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([25780])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
